{"RES": {"2412.10694": "|**2024-12-14**|**Grasp What You Want: Embodied Dexterous Grasping System Driven by Your Voice**|Junliang Li et.al.|[2412.10694](http://arxiv.org/abs/2412.10694)|null|\n", "2412.02402": "|**2024-12-03**|**RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2412.02402](http://arxiv.org/abs/2412.02402)|**[link](https://github.com/sosppxo/rg-san)**|\n", "2411.19067": "|**2024-11-28**|**MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation**|Minhyun Lee et.al.|[2411.19067](http://arxiv.org/abs/2411.19067)|**[link](https://github.com/naver-ai/maskris)**|\n", "2411.15087": "|**2024-11-22**|**Instance-Aware Generalized Referring Expression Segmentation**|E-Ro Nguyen et.al.|[2411.15087](http://arxiv.org/abs/2411.15087)|null|\n", "2411.01494": "|**2024-11-03**|**Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation**|Seongsu Ha et.al.|[2411.01494](http://arxiv.org/abs/2411.01494)|null|\n", "2410.18923": "|**2024-10-31**|**SegLLM: Multi-round Reasoning Segmentation**|XuDong Wang et.al.|[2410.18923](http://arxiv.org/abs/2410.18923)|null|\n", "2410.09855": "|**2024-10-13**|**Text4Seg: Reimagining Image Segmentation as Text Generation**|Mengcheng Lan et.al.|[2410.09855](http://arxiv.org/abs/2410.09855)|**[link](https://github.com/mc-lan/text4seg)**|\n", "2410.01544": "|**2024-12-04**|**Boosting Weakly-Supervised Referring Image Segmentation via Progressive Comprehension**|Zaiquan Yang et.al.|[2410.01544](http://arxiv.org/abs/2410.01544)|null|\n", "2409.19569": "|**2024-09-29**|**Fully Aligned Network for Referring Image Segmentation**|Yong Liu et.al.|[2409.19569](http://arxiv.org/abs/2409.19569)|null|\n", "2409.19457": "|**2024-09-28**|**A Parameter-Efficient Tuning Framework for Language-guided Object Grounding and Robot Grasping**|Houjian Yu et.al.|[2409.19457](http://arxiv.org/abs/2409.19457)|null|\n", "2409.17020": "|**2024-09-25**|**PTQ4RIS: Post-Training Quantization for Referring Image Segmentation**|Xiaoyan Jiang et.al.|[2409.17020](http://arxiv.org/abs/2409.17020)|**[link](https://github.com/gugu511yy/ptq4ris)**|\n", "2409.11518": "|**2024-09-17**|**Robot Manipulation in Salient Vision through Referring Image Segmentation and Geometric Constraints**|Chen Jiang et.al.|[2409.11518](http://arxiv.org/abs/2409.11518)|null|\n", "2409.10542": "|**2024-12-14**|**SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation**|Yi-Chia Chen et.al.|[2409.10542](http://arxiv.org/abs/2409.10542)|null|\n", "2408.15521": "|**2024-09-19**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|Seonghoon Yu et.al.|[2408.15521](http://arxiv.org/abs/2408.15521)|null|\n", "2408.07539": "|**2024-08-14**|**Cross-aware Early Fusion with Stage-divided Vision and Language Transformer Encoders for Referring Image Segmentation**|Yubin Cho et.al.|[2408.07539](http://arxiv.org/abs/2408.07539)|null|\n", "2408.03940": "|**2024-08-07**|**How Well Can Vision Language Models See Image Details?**|Chenhui Gou et.al.|[2408.03940](http://arxiv.org/abs/2408.03940)|null|\n", "2407.20664": "|**2024-07-31**|**3D-GRES: Generalized 3D Referring Expression Segmentation**|Changli Wu et.al.|[2407.20664](http://arxiv.org/abs/2407.20664)|**[link](https://github.com/sosppxo/MDIN)**|\n", "2407.18244": "|**2024-07-25**|**RefMask3D: Language-Guided Transformer for 3D Referring Segmentation**|Shuting He et.al.|[2407.18244](http://arxiv.org/abs/2407.18244)|**[link](https://github.com/heshuting555/refmask3d)**|\n", "2407.07412": "|**2024-07-17**|**Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation**|Seonghoon Yu et.al.|[2407.07412](http://arxiv.org/abs/2407.07412)|**[link](https://github.com/seonghoon-yu/pseudo-ris)**|\n", "2407.02389": "|**2024-07-02**|**SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation**|Sayan Nag et.al.|[2407.02389](http://arxiv.org/abs/2407.02389)|null|\n", "2406.20076": "|**2024-10-15**|**EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model**|Yuxuan Zhang et.al.|[2406.20076](http://arxiv.org/abs/2406.20076)|**[link](https://github.com/hustvl/evf-sam)**|\n", "2406.05821": "|**2024-06-09**|**F-LMM: Grounding Frozen Large Multimodal Models**|Size Wu et.al.|[2406.05821](http://arxiv.org/abs/2406.05821)|**[link](https://github.com/wusize/f-lmm)**|\n", "2406.01451": "|**2024-06-03**|**SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation**|Danni Yang et.al.|[2406.01451](http://arxiv.org/abs/2406.01451)|**[link](https://github.com/nini0919/semires)**|\n", "2405.17596": "|**2024-07-27**|**GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane**|Yansong Qu et.al.|[2405.17596](http://arxiv.org/abs/2405.17596)|null|\n", "2405.15658": "|**2024-11-25**|**CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation**|Zhuoyan Luo et.al.|[2405.15658](http://arxiv.org/abs/2405.15658)|**[link](https://github.com/robertluo1/cohd)**|\n", "2405.15169": "|**2024-05-24**|**Bring Adaptive Binding Prototypes to Generalized Referring Expression Segmentation**|Weize Li et.al.|[2405.15169](http://arxiv.org/abs/2405.15169)|**[link](https://github.com/buptlwz/mabp)**|\n", "2405.11205": "|**2024-05-18**|**Fuse & Calibrate: A bi-directional Vision-Language Guided Framework for Referring Image Segmentation**|Yichen Yan et.al.|[2405.11205](http://arxiv.org/abs/2405.11205)|null|\n", "2405.10707": "|**2024-05-21**|**HARIS: Human-Like Attention for Reference Image Segmentation**|Mengxi Zhang et.al.|[2405.10707](http://arxiv.org/abs/2405.10707)|null|\n", "2405.09006": "|**2024-05-15**|**Spatial Semantic Recurrent Mining for Referring Image Segmentation**|Jiaxing Yang et.al.|[2405.09006](http://arxiv.org/abs/2405.09006)|null|\n", "2404.11998": "|**2024-04-18**|**Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation**|Qiyuan Dai et.al.|[2404.11998](http://arxiv.org/abs/2404.11998)|null|\n", "2404.08590": "|**2024-11-04**|**Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding**|Hai Nguyen-Truong et.al.|[2404.08590](http://arxiv.org/abs/2404.08590)|null|\n", "2404.08281": "|**2024-04-12**|**Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation**|Yichen Yan et.al.|[2404.08281](http://arxiv.org/abs/2404.08281)|null|\n", "2404.00650": "|**2024-04-27**|**Deep Instruction Tuning for Segment Anything Model**|Xiaorui Huang et.al.|[2404.00650](http://arxiv.org/abs/2404.00650)|**[link](https://github.com/wysnzzzz/dit)**|\n", "2403.17839": "|**2024-07-25**|**ReMamber: Referring Image Segmentation with Mamba Twister**|Yuhuan Yang et.al.|[2403.17839](http://arxiv.org/abs/2403.17839)|**[link](https://github.com/yyh-rain-song/ReMamber)**|\n", "2403.14598": "|**2024-03-21**|**PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model**|Zheng Zhang et.al.|[2403.14598](http://arxiv.org/abs/2403.14598)|**[link](https://github.com/zamling/psalm)**|\n", "2402.18400": "|**2024-06-27**|**Towards Alleviating Text-to-Image Retrieval Hallucination for CLIP in Zero-shot Learning**|Hanyao Wang et.al.|[2402.18400](http://arxiv.org/abs/2402.18400)|null|\n", "2402.05589": "|**2024-02-11**|**RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner**|Ying Zang et.al.|[2402.05589](http://arxiv.org/abs/2402.05589)|null|\n", "2402.02555": "|**2024-02-04**|**Generalizable Entity Grounding via Assistance of Large Language Model**|Lu Qi et.al.|[2402.02555](http://arxiv.org/abs/2402.02555)|null|\n", "2401.11775": "|**2024-01-22**|**Collaborative Position Reasoning Network for Referring Image Segmentation**|Jianjian Cao et.al.|[2401.11775](http://arxiv.org/abs/2401.11775)|null|\n", "2312.15715": "|**2023-12-25**|**UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces**|Jiannan Wu et.al.|[2312.15715](http://arxiv.org/abs/2312.15715)|**[link](https://github.com/foundationvision/uniref)**|\n", "2312.12198": "|**2024-03-25**|**Mask Grounding for Referring Image Segmentation**|Yong Xien Chng et.al.|[2312.12198](http://arxiv.org/abs/2312.12198)|**[link](https://github.com/yxchng/mask-grounding)**|\n", "2312.12470": "|**2024-04-02**|**Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation**|Sihan Liu et.al.|[2312.12470](http://arxiv.org/abs/2312.12470)|**[link](https://github.com/lsan2401/rmsin)**|\n", "2312.10103": "|**2024-03-21**|**GSVA: Generalized Segmentation via Multimodal Large Language Models**|Zhuofan Xia et.al.|[2312.10103](http://arxiv.org/abs/2312.10103)|**[link](https://github.com/leaplabthu/gsva)**|\n", "2312.08007": "|**2024-03-21**|**Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring Expression Segmentation**|Wenxuan Wang et.al.|[2312.08007](http://arxiv.org/abs/2312.08007)|**[link](https://github.com/rubics-xuan/mres)**|\n", "2312.00452": "|**2023-12-01**|**Towards Generalizable Referring Image Segmentation via Target Prompt and Visual Coherence**|Yajie Liu et.al.|[2312.00452](http://arxiv.org/abs/2312.00452)|null|\n", "2311.18835": "|**2023-11-30**|**InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation**|Rongyao Fang et.al.|[2311.18835](http://arxiv.org/abs/2311.18835)|null|\n", "2311.17952": "|**2023-11-29**|**Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation**|Minhyeok Lee et.al.|[2311.17952](http://arxiv.org/abs/2311.17952)|null|\n", "2311.15727": "|**2024-05-21**|**RISAM: Referring Image Segmentation via Mutual-Aware Attention Features**|Mengxi Zhang et.al.|[2311.15727](http://arxiv.org/abs/2311.15727)|null|\n", "2311.13601": "|**2023-11-22**|**Visual In-Context Prompting**|Feng Li et.al.|[2311.13601](http://arxiv.org/abs/2311.13601)|**[link](https://github.com/ux-decoder/dinov)**|\n", "2311.12327": "|**2024-04-26**|**Enhancing Visual Grounding and Generalization: A Multi-Task Cycle Training Approach for Vision-Language Models**|Xiaoyu Yang et.al.|[2311.12327](http://arxiv.org/abs/2311.12327)|**[link](https://github.com/anonymgiant/vilam)**|\n", "2311.04498": "|**2023-12-18**|**NExT-Chat: An LMM for Chat, Detection and Segmentation**|Ao Zhang et.al.|[2311.04498](http://arxiv.org/abs/2311.04498)|**[link](https://github.com/next-chatv/next-chat)**|\n", "2311.03356": "|**2024-06-02**|**GLaMM: Pixel Grounding Large Multimodal Model**|Hanoona Rasheed et.al.|[2311.03356](http://arxiv.org/abs/2311.03356)|**[link](https://github.com/mbzuai-oryx/groundingLMM)**|\n", "2311.00397": "|**2023-11-27**|**Towards Omni-supervised Referring Expression Segmentation**|Minglang Huang et.al.|[2311.00397](http://arxiv.org/abs/2311.00397)|**[link](https://github.com/nineblu/omni-res)**|\n", "2310.18049": "|**2023-10-27**|**Text Augmented Spatial-aware Zero-shot Referring Image Segmentation**|Yucheng Suo et.al.|[2310.18049](http://arxiv.org/abs/2310.18049)|null|\n", "2310.13479": "|**2024-08-20**|**Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation**|Francisco Eiras et.al.|[2310.13479](http://arxiv.org/abs/2310.13479)|**[link](https://github.com/fgirbal/segment-select-correct)**|\n", "2309.17205": "|**2023-09-29**|**Towards Complex-query Referring Image Segmentation: A Novel Benchmark**|Wei Ji et.al.|[2309.17205](http://arxiv.org/abs/2309.17205)|null|\n", "2309.09183": "|**2023-09-17**|**CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation**|Chen Jiang et.al.|[2309.09183](http://arxiv.org/abs/2309.09183)|null|\n", "2309.04109": "|**2024-10-01**|**From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models**|Changming Xiao et.al.|[2309.04109](http://arxiv.org/abs/2309.04109)|**[link](https://github.com/Big-Brother-Pikachu/Text2Mask)**|\n", "2309.01017": "|**2023-09-02**|**Contrastive Grouping with Transformer for Referring Image Segmentation**|Jiajin Tang et.al.|[2309.01017](http://arxiv.org/abs/2309.01017)|**[link](https://github.com/toneyaya/cgformer)**|\n", "2308.16777": "|**2023-09-01**|**Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models**|Minheng Ni et.al.|[2308.16777](http://arxiv.org/abs/2308.16777)|null|\n", "2308.16632": "|**2023-08-31**|**3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2308.16632](http://arxiv.org/abs/2308.16632)|**[link](https://github.com/sosppxo/3d-stmn)**|\n", "2308.15512": "|**2023-10-24**|**Shatter and Gather: Learning Referring Image Segmentation with Text Supervision**|Dongwon Kim et.al.|[2308.15512](http://arxiv.org/abs/2308.15512)|**[link](https://github.com/kdwonn/SaG)**|\n", "2308.14575": "|**2023-08-28**|**Referring Image Segmentation Using Text Supervision**|Fang Liu et.al.|[2308.14575](http://arxiv.org/abs/2308.14575)|**[link](https://github.com/fawnliu/tris)**|\n", "2308.13853": "|**2023-08-26**|**Beyond One-to-One: Rethinking the Referring Image Segmentation**|Yutao Hu et.al.|[2308.13853](http://arxiv.org/abs/2308.13853)|**[link](https://github.com/toggle1995/ris-dmmi)**|\n", "2308.09779": "|**2024-10-12**|**EAVL: Explicitly Align Vision and Language for Referring Image Segmentation**|Yichen Yan et.al.|[2308.09779](http://arxiv.org/abs/2308.09779)|null|\n", "2307.11545": "|**2023-07-21**|**Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation**|Zunnan Xu et.al.|[2307.11545](http://arxiv.org/abs/2307.11545)|**[link](https://github.com/kkakkkka/etris)**|\n", "2307.00997": "|**2024-09-03**|**RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation**|Yonglin Li et.al.|[2307.00997](http://arxiv.org/abs/2307.00997)|**[link](https://github.com/lancasterli/refsam)**|\n", "2306.10750": "|**2023-06-19**|**WiCo: Win-win Cooperation of Bottom-up and Top-down Referring Image Segmentation**|Zesen Cheng et.al.|[2306.10750](http://arxiv.org/abs/2306.10750)|null|\n", "2306.08625": "|**2024-03-01**|**RRSIS: Referring Remote Sensing Image Segmentation**|Zhenghang Yuan et.al.|[2306.08625](http://arxiv.org/abs/2306.08625)|null|\n", "2306.08498": "|**2024-04-07**|**Extending CLIP's Image-Text Alignment to Referring Image Segmentation**|Seoyeon Kim et.al.|[2306.08498](http://arxiv.org/abs/2306.08498)|null|\n", "2306.00968": "|**2023-06-01**|**GRES: Generalized Referring Expression Segmentation**|Chang Liu et.al.|[2306.00968](http://arxiv.org/abs/2306.00968)|**[link](https://github.com/henghuiding/ReLA)**|\n", "2305.18279": "|**2024-08-12**|**Contextual Object Detection with Multimodal Large Language Models**|Yuhang Zang et.al.|[2305.18279](http://arxiv.org/abs/2305.18279)|**[link](https://github.com/yuhangzang/contextdet)**|\n", "2305.15302": "|**2023-05-24**|**Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation**|Chang Liu et.al.|[2305.15302](http://arxiv.org/abs/2305.15302)|null|\n", "2305.14969": "|**2023-05-24**|**MMNet: Multi-Mask Network for Referring Image Segmentation**|Yichen Yan et.al.|[2305.14969](http://arxiv.org/abs/2305.14969)|null|\n", "2305.12452": "|**2023-05-21**|**Advancing Referring Expression Segmentation Beyond Single Image**|Yixuan Wu et.al.|[2305.12452](http://arxiv.org/abs/2305.12452)|null|\n", "2305.11481": "|**2024-02-14**|**CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation**|Wenxuan Wang et.al.|[2305.11481](http://arxiv.org/abs/2305.11481)|null|\n", "2304.04415": "|**2023-04-12**|**Meta Compositional Referring Expression Segmentation**|Li Xu et.al.|[2304.04415](http://arxiv.org/abs/2304.04415)|null|\n", "2303.17811": "|**2023-04-03**|**Zero-shot Referring Image Segmentation with Global-Local Context Features**|Seonghoon Yu et.al.|[2303.17811](http://arxiv.org/abs/2303.17811)|**[link](https://github.com/seonghoon-yu/zero-shot-ris)**|\n", "2303.06345": "|**2023-03-11**|**Semantics-Aware Dynamic Localization and Refinement for Referring Image Segmentation**|Zhao Yang et.al.|[2303.06345](http://arxiv.org/abs/2303.06345)|null|\n", "2303.02153": "|**2023-03-03**|**Unleashing Text-to-Image Diffusion Models for Visual Perception**|Wenliang Zhao et.al.|[2303.02153](http://arxiv.org/abs/2303.02153)|**[link](https://github.com/wl-zhao/VPD)**|\n", "2302.07387": "|**2023-03-27**|**PolyFormer: Referring Image Segmentation as Sequential Polygon Generation**|Jiang Liu et.al.|[2302.07387](http://arxiv.org/abs/2302.07387)|**[link](https://github.com/amazon-science/polygon-transformer)**|\n", "2301.06429": "|**2023-03-22**|**Linguistic Query-Guided Mask Generation for Referring Image Segmentation**|Zhichao Wei et.al.|[2301.06429](http://arxiv.org/abs/2301.06429)|null|\n", "2212.13419": "|**2022-12-27**|**Position-Aware Contrastive Alignment for Referring Image Segmentation**|Bo Chen et.al.|[2212.13419](http://arxiv.org/abs/2212.13419)|null|\n", "2212.10278": "|**2022-12-17**|**Fully and Weakly Supervised Referring Expression Segmentation with End-to-End Learning**|Hui Li et.al.|[2212.10278](http://arxiv.org/abs/2212.10278)|null|\n", "2212.01769": "|**2022-12-04**|**CoupAlign: Coupling Word-Pixel with Sentence-Mask Alignments for Referring Image Segmentation**|Zicheng Zhang et.al.|[2212.01769](http://arxiv.org/abs/2212.01769)|null|\n", "2211.07919": "|**2022-11-15**|**A Unified Mutual Supervision Framework for Referring Expression Segmentation and Generation**|Shijia Huang et.al.|[2211.07919](http://arxiv.org/abs/2211.07919)|null|\n", "2209.10126": "|**2022-09-21**|**Exploring Modulated Detection Transformer as a Tool for Action Recognition in Videos**|Tom\u00e1s Crisol et.al.|[2209.10126](http://arxiv.org/abs/2209.10126)|**[link](https://github.com/bhi-research/ava_mdetr)**|\n", "2209.09554": "|**2023-07-23**|**Towards Robust Referring Image Segmentation**|Jianzong Wu et.al.|[2209.09554](http://arxiv.org/abs/2209.09554)|**[link](https://github.com/jianzongwu/robust-ref-seg)**|\n", "2205.04725": "|**2022-05-12**|**Weakly-supervised segmentation of referring expressions**|Robin Strudel et.al.|[2205.04725](http://arxiv.org/abs/2205.04725)|null|\n", "2203.16768": "|**2022-03-31**|**ReSTR: Convolution-free Referring Image Segmentation Using Transformers**|Namyup Kim et.al.|[2203.16768](http://arxiv.org/abs/2203.16768)|null|\n", "2112.13031": "|**2021-12-24**|**Grounding Linguistic Commands to Navigable Regions**|Nivedita Rufus et.al.|[2112.13031](http://arxiv.org/abs/2112.13031)|**[link](https://github.com/kanji95/Talk2car-Refseg)**|\n", "2112.10003": "|**2022-03-30**|**Image Segmentation Using Text and Image Prompts**|Timo L\u00fcddecke et.al.|[2112.10003](http://arxiv.org/abs/2112.10003)|**[link](https://github.com/timojl/clipseg)**|\n", "2112.02244": "|**2022-04-05**|**LAVT: Language-Aware Vision Transformer for Referring Image Segmentation**|Zhao Yang et.al.|[2112.02244](http://arxiv.org/abs/2112.02244)|**[link](https://github.com/yz93/lavt-ris)**|\n", "2111.15174": "|**2022-03-14**|**CRIS: CLIP-Driven Referring Image Segmentation**|Zhaoqing Wang et.al.|[2111.15174](http://arxiv.org/abs/2111.15174)|**[link](https://github.com/DerrickWang005/CRIS.pytorch)**|\n", "2111.10747": "|**2021-11-25**|**MaIL: A Unified Mask-Image-Language Trimodal Network for Referring Image Segmentation**|Zizhang Li et.al.|[2111.10747](http://arxiv.org/abs/2111.10747)|null|\n", "2110.04435": "|**2021-10-09**|**Two-stage Visual Cues Enhancement Network for Referring Image Segmentation**|Yang Jiao et.al.|[2110.04435](http://arxiv.org/abs/2110.04435)|**[link](https://github.com/sxjyjay/tv-net)**|\n", "2106.08617": "|**2021-06-16**|**CMF: Cascaded Multi-model Fusion for Referring Image Segmentation**|Jianhua Yang et.al.|[2106.08617](http://arxiv.org/abs/2106.08617)|**[link](https://github.com/jianhua2022/CMF-Refseg)**|\n", "2105.07175": "|**2021-05-15**|**Cross-Modal Progressive Comprehension for Referring Segmentation**|Si Liu et.al.|[2105.07175](http://arxiv.org/abs/2105.07175)|**[link](https://github.com/spyflying/CMPC-Refseg)**|\n", "2105.01839": "|**2021-05-05**|**Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation**|Guang Feng et.al.|[2105.01839](http://arxiv.org/abs/2105.01839)|null|\n", "2104.10412": "|**2022-08-14**|**Comprehensive Multi-Modal Interactions for Referring Image Segmentation**|Kanishk Jain et.al.|[2104.10412](http://arxiv.org/abs/2104.10412)|**[link](https://github.com/kanji95/SHNET)**|\n", "2103.16284": "|**2021-03-30**|**Locate then Segment: A Strong Pipeline for Referring Image Segmentation**|Ya Jing et.al.|[2103.16284](http://arxiv.org/abs/2103.16284)|null|\n", "2103.07679": "|**2021-04-14**|**OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene Grounding**|Ke-Jyun Wang et.al.|[2103.07679](http://arxiv.org/abs/2103.07679)|**[link](https://github.com/lluma/OCID-Ref)**|\n", "2010.00515": "|**2020-10-05**|**Linguistic Structure Guided Context Modeling for Referring Image Segmentation**|Tianrui Hui et.al.|[2010.00515](http://arxiv.org/abs/2010.00515)|**[link](https://github.com/spyflying/LSCM-Refseg)**|\n", "2010.00514": "|**2020-10-01**|**Referring Image Segmentation via Cross-Modal Progressive Comprehension**|Shaofei Huang et.al.|[2010.00514](http://arxiv.org/abs/2010.00514)|**[link](https://github.com/spyflying/CMPC-Refseg)**|\n", "2003.12739": "|**2022-06-23**|**Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters**|\u0130lker Kesen et.al.|[2003.12739](http://arxiv.org/abs/2003.12739)|**[link](https://github.com/ilkerkesen/bvpr)**|\n", "2001.11561": "|**2020-01-30**|**Dual Convolutional LSTM Network for Referring Image Segmentation**|Linwei Ye et.al.|[2001.11561](http://arxiv.org/abs/2001.11561)|null|\n", "1904.04745": "|**2019-04-09**|**Cross-Modal Self-Attention Network for Referring Image Segmentation**|Linwei Ye et.al.|[1904.04745](http://arxiv.org/abs/1904.04745)|null|\n", "1901.00850": "|**2019-04-06**|**CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions**|Runtao Liu et.al.|[1901.00850](http://arxiv.org/abs/1901.00850)|null|\n", "1703.07939": "|**2017-08-04**|**Recurrent Multimodal Interaction for Referring Image Segmentation**|Chenxi Liu et.al.|[1703.07939](http://arxiv.org/abs/1703.07939)|**[link](https://github.com/chenxi116/TF-phrasecut-public)**|\n"}, "REC": {"2412.11621": "|**2024-12-16**|**VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting**|Muhammet Furkan Ilaslan et.al.|[2412.11621](http://arxiv.org/abs/2412.11621)|**[link](https://github.com/mfurkanilaslan/vg-tvp)**|\n", "2412.11576": "|**2024-12-16**|**Aligning Visual and Semantic Interpretability through Visually Grounded Concept Bottleneck Models**|Patrick Knab et.al.|[2412.11576](http://arxiv.org/abs/2412.11576)|**[link](https://github.com/kathpra/gcbm)**|\n", "2412.10302": "|**2024-12-13**|**DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding**|Zhiyu Wu et.al.|[2412.10302](http://arxiv.org/abs/2412.10302)|**[link](https://github.com/deepseek-ai/deepseek-vl2)**|\n", "2412.09770": "|**2024-12-13**|**Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation**|Jonghyuk Park et.al.|[2412.09770](http://arxiv.org/abs/2412.09770)|**[link](https://github.com/jpstyle/ns-arch-unity)**|\n", "2412.08125": "|**2024-12-19**|**Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models**|Quang-Hung Le et.al.|[2412.08125](http://arxiv.org/abs/2412.08125)|**[link](https://github.com/lqh52/promvil)**|\n", "2412.08110": "|**2024-12-11**|**Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic Losses**|Jiayun Luo et.al.|[2412.08110](http://arxiv.org/abs/2412.08110)|null|\n", "2412.06613": "|**2024-12-09**|**3D Spatial Understanding in MLLMs: Disambiguation and Evaluation**|Chun-Peng Chang et.al.|[2412.06613](http://arxiv.org/abs/2412.06613)|null|\n", "2412.05479": "|**2024-12-10**|**TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action**|Zixian Ma et.al.|[2412.05479](http://arxiv.org/abs/2412.05479)|null|\n", "2412.05271": "|**2024-12-17**|**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**|Zhe Chen et.al.|[2412.05271](http://arxiv.org/abs/2412.05271)|**[link](https://github.com/opengvlab/internvl)**|\n", "2412.04383": "|**2024-12-05**|**SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding**|Rong Li et.al.|[2412.04383](http://arxiv.org/abs/2412.04383)|null|\n", "2412.04026": "|**2024-12-15**|**M$^{3}$D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction**|Jiang Liu et.al.|[2412.04026](http://arxiv.org/abs/2412.04026)|**[link](https://github.com/solkx/m3d)**|\n", "2412.00684": "|**2024-12-01**|**Paint Outside the Box: Synthesizing and Selecting Training Data for Visual Grounding**|Zilin Du et.al.|[2412.00684](http://arxiv.org/abs/2412.00684)|null|\n", "2411.18666": "|**2024-11-27**|**3D Scene Graph Guided Vision-Language Pre-training**|Hao Liu et.al.|[2411.18666](http://arxiv.org/abs/2411.18666)|null|\n", "2411.16198": "|**2024-11-25**|**Interpreting Object-level Foundation Models via Visual Precision Search**|Ruoyu Chen et.al.|[2411.16198](http://arxiv.org/abs/2411.16198)|null|\n", "2411.14869": "|**2024-11-27**|**BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence**|Xuewu Lin et.al.|[2411.14869](http://arxiv.org/abs/2411.14869)|**[link](https://github.com/HorizonRobotics/BIP3D)**|\n", "2411.14807": "|**2024-11-22**|**Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension**|Luca Parolari et.al.|[2411.14807](http://arxiv.org/abs/2411.14807)|null|\n", "2411.14594": "|**2024-11-21**|**Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction Problems**|Qihao Yuan et.al.|[2411.14594](http://arxiv.org/abs/2411.14594)|**[link](https://github.com/sunsleaf/csvg)**|\n", "2411.14137": "|**2024-11-21**|**Visual Contexts Clarify Ambiguous Expressions: A Benchmark Dataset**|Heejeong Nam et.al.|[2411.14137](http://arxiv.org/abs/2411.14137)|**[link](https://github.com/hazel-heejeong-nam/vague)**|\n", "2411.11904": "|**2024-11-16**|**GeoGround: A Unified Large Vision-Language Model. for Remote Sensing Visual Grounding**|Yue Zhou et.al.|[2411.11904](http://arxiv.org/abs/2411.11904)|**[link](https://github.com/zytx121/geoground)**|\n", "2411.09921": "|**2024-11-15**|**Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level**|Andong Deng et.al.|[2411.09921](http://arxiv.org/abs/2411.09921)|null|\n", "2411.04923": "|**2024-11-07**|**VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos**|Shehan Munasinghe et.al.|[2411.04923](http://arxiv.org/abs/2411.04923)|null|\n", "2411.04351": "|**2024-11-07**|**LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with Transformers**|Yeong-Seung Baek et.al.|[2411.04351](http://arxiv.org/abs/2411.04351)|null|\n", "2411.03405": "|**2024-11-05**|**Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding**|Sombit Dey et.al.|[2411.03405](http://arxiv.org/abs/2411.03405)|null|\n", "2410.23822": "|**2024-10-31**|**Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding**|Jinlong He et.al.|[2410.23822](http://arxiv.org/abs/2410.23822)|null|\n", "2410.23570": "|**2024-10-31**|**Phrase Decoupling Cross-Modal Hierarchical Matching and Progressive Position Correction for Visual Grounding**|Minghong Xie et.al.|[2410.23570](http://arxiv.org/abs/2410.23570)|**[link](https://github.com/X7J92/VGNet)**|\n", "2410.16163": "|**2024-10-21**|**Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models**|Yufei Zhan et.al.|[2410.16163](http://arxiv.org/abs/2410.16163)|**[link](https://github.com/jefferyzhan/griffon)**|\n", "2410.15615": "|**2024-10-21**|**Joint Top-Down and Bottom-Up Frameworks for 3D Visual Grounding**|Yang Liu et.al.|[2410.15615](http://arxiv.org/abs/2410.15615)|null|\n", "2410.13860": "|**2024-10-17**|**VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding**|Runsen Xu et.al.|[2410.13860](http://arxiv.org/abs/2410.13860)|**[link](https://github.com/openrobotlab/vlm-grounder)**|\n", "2410.13121": "|**2024-10-17**|**Trust but Verify: Programmatic VLM Evaluation in the Wild**|Viraj Prabhu et.al.|[2410.13121](http://arxiv.org/abs/2410.13121)|null|\n", "2410.12705": "|**2024-11-28**|**WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines**|Genta Indra Winata et.al.|[2410.12705](http://arxiv.org/abs/2410.12705)|**[link](https://github.com/worldcuisines/worldcuisines)**|\n", "2410.12694": "|**2024-10-16**|**VividMed: Vision Language Model with Versatile Visual Grounding for Medicine**|Lingxiao Luo et.al.|[2410.12694](http://arxiv.org/abs/2410.12694)|**[link](https://github.com/function2-llx/mmmm)**|\n", "2410.12369": "|**2024-10-16**|**Context-Infused Visual Grounding for Art**|Selina Khan et.al.|[2410.12369](http://arxiv.org/abs/2410.12369)|**[link](https://github.com/selinakhan/CIGAr)**|\n", "2410.12332": "|**2024-10-16**|**MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs**|Yunqiu Xu et.al.|[2410.12332](http://arxiv.org/abs/2410.12332)|null|\n", "2410.11623": "|**2024-10-15**|**VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI**|Sijie Cheng et.al.|[2410.11623](http://arxiv.org/abs/2410.11623)|null|\n", "2410.10491": "|**2024-10-14**|**Learning to Ground VLMs without Forgetting**|Aritra Bhowmik et.al.|[2410.10491](http://arxiv.org/abs/2410.10491)|null|\n", "2410.08257": "|**2024-10-10**|**Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics**|Junyi Cao et.al.|[2410.08257](http://arxiv.org/abs/2410.08257)|null|\n", "2410.06473": "|**2024-10-10**|**Grounding Robot Policies with Visuomotor Language Guidance**|Arthur Bucker et.al.|[2410.06473](http://arxiv.org/abs/2410.06473)|null|\n", "2410.06355": "|**2024-10-10**|**Context-Aware Command Understanding for Tabletop Scenarios**|Paul Gajewski et.al.|[2410.06355](http://arxiv.org/abs/2410.06355)|null|\n", "2410.05243": "|**2024-10-07**|**Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents**|Boyu Gou et.al.|[2410.05243](http://arxiv.org/abs/2410.05243)|**[link](https://github.com/OSU-NLP-Group/UGround)**|\n", "2410.05160": "|**2024-10-11**|**VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks**|Ziyan Jiang et.al.|[2410.05160](http://arxiv.org/abs/2410.05160)|null|\n", "2410.03161": "|**2024-10-04**|**Adaptive Masking Enhances Visual Grounding**|Sen Jia et.al.|[2410.03161](http://arxiv.org/abs/2410.03161)|null|\n", "2409.20424": "|**2024-09-30**|**World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering**|Jiacong Wang et.al.|[2409.20424](http://arxiv.org/abs/2409.20424)|**[link](https://github.com/foundation-multimodal-models/world2code)**|\n", "2409.18868": "|**2024-09-27**|**Individuation in Neural Models with and without Visual Grounding**|Alexey Tikhonov et.al.|[2409.18868](http://arxiv.org/abs/2409.18868)|null|\n", "2409.17610": "|**2024-10-29**|**ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue**|Zhangpu Li et.al.|[2409.17610](http://arxiv.org/abs/2409.17610)|null|\n", "2409.17531": "|**2024-10-28**|**SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion**|Ming Dai et.al.|[2409.17531](http://arxiv.org/abs/2409.17531)|**[link](https://github.com/dmmm1997/simvg)**|\n", "2409.17508": "|**2024-11-01**|**Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE**|Xun Zhu et.al.|[2409.17508](http://arxiv.org/abs/2409.17508)|**[link](https://github.com/tsinghua-msiip/uni-med)**|\n", "2409.14750": "|**2024-09-23**|**FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension**|Junzhuo Liu et.al.|[2409.14750](http://arxiv.org/abs/2409.14750)|**[link](https://github.com/liujunzhuo/FineCops-Ref)**|\n", "2409.13609": "|**2024-10-06**|**MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension**|Ting Liu et.al.|[2409.13609](http://arxiv.org/abs/2409.13609)|**[link](https://github.com/liuting20/mapper)**|\n", "2409.11919": "|**2024-10-15**|**LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension**|Amaia Cardiel et.al.|[2409.11919](http://arxiv.org/abs/2409.11919)|null|\n", "2409.11148": "|**2024-12-14**|**Improving the Efficiency of Visually Augmented Language Models**|Paula Ontalvilla et.al.|[2409.11148](http://arxiv.org/abs/2409.11148)|**[link](https://github.com/paulaonta/blind-valm)**|\n", "2409.10419": "|**2024-09-16**|**HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models**|Vineet Bhat et.al.|[2409.10419](http://arxiv.org/abs/2409.10419)|null|\n", "2409.08102": "|**2024-09-12**|**Bayesian Self-Training for Semi-Supervised 3D Segmentation**|Ozan Unal et.al.|[2409.08102](http://arxiv.org/abs/2409.08102)|null|\n", "2409.06013": "|**2024-09-09**|**Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings**|Leanne Nortje et.al.|[2409.06013](http://arxiv.org/abs/2409.06013)|**[link](https://github.com/LeanneNortje/low-resource_VPKL)**|\n", "2409.05721": "|**2024-09-09**|**Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding**|Bram Willemsen et.al.|[2409.05721](http://arxiv.org/abs/2409.05721)|**[link](https://github.com/willemsenbram/reg-with-guiding)**|\n", "2409.05395": "|**2024-10-01**|**Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling**|Georgios Pantazopoulos et.al.|[2409.05395](http://arxiv.org/abs/2409.05395)|**[link](https://github.com/gpantaz/vl_mamba)**|\n", "2409.04999": "|**2024-09-08**|**Visual Grounding with Multi-modal Conditional Adaptation**|Ruilin Yao et.al.|[2409.04999](http://arxiv.org/abs/2409.04999)|**[link](https://github.com/mr-bigworth/mmca)**|\n", "2409.03757": "|**2024-11-23**|**Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding**|Yunze Man et.al.|[2409.03757](http://arxiv.org/abs/2409.03757)|null|\n", "2409.03385": "|**2024-09-05**|**Make Graph-based Referring Expression Comprehension Great Again through Expression-guided Dynamic Gating and Regression**|Jingcheng Ke et.al.|[2409.03385](http://arxiv.org/abs/2409.03385)|null|\n", "2409.15310": "|**2024-09-05**|**Visual Prompting in Multimodal Large Language Models: A Survey**|Junda Wu et.al.|[2409.15310](http://arxiv.org/abs/2409.15310)|null|\n", "2409.02865": "|**2024-09-03**|**Visually Grounded Speech Models for Low-resource Languages and Cognitive Modelling**|Leanne Nortje et.al.|[2409.02865](http://arxiv.org/abs/2409.02865)|null|\n", "2409.01652": "|**2024-11-12**|**ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation**|Wenlong Huang et.al.|[2409.01652](http://arxiv.org/abs/2409.01652)|null|\n", "2409.01389": "|**2024-09-02**|**CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding**|Ivana Be\u0148ov\u00e1 et.al.|[2409.01389](http://arxiv.org/abs/2409.01389)|null|\n", "2408.17207": "|**2024-08-30**|**NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on Prompt-Guided Camera and 4D mmWave Radar**|Runwei Guan et.al.|[2408.17207](http://arxiv.org/abs/2408.17207)|null|\n", "2408.16314": "|**2024-08-29**|**ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding**|Minghang Zheng et.al.|[2408.16314](http://arxiv.org/abs/2408.16314)|**[link](https://github.com/minghangz/resvg)**|\n", "2408.16213": "|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213](http://arxiv.org/abs/2408.16213)|null|\n", "2408.12902": "|**2024-08-23**|**IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities**|Bin Wang et.al.|[2408.12902](http://arxiv.org/abs/2408.12902)|**[link](https://github.com/360cvgroup/inner-adaptor-architecture)**|\n", "2408.10787": "|**2024-10-22**|**A Lightweight Modular Framework for Low-Cost Open-Vocabulary Object Detection Training**|Bilal Faye et.al.|[2408.10787](http://arxiv.org/abs/2408.10787)|**[link](https://github.com/b-faye/lightmdetr)**|\n", "2408.07975": "|**2024-08-15**|**Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models**|Tianyu Wang et.al.|[2408.07975](http://arxiv.org/abs/2408.07975)|null|\n", "2408.05334": "|**2024-08-09**|**Revisiting Multi-Modal LLM Evaluation**|Jian Lu et.al.|[2408.05334](http://arxiv.org/abs/2408.05334)|null|\n", "2408.04961": "|**2024-08-09**|**In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation**|Dahyun Kang et.al.|[2408.04961](http://arxiv.org/abs/2408.04961)|**[link](https://github.com/dahyun-kang/lazygrounding)**|\n", "2408.04034": "|**2024-08-07**|**Task-oriented Sequential Grounding in 3D Scenes**|Zhuofan Zhang et.al.|[2408.04034](http://arxiv.org/abs/2408.04034)|null|\n", "2408.01942": "|**2024-08-04**|**Visual Grounding for Object-Level Generalization in Reinforcement Learning**|Haobin Jiang et.al.|[2408.01942](http://arxiv.org/abs/2408.01942)|**[link](https://github.com/pku-rl/copl)**|\n", "2408.01120": "|**2024-08-02**|**An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding**|Wei Chen et.al.|[2408.01120](http://arxiv.org/abs/2408.01120)|**[link](https://github.com/chenwei746/eevg)**|\n", "2407.20034": "|**2024-07-29**|**MaskInversion: Localized Embeddings via Optimization of Explainability Maps**|Walid Bousselham et.al.|[2407.20034](http://arxiv.org/abs/2407.20034)|null|\n", "2407.18391": "|**2024-07-25**|**UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models**|Xinyu Pi et.al.|[2407.18391](http://arxiv.org/abs/2407.18391)|null|\n", "2407.18244": "|**2024-07-25**|**RefMask3D: Language-Guided Transformer for 3D Referring Segmentation**|Shuting He et.al.|[2407.18244](http://arxiv.org/abs/2407.18244)|**[link](https://github.com/heshuting555/refmask3d)**|\n", "2407.16638": "|**2024-07-23**|**Unveiling and Mitigating Bias in Audio Visual Segmentation**|Peiwen Sun et.al.|[2407.16638](http://arxiv.org/abs/2407.16638)|null|\n", "2407.14491": "|**2024-09-02**|**PD-APE: A Parallel Decoding Framework with Adaptive Position Encoding for 3D Visual Grounding**|Chenshu Hou et.al.|[2407.14491](http://arxiv.org/abs/2407.14491)|null|\n", "2407.14563": "|**2024-07-18**|**Learning Visual Grounding from Generative Vision and Language Model**|Shijie Wang et.al.|[2407.14563](http://arxiv.org/abs/2407.14563)|null|\n", "2408.01432": "|**2024-11-28**|**VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance**|Divyansh Srivastava et.al.|[2408.01432](http://arxiv.org/abs/2408.01432)|**[link](https://github.com/trustworthy-ml-lab/vlg-cbm)**|\n", "2407.13642": "|**2024-07-18**|**Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models**|Xiaoyu Zhu et.al.|[2407.13642](http://arxiv.org/abs/2407.13642)|null|\n", "2407.11393": "|**2024-07-17**|**CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**|Kalliopi Basioti et.al.|[2407.11393](http://arxiv.org/abs/2407.11393)|**[link](https://github.com/SamsungLabs/CIC-BART-SSA)**|\n", "2407.08693": "|**2024-07-12**|**Robotic Control via Embodied Chain-of-Thought Reasoning**|Micha\u0142 Zawalski et.al.|[2407.08693](http://arxiv.org/abs/2407.08693)|null|\n", "2407.06304": "|**2024-07-08**|**VIMI: Grounding Video Generation through Multi-modal Instruction**|Yuwei Fang et.al.|[2407.06304](http://arxiv.org/abs/2407.06304)|null|\n", "2407.06084": "|**2024-07-08**|**3D Vision and Language Pretraining with Large-Scale Synthetic Data**|Dejie Yang et.al.|[2407.06084](http://arxiv.org/abs/2407.06084)|**[link](https://github.com/idejie/3DSyn)**|\n", "2407.05578": "|**2024-08-21**|**FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance**|Jiedong Zhuang et.al.|[2407.05578](http://arxiv.org/abs/2407.05578)|null|\n", "2407.05363": "|**2024-07-10**|**Multi-branch Collaborative Learning Network for 3D Visual Grounding**|Zhipeng Qian et.al.|[2407.05363](http://arxiv.org/abs/2407.05363)|**[link](https://github.com/qzp2018/MCLN)**|\n", "2407.05352": "|**2024-07-07**|**Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model**|Danni Yang et.al.|[2407.05352](http://arxiv.org/abs/2407.05352)|**[link](https://github.com/nini0919/diffpng)**|\n", "2407.04998": "|**2024-07-06**|**The Solution for the 5th GCAIAC Zero-shot Referring Expression Comprehension Challenge**|Longfei Huang et.al.|[2407.04998](http://arxiv.org/abs/2407.04998)|null|\n", "2407.04559": "|**2024-10-25**|**Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition**|Aditya K Surikuchi et.al.|[2407.04559](http://arxiv.org/abs/2407.04559)|**[link](https://github.com/akskuchi/dhm-visual-storytelling)**|\n", "2407.04255": "|**2024-07-05**|**Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge**|Xiangyu Wu et.al.|[2407.04255](http://arxiv.org/abs/2407.04255)|null|\n", "2407.04212": "|**2024-07-05**|**Smart Vision-Language Reasoners**|Denisa Roberts et.al.|[2407.04212](http://arxiv.org/abs/2407.04212)|**[link](https://github.com/smarter-vlm/smarter)**|\n", "2407.03251": "|**2024-07-06**|**ACTRESS: Active Retraining for Semi-supervised Visual Grounding**|Weitai Kang et.al.|[2407.03251](http://arxiv.org/abs/2407.03251)|null|\n", "2407.03243": "|**2024-07-06**|**Visual Grounding with Attention-Driven Constraint Balancing**|Weitai Kang et.al.|[2407.03243](http://arxiv.org/abs/2407.03243)|null|\n", "2407.03200": "|**2024-07-06**|**SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding**|Weitai Kang et.al.|[2407.03200](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|\n", "2407.01996": "|**2024-08-04**|**ViG-Bias: Visually Grounded Bias Discovery and Mitigation**|Badr-Eddine Marani et.al.|[2407.01996](http://arxiv.org/abs/2407.01996)|null|\n", "2407.01907": "|**2024-07-02**|**The Solution for the ICCV 2023 Perception Test Challenge 2023 -- Task 6 -- Grounded videoQA**|Hailiang Zhang et.al.|[2407.01907](http://arxiv.org/abs/2407.01907)|null|\n", "2407.01525": "|**2024-07-17**|**ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities**|Chenming Zhu et.al.|[2407.01525](http://arxiv.org/abs/2407.01525)|null|\n", "2407.01131": "|**2024-10-29**|**M$^2$IST: Multi-Modal Interactive Side-Tuning for Efficient Referring Expression Comprehension**|Xuyang Liu et.al.|[2407.01131](http://arxiv.org/abs/2407.01131)|null|\n", "2407.01081": "|**2024-07-01**|**CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation**|Yuxuan Wang et.al.|[2407.01081](http://arxiv.org/abs/2407.01081)|**[link](https://github.com/WangYuxuan93/CVLUE)**|\n", "2407.00263": "|**2024-06-28**|**From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models**|Mehar Bhatia et.al.|[2407.00263](http://arxiv.org/abs/2407.00263)|null|\n", "2406.19237": "|**2024-06-28**|**FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts**|Shubhankar Singh et.al.|[2406.19237](http://arxiv.org/abs/2406.19237)|null|\n", "2406.19057": "|**2024-06-30**|**Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO**|Fuseini Mumuni et.al.|[2406.19057](http://arxiv.org/abs/2406.19057)|null|\n", "2406.18722": "|**2024-10-13**|**Towards Open-World Grasping with Large Vision-Language Models**|Georgios Tziafas et.al.|[2406.18722](http://arxiv.org/abs/2406.18722)|null|\n", "2406.18253": "|**2024-06-26**|**On the Role of Visual Grounding in VQA**|Daniel Reich et.al.|[2406.18253](http://arxiv.org/abs/2406.18253)|null|\n", "2406.18048": "|**2024-06-26**|**ScanFormer: Referring Expression Comprehension by Iteratively Scanning**|Wei Su et.al.|[2406.18048](http://arxiv.org/abs/2406.18048)|null|\n", "2406.16866": "|**2024-06-24**|**Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models**|Jierun Chen et.al.|[2406.16866](http://arxiv.org/abs/2406.16866)|**[link](https://github.com/jierunchen/ref-l4)**|\n", "2406.16860": "|**2024-12-04**|**Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs**|Shengbang Tong et.al.|[2406.16860](http://arxiv.org/abs/2406.16860)|**[link](https://github.com/cambrian-mllm/cambrian)**|\n", "2406.12718": "|**2024-06-21**|**AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**|Wenbin An et.al.|[2406.12718](http://arxiv.org/abs/2406.12718)|**[link](https://github.com/lackel/agla)**|\n", "2406.12384": "|**2024-11-11**|**VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding**|Xiang Li et.al.|[2406.12384](http://arxiv.org/abs/2406.12384)|**[link](https://github.com/lx709/vrsbench)**|\n", "2406.11977": "|**2024-06-17**|**Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models**|Eva Portelance et.al.|[2406.11977](http://arxiv.org/abs/2406.11977)|null|\n", "2406.11633": "|**2024-09-11**|**DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models**|Renqiu Xia et.al.|[2406.11633](http://arxiv.org/abs/2406.11633)|**[link](https://github.com/UniModal4Reasoning/DocGenome)**|\n", "2406.09961": "|**2024-06-14**|**ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation**|Chufan Shi et.al.|[2406.09961](http://arxiv.org/abs/2406.09961)|**[link](https://github.com/chartmimic/chartmimic)**|\n", "2406.09662": "|**2024-10-21**|**Learning Language Structures through Grounding**|Freda Shi et.al.|[2406.09662](http://arxiv.org/abs/2406.09662)|null|\n", "2406.09401": "|**2024-06-13**|**MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations**|Ruiyuan Lyu et.al.|[2406.09401](http://arxiv.org/abs/2406.09401)|**[link](https://github.com/openrobotlab/embodiedscan)**|\n", "2406.09385": "|**2024-06-13**|**Towards Vision-Language Geo-Foundation Model: A Survey**|Yue Zhou et.al.|[2406.09385](http://arxiv.org/abs/2406.09385)|**[link](https://github.com/zytx121/awesome-vlgfm)**|\n", "2406.08907": "|**2024-06-13**|**Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding**|Yue Xu et.al.|[2406.08907](http://arxiv.org/abs/2406.08907)|null|\n", "2406.07268": "|**2024-06-11**|**Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation**|Jinyuan Li et.al.|[2406.07268](http://arxiv.org/abs/2406.07268)|**[link](https://github.com/JinYuanLi0012/RiVEG)**|\n", "2406.07133": "|**2024-06-11**|**Translating speech with just images**|Dan Oneata et.al.|[2406.07133](http://arxiv.org/abs/2406.07133)|**[link](https://github.com/danoneata/strim)**|\n", "2406.05821": "|**2024-06-09**|**F-LMM: Grounding Frozen Large Multimodal Models**|Size Wu et.al.|[2406.05821](http://arxiv.org/abs/2406.05821)|**[link](https://github.com/wusize/f-lmm)**|\n", "2406.05785": "|**2024-07-22**|**A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances, and Future Directions**|Daizong Liu et.al.|[2406.05785](http://arxiv.org/abs/2406.05785)|**[link](https://github.com/liudaizong/awesome-3d-visual-grounding)**|\n", "2406.05629": "|**2024-06-09**|**Separating the \"Chirp\" from the \"Chat\": Self-supervised Visual Grounding of Sound and Language**|Mark Hamilton et.al.|[2406.05629](http://arxiv.org/abs/2406.05629)|**[link](https://github.com/mhamilton723/DenseAV)**|\n", "2406.00980": "|**2024-06-03**|**Selectively Answering Visual Questions**|Julian Martin Eisenschlos et.al.|[2406.00980](http://arxiv.org/abs/2406.00980)|null|\n", "2406.00307": "|**2024-11-01**|**HENASY: Learning to Assemble Scene-Entities for Egocentric Video-Language Model**|Khoa Vo et.al.|[2406.00307](http://arxiv.org/abs/2406.00307)|null|\n", "2405.19783": "|**2024-10-16**|**Instruction-Guided Visual Masking**|Jinliang Zheng et.al.|[2405.19783](http://arxiv.org/abs/2405.19783)|**[link](https://github.com/2toinf/ivm)**|\n", "2405.18415": "|**2024-11-03**|**Why are Visually-Grounded Language Models Bad at Image Classification?**|Yuhui Zhang et.al.|[2405.18415](http://arxiv.org/abs/2405.18415)|**[link](https://github.com/yuhui-zh15/vlmclassifier)**|\n", "2405.18295": "|**2024-07-06**|**Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention**|Weitai Kang et.al.|[2405.18295](http://arxiv.org/abs/2405.18295)|null|\n", "2405.17104": "|**2024-05-28**|**LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding**|Haoyu Zhao et.al.|[2405.17104](http://arxiv.org/abs/2405.17104)|null|\n", "2405.16919": "|**2024-05-28**|**VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models**|Zejun Li et.al.|[2405.16919](http://arxiv.org/abs/2405.16919)|**[link](https://github.com/rupertluo/vocot)**|\n", "2405.15274": "|**2024-05-24**|**Talk to Parallel LiDARs: A Human-LiDAR Interaction Method Based on 3D Visual Grounding**|Yuhang Liu et.al.|[2405.15274](http://arxiv.org/abs/2405.15274)|null|\n", "2405.14113": "|**2024-05-23**|**Multi-modality Regional Alignment Network for Covid X-Ray Survival Prediction and Report Generation**|Zhusi Zhong et.al.|[2405.14113](http://arxiv.org/abs/2405.14113)|**[link](https://github.com/zzs95/mranet)**|\n", "2405.12821": "|**2024-07-19**|**Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension**|Runwei Guan et.al.|[2405.12821](http://arxiv.org/abs/2405.12821)|**[link](https://github.com/guanrunwei/talk2radar)**|\n", "2405.09981": "|**2024-05-16**|**Adversarial Robustness for Visual Grounding of Multimodal Large Language Models**|Kuofeng Gao et.al.|[2405.09981](http://arxiv.org/abs/2405.09981)|**[link](https://github.com/KuofengGao/MLLM-Grounding-Robustness)**|\n", "2405.07065": "|**2024-05-11**|**LogoMotion: Visually Grounded Code Generation for Content-Aware Animation**|Vivian Liu et.al.|[2405.07065](http://arxiv.org/abs/2405.07065)|null|\n", "2405.06217": "|**2024-06-08**|**DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding**|Ting Liu et.al.|[2405.06217](http://arxiv.org/abs/2405.06217)|**[link](https://github.com/liuting20/dara)**|\n", "2407.01558": "|**2024-09-17**|**Visual grounding for desktop graphical user interfaces**|Tassnim Dardouri et.al.|[2407.01558](http://arxiv.org/abs/2407.01558)|null|\n", "2404.19696": "|**2024-04-30**|**Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners**|Chun Feng et.al.|[2404.19696](http://arxiv.org/abs/2404.19696)|null|\n", "2404.19128": "|**2024-04-29**|**Q-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM**|Navid Rajabi et.al.|[2404.19128](http://arxiv.org/abs/2404.19128)|null|\n", "2404.17672": "|**2024-08-02**|**BlenderAlchemy: Editing 3D Graphics with Vision-Language Models**|Ian Huang et.al.|[2404.17672](http://arxiv.org/abs/2404.17672)|null|\n", "2404.16375": "|**2024-04-25**|**List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs**|An Yan et.al.|[2404.16375](http://arxiv.org/abs/2404.16375)|**[link](https://github.com/zzxslp/som-llava)**|\n", "2404.15770": "|**2024-07-15**|**ChEX: Interactive Localization and Region Description in Chest X-rays**|Philip M\u00fcller et.al.|[2404.15770](http://arxiv.org/abs/2404.15770)|**[link](https://github.com/philip-mueller/chex)**|\n", "2404.15190": "|**2024-04-21**|**Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following**|Suyeon Shin et.al.|[2404.15190](http://arxiv.org/abs/2404.15190)|null|\n", "2404.13400": "|**2024-09-05**|**HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual Grounding**|Linhui Xiao et.al.|[2404.13400](http://arxiv.org/abs/2404.13400)|**[link](https://github.com/linhuixiao/hivg)**|\n", "2404.13013": "|**2024-04-19**|**Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models**|Chuofan Ma et.al.|[2404.13013](http://arxiv.org/abs/2404.13013)|**[link](https://github.com/FoundationVision/Groma)**|\n", "2404.11064": "|**2024-12-18**|**Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework through Prompt-based Localization**|Yongdong Luo et.al.|[2404.11064](http://arxiv.org/abs/2404.11064)|**[link](https://github.com/leon1207/3dgctr)**|\n", "2404.06798": "|**2024-04-10**|**MedRG: Medical Report Grounding with Multi-modal Large Language Model**|Ke Zou et.al.|[2404.06798](http://arxiv.org/abs/2404.06798)|null|\n", "2404.02523": "|**2024-04-03**|**Text-driven Affordance Learning from Egocentric Vision**|Tomoya Yoshida et.al.|[2404.02523](http://arxiv.org/abs/2404.02523)|null|\n", "2403.20213": "|**2024-12-19**|**VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis**|Chao Pang et.al.|[2403.20213](http://arxiv.org/abs/2403.20213)|**[link](https://github.com/opendatalab/vhm)**|\n", "2403.16921": "|**2024-07-22**|**PropTest: Automatic Property Testing for Improved Visual Programming**|Jaywon Koo et.al.|[2403.16921](http://arxiv.org/abs/2403.16921)|null|\n", "2403.16539": "|**2024-12-04**|**Data-Efficient 3D Visual Grounding via Order-Aware Referring**|Tung-Yu Wu et.al.|[2403.16539](http://arxiv.org/abs/2403.16539)|null|\n", "2403.15585": "|**2024-03-29**|**MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis**|Mai A. Shaaban et.al.|[2403.15585](http://arxiv.org/abs/2403.15585)|**[link](https://github.com/biomedia-mbzuai/medpromptx)**|\n", "2403.15054": "|**2024-10-06**|**Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping**|Pengwei Xie et.al.|[2403.15054](http://arxiv.org/abs/2403.15054)|null|\n", "2405.10948": "|**2024-03-22**|**Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery**|Guankun Wang et.al.|[2405.10948](http://arxiv.org/abs/2405.10948)|null|\n", "2403.14870": "|**2024-03-21**|**VidLA: Video-Language Alignment at Scale**|Mamshad Nayeem Rizve et.al.|[2403.14870](http://arxiv.org/abs/2403.14870)|null|\n", "2403.14551": "|**2024-03-21**|**Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling**|Chengxu Zhuang et.al.|[2403.14551](http://arxiv.org/abs/2403.14551)|null|\n", "2403.13922": "|**2024-03-20**|**Visually Grounded Speech Models have a Mutual Exclusivity Bias**|Leanne Nortje et.al.|[2403.13922](http://arxiv.org/abs/2403.13922)|null|\n", "2403.13804": "|**2024-12-16**|**Learning from Synthetic Data for Visual Grounding**|Ruozhen He et.al.|[2403.13804](http://arxiv.org/abs/2403.13804)|null|\n", "2403.12686": "|**2024-04-05**|**WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar**|Runwei Guan et.al.|[2403.12686](http://arxiv.org/abs/2403.12686)|null|\n", "2403.12488": "|**2024-07-23**|**DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM**|Yixuan Wu et.al.|[2403.12488](http://arxiv.org/abs/2403.12488)|**[link](https://github.com/yixuan730/DetToolChain)**|\n", "2403.08182": "|**2024-03-13**|**SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention**|Feng Xiao et.al.|[2403.08182](http://arxiv.org/abs/2403.08182)|null|\n", "2403.03077": "|**2024-12-01**|**MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding**|Chun-Peng Chang et.al.|[2403.03077](http://arxiv.org/abs/2403.03077)|**[link](https://github.com/dfki-av/mikasa-3dvg)**|\n", "2403.03075": "|**2024-03-05**|**Detecting Concrete Visual Tokens for Multimodal Machine Translation**|Braeden Bowen et.al.|[2403.03075](http://arxiv.org/abs/2403.03075)|null|\n", "2403.02330": "|**2024-03-04**|**RegionGPT: Towards Region Understanding Vision Language Model**|Qiushan Guo et.al.|[2403.02330](http://arxiv.org/abs/2403.02330)|null|\n", "2403.02325": "|**2024-03-04**|**Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training**|David Wan et.al.|[2403.02325](http://arxiv.org/abs/2403.02325)|null|\n", "2403.01118": "|**2024-03-02**|**Adversarial Testing for Visual Grounding via Image-Aware Property Reduction**|Zhiyuan Chang et.al.|[2403.01118](http://arxiv.org/abs/2403.01118)|null|\n", "2402.18400": "|**2024-06-27**|**Towards Alleviating Text-to-Image Retrieval Hallucination for CLIP in Zero-shot Learning**|Hanyao Wang et.al.|[2402.18400](http://arxiv.org/abs/2402.18400)|null|\n", "2402.17766": "|**2024-07-12**|**ShapeLLM: Universal 3D Object Understanding for Embodied Interaction**|Zekun Qi et.al.|[2402.17766](http://arxiv.org/abs/2402.17766)|**[link](https://github.com/qizekun/ShapeLLM)**|\n", "2402.17553": "|**2024-07-21**|**OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web**|Raghav Kapoor et.al.|[2402.17553](http://arxiv.org/abs/2402.17553)|null|\n", "2402.15300": "|**2024-04-23**|**Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding**|Ailin Deng et.al.|[2402.15300](http://arxiv.org/abs/2402.15300)|**[link](https://github.com/d-ailin/clip-guided-decoding)**|\n", "2402.12451": "|**2024-06-06**|**The Revolution of Multimodal Large Language Models: A Survey**|Davide Caffagni et.al.|[2402.12451](http://arxiv.org/abs/2402.12451)|null|\n", "2402.11792": "|**2024-02-20**|**SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction**|Jie Xu et.al.|[2402.11792](http://arxiv.org/abs/2402.11792)|null|\n", "2402.11265": "|**2024-05-24**|**Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions**|Wenxuan Wang et.al.|[2402.11265](http://arxiv.org/abs/2402.11265)|**[link](https://github.com/rubics-xuan/ivg)**|\n", "2402.09989": "|**2024-05-29**|**LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition**|Jinyuan Li et.al.|[2402.09989](http://arxiv.org/abs/2402.09989)|**[link](https://github.com/jinyuanli0012/pgim)**|\n", "2402.08183": "|**2024-02-13**|**Pixel Sentence Representation Learning**|Chenghao Xiao et.al.|[2402.08183](http://arxiv.org/abs/2402.08183)|**[link](https://github.com/gowitheflow-1998/pixel-linguist)**|\n", "2402.06959": "|**2024-02-10**|**SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data**|Hsuan-Fu Wang et.al.|[2402.06959](http://arxiv.org/abs/2402.06959)|**[link](https://github.com/ShampooWang/SpeechCLIP_plus)**|\n", "2402.06118": "|**2024-10-13**|**ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling**|Siming Yan et.al.|[2402.06118](http://arxiv.org/abs/2402.06118)|**[link](https://github.com/amazon-science/vigor)**|\n", "2402.05819": "|**2024-02-08**|**Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model**|Hung-Chieh Fang et.al.|[2402.05819](http://arxiv.org/abs/2402.05819)|null|\n", "2402.00319": "|**2024-02-01**|**SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling**|Eileen Wang et.al.|[2402.00319](http://arxiv.org/abs/2402.00319)|null|\n", "2401.16699": "|**2024-02-18**|**Towards Unified Interactive Visual Grounding in The Wild**|Jie Xu et.al.|[2401.16699](http://arxiv.org/abs/2401.16699)|null|\n", "2401.15842": "|**2024-03-23**|**LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answering**|Yuhan Chen et.al.|[2401.15842](http://arxiv.org/abs/2401.15842)|null|\n", "2401.13307": "|**2024-01-24**|**ChatterBox: Multi-round Multimodal Referring and Grounding**|Yunjie Tian et.al.|[2401.13307](http://arxiv.org/abs/2401.13307)|**[link](https://github.com/sunsmarterjie/chatterbox)**|\n", "2401.11228": "|**2024-01-20**|**Unifying Visual and Vision-Language Tracking via Contrastive Learning**|Yinchao Ma et.al.|[2401.11228](http://arxiv.org/abs/2401.11228)|**[link](https://github.com/openspaceai/uvltrack)**|\n", "2403.08773": "|**2024-10-27**|**Veagle: Advancements in Multimodal Representation Learning**|Rajat Chawla et.al.|[2403.08773](http://arxiv.org/abs/2403.08773)|**[link](https://github.com/superagi/veagle)**|\n", "2401.09712": "|**2024-01-18**|**SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**|Yang Zhan et.al.|[2401.09712](http://arxiv.org/abs/2401.09712)|**[link](https://github.com/zhanyang-nwpu/skyeyegpt)**|\n", "2401.09340": "|**2024-09-24**|**SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding**|Baoxiong Jia et.al.|[2401.09340](http://arxiv.org/abs/2401.09340)|null|\n", "2401.07803": "|**2024-02-15**|**Uncovering the Full Potential of Visual Grounding Methods in VQA**|Daniel Reich et.al.|[2401.07803](http://arxiv.org/abs/2401.07803)|**[link](https://github.com/dreichcsl/truevg)**|\n", "2401.06209": "|**2024-04-25**|**Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs**|Shengbang Tong et.al.|[2401.06209](http://arxiv.org/abs/2401.06209)|**[link](https://github.com/tsb0601/MMVP)**|\n", "2401.02361": "|**2024-01-05**|**An Open and Comprehensive Pipeline for Unified Object Grounding and Detection**|Xiangyu Zhao et.al.|[2401.02361](http://arxiv.org/abs/2401.02361)|**[link](https://github.com/open-mmlab/mmdetection)**|\n", "2401.01974": "|**2024-05-14**|**Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers**|Aleksandar Stani\u0107 et.al.|[2401.01974](http://arxiv.org/abs/2401.01974)|null|\n", "2312.17648": "|**2024-07-06**|**Bridging Modality Gap for Visual Grounding with Effecitve Cross-modal Distillation**|Jiaxi Wang et.al.|[2312.17648](http://arxiv.org/abs/2312.17648)|null|\n", "2312.17183": "|**2024-07-11**|**One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts**|Ziheng Zhao et.al.|[2312.17183](http://arxiv.org/abs/2312.17183)|**[link](https://github.com/zhaoziheng/sat-ds)**|\n", "2312.15162": "|**2023-12-23**|**Cycle-Consistency Learning for Captioning and Grounding**|Ning Wang et.al.|[2312.15162](http://arxiv.org/abs/2312.15162)|null|\n", "2312.15043": "|**2023-12-22**|**GroundVLP: Harnessing Zero-shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection**|Haozhan Shen et.al.|[2312.15043](http://arxiv.org/abs/2312.15043)|**[link](https://github.com/om-ai-lab/groundvlp)**|\n", "2312.13655": "|**2023-12-21**|**Compositional Zero-Shot Learning for Attribute-Based Object Reference in Human-Robot Interaction**|Peng Gao et.al.|[2312.13655](http://arxiv.org/abs/2312.13655)|null|\n", "2312.12198": "|**2024-03-25**|**Mask Grounding for Referring Image Segmentation**|Yong Xien Chng et.al.|[2312.12198](http://arxiv.org/abs/2312.12198)|**[link](https://github.com/yxchng/mask-grounding)**|\n", "2312.11967": "|**2023-12-19**|**Context Disentangling and Prototype Inheriting for Robust Visual Grounding**|Wei Tang et.al.|[2312.11967](http://arxiv.org/abs/2312.11967)|**[link](https://github.com/waynetomas/transcp)**|\n", "2312.09625": "|**2024-08-30**|**Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment**|Xiaoxu Xu et.al.|[2312.09625](http://arxiv.org/abs/2312.09625)|null|\n", "2312.08022": "|**2023-12-13**|**Mono3DVG: 3D Visual Grounding in Monocular Images**|Yang Zhan et.al.|[2312.08022](http://arxiv.org/abs/2312.08022)|**[link](https://github.com/zhanyang-nwpu/mono3dvg)**|\n", "2312.08007": "|**2024-03-21**|**Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring Expression Segmentation**|Wenxuan Wang et.al.|[2312.08007](http://arxiv.org/abs/2312.08007)|**[link](https://github.com/rubics-xuan/mres)**|\n", "2312.04794": "|**2023-12-08**|**Visual Grounding of Whole Radiology Reports for 3D CT Images**|Akimichi Ichinose et.al.|[2312.04794](http://arxiv.org/abs/2312.04794)|null|\n", "2312.04554": "|**2023-12-07**|**Improved Visual Grounding through Self-Consistent Explanations**|Ruozhen He et.al.|[2312.04554](http://arxiv.org/abs/2312.04554)|null|\n", "2312.03543": "|**2023-12-06**|**GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models**|Haicheng Liao et.al.|[2312.03543](http://arxiv.org/abs/2312.03543)|**[link](https://github.com/petrichor625/talk2car_cavg)**|\n", "2312.03766": "|**2024-07-17**|**Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment**|Brian Gordon et.al.|[2312.03766](http://arxiv.org/abs/2312.03766)|**[link](https://github.com/mismatchquest/mismatchquest)**|\n", "2312.03026": "|**2023-12-05**|**Uni3DL: Unified Model for 3D and Language Understanding**|Xiang Li et.al.|[2312.03026](http://arxiv.org/abs/2312.03026)|null|\n", "2312.02431": "|**2023-12-05**|**Visually Grounded Language Learning: a review of language games, datasets, tasks, and models**|Alessandro Suglia et.al.|[2312.02431](http://arxiv.org/abs/2312.02431)|null|\n", "2312.02153": "|**2023-12-04**|**Aligning and Prompting Everything All at Once for Universal Visual Perception**|Yunhang Shen et.al.|[2312.02153](http://arxiv.org/abs/2312.02153)|**[link](https://github.com/shenyunhang/ape)**|\n", "2312.02103": "|**2023-12-04**|**Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object Detection**|Sunghun Kang et.al.|[2312.02103](http://arxiv.org/abs/2312.02103)|null|\n", "2312.01592": "|**2024-01-09**|**Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment**|Cong-Duy Nguyen et.al.|[2312.01592](http://arxiv.org/abs/2312.01592)|null|\n", "2312.01522": "|**2024-10-24**|**G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training**|Che Liu et.al.|[2312.01522](http://arxiv.org/abs/2312.01522)|**[link](https://github.com/cheliu-computation/g2d-neurips24)**|\n", "2312.02219": "|**2024-06-12**|**Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models**|Andr\u00e9s Villa et.al.|[2312.02219](http://arxiv.org/abs/2312.02219)|**[link](https://github.com/ojedaf/merlim)**|\n", "2311.17048": "|**2024-04-09**|**Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions**|Zeyu Han et.al.|[2311.17048](http://arxiv.org/abs/2311.17048)|**[link](https://github.com/show-han/zeroshot_rec)**|\n", "2311.15383": "|**2024-03-23**|**Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding**|Zhihao Yuan et.al.|[2311.15383](http://arxiv.org/abs/2311.15383)|**[link](https://github.com/CurryYuan/ZSVG3D)**|\n", "2311.16501": "|**2024-08-11**|**Context-Aware Indoor Point Cloud Object Generation through User Instructions**|Yiyang Luo et.al.|[2311.16501](http://arxiv.org/abs/2311.16501)|null|\n", "2311.14909": "|**2023-11-25**|**Continual Referring Expression Comprehension via Dual Modular Memorization**|Heng Tao Shen et.al.|[2311.14909](http://arxiv.org/abs/2311.14909)|**[link](https://github.com/zackschen/DMM)**|\n", "2311.15826": "|**2023-11-24**|**GeoChat: Grounded Large Vision-Language Model for Remote Sensing**|Kartik Kuckreja et.al.|[2311.15826](http://arxiv.org/abs/2311.15826)|**[link](https://github.com/mbzuai-oryx/geochat)**|\n", "2311.12327": "|**2024-04-26**|**Enhancing Visual Grounding and Generalization: A Multi-Task Cycle Training Approach for Vision-Language Models**|Xiaoyu Yang et.al.|[2311.12327](http://arxiv.org/abs/2311.12327)|**[link](https://github.com/anonymgiant/vilam)**|\n", "2311.06791": "|**2023-12-06**|**InfMLLM: A Unified Framework for Visual-Language Tasks**|Qiang Zhou et.al.|[2311.06791](http://arxiv.org/abs/2311.06791)|**[link](https://github.com/mightyzau/infmllm)**|\n", "2311.05779": "|**2023-11-09**|**Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter**|Georgios Tziafas et.al.|[2311.05779](http://arxiv.org/abs/2311.05779)|**[link](https://github.com/gtziafas/ocid-vlg)**|\n", "2311.04901": "|**2023-11-08**|**GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs**|Zhenfang Chen et.al.|[2311.04901](http://arxiv.org/abs/2311.04901)|null|\n", "2311.04498": "|**2023-12-18**|**NExT-Chat: An LMM for Chat, Detection and Segmentation**|Ao Zhang et.al.|[2311.04498](http://arxiv.org/abs/2311.04498)|**[link](https://github.com/next-chatv/next-chat)**|\n", "2311.03356": "|**2024-06-02**|**GLaMM: Pixel Grounding Large Multimodal Model**|Hanoona Rasheed et.al.|[2311.03356](http://arxiv.org/abs/2311.03356)|**[link](https://github.com/mbzuai-oryx/groundingLMM)**|\n", "2311.03354": "|**2023-11-06**|**CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding**|Junyan Li et.al.|[2311.03354](http://arxiv.org/abs/2311.03354)|null|\n", "2311.02612": "|**2024-04-16**|**GPT-4V-AD: Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection**|Jiangning Zhang et.al.|[2311.02612](http://arxiv.org/abs/2311.02612)|**[link](https://github.com/zhangzjn/gpt-4v-ad)**|\n", "2311.01580": "|**2023-11-02**|**MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition**|Guangyue Xu et.al.|[2311.01580](http://arxiv.org/abs/2311.01580)|null|\n", "2310.20381": "|**2024-01-30**|**A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis**|Yingshu Li et.al.|[2310.20381](http://arxiv.org/abs/2310.20381)|null|\n", "2310.19644": "|**2023-10-30**|**Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction**|Zexu Pan et.al.|[2310.19644](http://arxiv.org/abs/2310.19644)|null|\n", "2310.18773": "|**2023-10-28**|**CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data**|Taiki Miyanishi et.al.|[2310.18773](http://arxiv.org/abs/2310.18773)|**[link](https://github.com/atr-dbi/cityrefer)**|\n", "2310.17770": "|**2023-10-26**|**GROOViST: A Metric for Grounding Objects in Visual Storytelling**|Aditya K Surikuchi et.al.|[2310.17770](http://arxiv.org/abs/2310.17770)|**[link](https://github.com/akskuchi/groovist)**|\n", "2310.16616": "|**2023-10-25**|**Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network**|Yiming Lin et.al.|[2310.16616](http://arxiv.org/abs/2310.16616)|null|\n", "2310.16402": "|**2023-10-25**|**Video Referring Expression Comprehension via Transformer with Content-conditioned Query**|Ji Jiang et.al.|[2310.16402](http://arxiv.org/abs/2310.16402)|null|\n", "2310.15571": "|**2023-10-24**|**Visually Grounded Continual Language Learning with Selective Specialization**|Kyra Ahrens et.al.|[2310.15571](http://arxiv.org/abs/2310.15571)|**[link](https://github.com/ky-ah/selective-lilac)**|\n", "2310.14374": "|**2023-10-22**|**OV-VG: A Benchmark for Open-Vocabulary Visual Grounding**|Chunlei Wang et.al.|[2310.14374](http://arxiv.org/abs/2310.14374)|**[link](https://github.com/cv516buaa/ov-vg)**|\n", "2310.14107": "|**2023-10-21**|**On the Transferability of Visually Grounded PCFGs**|Yanpeng Zhao et.al.|[2310.14107](http://arxiv.org/abs/2310.14107)|**[link](https://github.com/zhaoyanpeng/cpcfg)**|\n", "2310.13257": "|**2024-03-25**|**Visual Grounding Helps Learn Word Meanings in Low-Data Regimes**|Chengxu Zhuang et.al.|[2310.13257](http://arxiv.org/abs/2310.13257)|**[link](https://github.com/EvLab-MIT/LexiContrastiveGrd)**|\n", "2310.12147": "|**2023-10-18**|**InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot Interactions**|Hanbo Zhang et.al.|[2310.12147](http://arxiv.org/abs/2310.12147)|null|\n", "2310.11441": "|**2023-11-06**|**Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V**|Jianwei Yang et.al.|[2310.11441](http://arxiv.org/abs/2310.11441)|**[link](https://github.com/microsoft/SoM)**|\n", "2310.10975": "|**2023-10-23**|**NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning**|Haowei Wang et.al.|[2310.10975](http://arxiv.org/abs/2310.10975)|**[link](https://github.com/mr-neko/nice)**|\n", "2310.10418": "|**2023-11-11**|**Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms**|Seungju Han et.al.|[2310.10418](http://arxiv.org/abs/2310.10418)|**[link](https://github.com/wade3han/normlens)**|\n", "2310.09478": "|**2023-11-07**|**MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning**|Jun Chen et.al.|[2310.09478](http://arxiv.org/abs/2310.09478)|null|\n", "2310.08825": "|**2024-03-08**|**From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models**|Dongsheng Jiang et.al.|[2310.08825](http://arxiv.org/abs/2310.08825)|**[link](https://github.com/yuchenliu98/comm)**|\n", "2310.07654": "|**2023-10-11**|**Audio-Visual Neural Syntax Acquisition**|Cheng-I Jeff Lai et.al.|[2310.07654](http://arxiv.org/abs/2310.07654)|null|\n", "2310.06214": "|**2024-10-05**|**CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding**|Eslam Abdelrahman et.al.|[2310.06214](http://arxiv.org/abs/2310.06214)|**[link](https://github.com/eslambakr/CoT3D_VG)**|\n", "2310.05861": "|**2024-04-02**|**Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models**|Archiki Prasad et.al.|[2310.05861](http://arxiv.org/abs/2310.05861)|**[link](https://github.com/archiki/repare)**|\n", "2310.05109": "|**2023-10-08**|**Lightweight In-Context Tuning for Multimodal Unified Models**|Yixin Chen et.al.|[2310.05109](http://arxiv.org/abs/2310.05109)|null|\n", "2310.02569": "|**2023-10-17**|**ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks**|Zejun Li et.al.|[2310.02569](http://arxiv.org/abs/2310.02569)|**[link](https://github.com/fudandisc/reform-eval)**|\n", "2309.17133": "|**2023-10-28**|**Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering**|Weizhe Lin et.al.|[2309.17133](http://arxiv.org/abs/2309.17133)|**[link](https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering)**|\n", "2309.13430": "|**2023-09-23**|**Resolving References in Visually-Grounded Dialogue via Text Generation**|Bram Willemsen et.al.|[2309.13430](http://arxiv.org/abs/2309.13430)|**[link](https://github.com/willemsenbram/reference-resolution-via-text-generation)**|\n", "2309.12311": "|**2023-09-21**|**LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent**|Jianing Yang et.al.|[2309.12311](http://arxiv.org/abs/2309.12311)|**[link](https://github.com/sled-group/chat-with-nerf)**|\n", "2309.10309": "|**2023-09-21**|**Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill**|Wenzhe Cai et.al.|[2309.10309](http://arxiv.org/abs/2309.10309)|**[link](https://github.com/wzcai99/pixel-navigator)**|\n", "2309.09456": "|**2023-09-18**|**Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection**|Chenming Zhu et.al.|[2309.09456](http://arxiv.org/abs/2309.09456)|null|\n", "2309.07759": "|**2024-04-05**|**PROGrasp: Pragmatic Human-Robot Communication for Object Grasping**|Gi-Cheon Kang et.al.|[2309.07759](http://arxiv.org/abs/2309.07759)|**[link](https://github.com/gicheonkang/prograsp)**|\n", "2309.07387": "|**2023-09-14**|**VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue**|Yunshui Li et.al.|[2309.07387](http://arxiv.org/abs/2309.07387)|null|\n", "2309.05251": "|**2023-09-11**|**Multi3DRefer: Grounding Text Description to Multiple 3D Objects**|Yiming Zhang et.al.|[2309.05251](http://arxiv.org/abs/2309.05251)|null|\n", "2309.05162": "|**2023-09-10**|**Collecting Visually-Grounded Dialogue with A Game Of Sorts**|Bram Willemsen et.al.|[2309.05162](http://arxiv.org/abs/2309.05162)|**[link](https://github.com/willemsenbram/a-game-of-sorts)**|\n", "2309.04628": "|**2023-09-08**|**Leveraging Pretrained Image-text Models for Improving Audio-Visual Learning**|Saurabhchand Bhati et.al.|[2309.04628](http://arxiv.org/abs/2309.04628)|null|\n", "2309.04561": "|**2024-07-16**|**Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding**|Ozan Unal et.al.|[2309.04561](http://arxiv.org/abs/2309.04561)|null|\n", "2309.04504": "|**2024-05-03**|**Compositional Learning of Visually-Grounded Concepts Using Reinforcement**|Zijun Lin et.al.|[2309.04504](http://arxiv.org/abs/2309.04504)|**[link](https://github.com/haidiazaman/rl-concept-learning-project)**|\n", "2309.03726": "|**2023-09-07**|**Interpretable Visual Question Answering via Reasoning Supervision**|Maria Parelli et.al.|[2309.03726](http://arxiv.org/abs/2309.03726)|null|\n", "2309.03483": "|**2023-09-07**|**DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners**|Clarence Lee et.al.|[2309.03483](http://arxiv.org/abs/2309.03483)|**[link](https://github.com/clarence-lee-sheng/determinet)**|\n", "2309.01327": "|**2024-03-30**|**Can I Trust Your Answer? Visually Grounded Video Question Answering**|Junbin Xiao et.al.|[2309.01327](http://arxiv.org/abs/2309.01327)|**[link](https://github.com/doc-doc/next-gqa)**|\n", "2309.01141": "|**2024-01-23**|**VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders**|Xuyang Liu et.al.|[2309.01141](http://arxiv.org/abs/2309.01141)|**[link](https://github.com/xuyang-liu16/vgdiffzero)**|\n", "2309.00035": "|**2023-08-31**|**FACET: Fairness in Computer Vision Evaluation Benchmark**|Laura Gustafson et.al.|[2309.00035](http://arxiv.org/abs/2309.00035)|null|\n", "2308.16349": "|**2024-08-27**|**Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations**|Kilichbek Haydarov et.al.|[2308.16349](http://arxiv.org/abs/2308.16349)|null|\n", "2308.16182": "|**2023-12-24**|**GREC: Generalized Referring Expression Comprehension**|Shuting He et.al.|[2308.16182](http://arxiv.org/abs/2308.16182)|**[link](https://github.com/henghuiding/grefcoco)**|\n", "2308.15962": "|**2023-08-31**|**WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model**|Tianyu Wang et.al.|[2308.15962](http://arxiv.org/abs/2308.15962)|null|\n", "2308.12966": "|**2023-10-13**|**Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond**|Jinze Bai et.al.|[2308.12966](http://arxiv.org/abs/2308.12966)|**[link](https://github.com/qwenlm/qwen-vl)**|\n", "2308.12537": "|**2023-08-24**|**HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks**|Zichao Dong et.al.|[2308.12537](http://arxiv.org/abs/2308.12537)|**[link](https://github.com/dzcgaara/HuBo-VLM)**|\n", "2308.12035": "|**2023-10-30**|**RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D**|Shuhei Kurita et.al.|[2308.12035](http://arxiv.org/abs/2308.12035)|**[link](https://github.com/shuheikurita/refego)**|\n", "2308.11887": "|**2023-11-20**|**A Unified Framework for 3D Point Cloud Visual Grounding**|Haojia Lin et.al.|[2308.11887](http://arxiv.org/abs/2308.11887)|**[link](https://github.com/leon1207/3dreftr)**|\n", "2308.11561": "|**2023-12-14**|**Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation**|Yifei Su et.al.|[2308.11561](http://arxiv.org/abs/2308.11561)|**[link](https://github.com/yifeisu/tg-gat)**|\n", "2308.11662": "|**2023-08-24**|**VQA Therapy: Exploring Answer Differences by Visually Grounding Answers**|Chongyan Chen et.al.|[2308.11662](http://arxiv.org/abs/2308.11662)|**[link](https://github.com/ccychongyanchen/vqatherapycrowdsourcing)**|\n", "2308.09977": "|**2023-08-19**|**Whether you can locate or not? Interactive Referring Expression Generation**|Fulong Ye et.al.|[2308.09977](http://arxiv.org/abs/2308.09977)|**[link](https://github.com/superhero-7/ireg)**|\n", "2308.09599": "|**2023-08-18**|**Language-Guided Diffusion Model for Visual Grounding**|Sijia Chen et.al.|[2308.09599](http://arxiv.org/abs/2308.09599)|**[link](https://github.com/iqua/vggbase)**|\n", "2308.08628": "|**2024-04-22**|**Learning the meanings of function words from grounded language using a visual question answering model**|Eva Portelance et.al.|[2308.08628](http://arxiv.org/abs/2308.08628)|**[link](https://github.com/evaportelance/vqa-function-word-learning)**|\n", "2308.06394": "|**2024-02-11**|**Detecting and Preventing Hallucinations in Large Vision Language Models**|Anisha Gunjal et.al.|[2308.06394](http://arxiv.org/abs/2308.06394)|**[link](https://github.com/hendryx-scale/mhal-detect)**|\n", "2308.04352": "|**2023-08-08**|**3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment**|Ziyu Zhu et.al.|[2308.04352](http://arxiv.org/abs/2308.04352)|null|\n", "2308.01143": "|**2023-08-02**|**ADS-Cap: A Framework for Accurate and Diverse Stylized Captioning with Unpaired Stylistic Corpora**|Kanzhi Cheng et.al.|[2308.01143](http://arxiv.org/abs/2308.01143)|**[link](https://github.com/njucckevin/ads-cap)**|\n", "2308.00640": "|**2023-08-01**|**VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes**|Yuhao Lu et.al.|[2308.00640](http://arxiv.org/abs/2308.00640)|**[link](https://github.com/luyh20/vl-grasp)**|\n", "2307.15554": "|**2023-07-28**|**'What are you referring to?' Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges**|Javier Chiyah-Garcia et.al.|[2307.15554](http://arxiv.org/abs/2307.15554)|**[link](https://github.com/jchiyah/what-are-you-referring-to)**|\n", "2307.13363": "|**2023-07-25**|**3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding**|Zehan Wang et.al.|[2307.13363](http://arxiv.org/abs/2307.13363)|null|\n", "2307.12813": "|**2023-10-11**|**Described Object Detection: Liberating Object Detection with Flexible Expressions**|Chi Xie et.al.|[2307.12813](http://arxiv.org/abs/2307.12813)|**[link](https://github.com/shikras/d-cube)**|\n", "2307.12392": "|**2023-07-23**|**Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision**|Menghao Li et.al.|[2307.12392](http://arxiv.org/abs/2307.12392)|**[link](https://github.com/cv516buaa/ir-vg)**|\n", "2307.11558": "|**2023-07-21**|**Advancing Visual Grounding with Scene Knowledge: Benchmark and Method**|Zhihong Chen et.al.|[2307.11558](http://arxiv.org/abs/2307.11558)|**[link](https://github.com/zhjohnchan/sk-vg)**|\n", "2307.09267": "|**2023-07-18**|**Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding**|Zehan Wang et.al.|[2307.09267](http://arxiv.org/abs/2307.09267)|**[link](https://github.com/ZzZZCHS/WS-3DVG)**|\n", "2307.08581": "|**2023-07-17**|**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**|Yang Zhao et.al.|[2307.08581](http://arxiv.org/abs/2307.08581)|null|\n", "2307.07166": "|**2023-07-14**|**Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks**|Ryosuke Korekata et.al.|[2307.07166](http://arxiv.org/abs/2307.07166)|null|\n", "2307.05963": "|**2023-07-12**|**GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation**|Junghyun Kim et.al.|[2307.05963](http://arxiv.org/abs/2307.05963)|**[link](https://github.com/JHKim-snu/GVCCI)**|\n", "2307.05873": "|**2023-07-12**|**OG: Equip vision occupancy with instance segmentation and visual grounding**|Zichao Dong et.al.|[2307.05873](http://arxiv.org/abs/2307.05873)|null|\n", "2307.05370": "|**2024-04-28**|**Origami Single-end Capacitive Sensing for Continuous Shape Estimation of Morphing Structures**|Lala Shakti Swarup Ray et.al.|[2307.05370](http://arxiv.org/abs/2307.05370)|null|\n", "2307.00162": "|**2024-01-31**|**What Do Self-Supervised Speech Models Know About Words?**|Ankita Pasad et.al.|[2307.00162](http://arxiv.org/abs/2307.00162)|**[link](https://github.com/ankitapasad/layerwise-analysis)**|\n", "2306.14824": "|**2023-07-13**|**Kosmos-2: Grounding Multimodal Large Language Models to the World**|Zhiliang Peng et.al.|[2306.14824](http://arxiv.org/abs/2306.14824)|**[link](https://github.com/microsoft/unilm/tree/master/kosmos-2)**|\n", "2306.14603": "|**2023-06-26**|**Learning with Difference Attention for Visually Grounded Self-supervised Representations**|Aishwarya Agarwal et.al.|[2306.14603](http://arxiv.org/abs/2306.14603)|null|\n", "2306.14182": "|**2023-06-25**|**Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input**|Qingpei Guo et.al.|[2306.14182](http://arxiv.org/abs/2306.14182)|null|\n", "2306.11371": "|**2024-04-18**|**Visually grounded few-shot word learning in low-resource settings**|Leanne Nortje et.al.|[2306.11371](http://arxiv.org/abs/2306.11371)|null|\n", "2306.10717": "|**2023-06-19**|**A neuro-symbolic approach for multimodal reference expression comprehension**|Aman Jain et.al.|[2306.10717](http://arxiv.org/abs/2306.10717)|null|\n", "2306.08685": "|**2023-06-14**|**World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models**|Ziqiao Ma et.al.|[2306.08685](http://arxiv.org/abs/2306.08685)|**[link](https://github.com/sled-group/world-to-words)**|\n", "2306.07298": "|**2023-06-10**|**Referring to Screen Texts with Voice Assistants**|Shruti Bhargava et.al.|[2306.07298](http://arxiv.org/abs/2306.07298)|null|\n", "2306.04488": "|**2023-10-16**|**Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards**|Alexandre Ram\u00e9 et.al.|[2306.04488](http://arxiv.org/abs/2306.04488)|**[link](https://github.com/alexrame/rewardedsoups)**|\n", "2306.04652": "|**2023-06-06**|**Language Adaptive Weight Generation for Multi-task Visual Grounding**|Wei Su et.al.|[2306.04652](http://arxiv.org/abs/2306.04652)|**[link](https://github.com/dcdcvgroup/vglaw-mindspore)**|\n", "2306.04451": "|**2023-06-06**|**Referring Expression Comprehension Using Language Adaptive Inference**|Wei Su et.al.|[2306.04451](http://arxiv.org/abs/2306.04451)|null|\n", "2306.02972": "|**2023-06-05**|**Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System**|Khazar Khorrami et.al.|[2306.02972](http://arxiv.org/abs/2306.02972)|null|\n", "2306.02348": "|**2023-06-04**|**Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations**|Aleksey Tikhonov et.al.|[2306.02348](http://arxiv.org/abs/2306.02348)|**[link](https://github.com/altsoph/modality_shifts)**|\n", "2305.19933": "|**2023-05-31**|**Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind**|Ece Takmaz et.al.|[2305.19933](http://arxiv.org/abs/2305.19933)|**[link](https://github.com/nicofirst1/speaker-adaptation)**|\n", "2305.18957": "|**2023-05-30**|**Wave to Syntax: Probing spoken language models for syntax**|Gaofei Shen et.al.|[2305.18957](http://arxiv.org/abs/2305.18957)|**[link](https://github.com/techsword/wave-to-syntax)**|\n", "2305.18824": "|**2023-05-30**|**Graph Neural Networks for Contextual ASR with the Tree-Constrained Pointer Generator**|Guangzhi Sun et.al.|[2305.18824](http://arxiv.org/abs/2305.18824)|**[link](https://github.com/briansidp/espnet)**|\n", "2305.17337": "|**2023-05-27**|**Benchmarking Diverse-Modal Entity Linking with Generative Models**|Sijia Wang et.al.|[2305.17337](http://arxiv.org/abs/2305.17337)|null|\n", "2305.15937": "|**2023-05-25**|**Visually grounded few-shot word acquisition with fewer shots**|Leanne Nortje et.al.|[2305.15937](http://arxiv.org/abs/2305.15937)|null|\n", "2305.15765": "|**2023-05-25**|**Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving**|Wenhao Cheng et.al.|[2305.15765](http://arxiv.org/abs/2305.15765)|null|\n", "2305.15015": "|**2023-10-14**|**Measuring Faithful and Plausible Visual Grounding in VQA**|Daniel Reich et.al.|[2305.15015](http://arxiv.org/abs/2305.15015)|**[link](https://github.com/dreichcsl/fpvg)**|\n", "2305.14998": "|**2024-02-06**|**An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics**|Saba Ahmadi et.al.|[2305.14998](http://arxiv.org/abs/2305.14998)|**[link](https://github.com/saba96/img-cap-metrics-robustness)**|\n", "2305.13876": "|**2024-02-07**|**Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans**|Taiki Miyanishi et.al.|[2305.13876](http://arxiv.org/abs/2305.13876)|**[link](https://github.com/atr-dbi/cross3dvg)**|\n", "2305.11497": "|**2023-05-19**|**TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding**|Chenchi Zhang et.al.|[2305.11497](http://arxiv.org/abs/2305.11497)|null|\n", "2305.11435": "|**2023-07-23**|**Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model**|Puyuan Peng et.al.|[2305.11435](http://arxiv.org/abs/2305.11435)|**[link](https://github.com/jasonppy/syllable-discovery)**|\n", "2305.11172": "|**2023-05-18**|**ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities**|Peng Wang et.al.|[2305.11172](http://arxiv.org/abs/2305.11172)|**[link](https://github.com/OFA-Sys/ONE-PEACE)**|\n", "2305.10714": "|**2023-05-18**|**Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding**|Taolin Zhang et.al.|[2305.10714](http://arxiv.org/abs/2305.10714)|null|\n", "2305.08685": "|**2024-11-19**|**CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding**|Linhui Xiao et.al.|[2305.08685](http://arxiv.org/abs/2305.08685)|**[link](https://github.com/linhuixiao/clip-vg)**|\n", "2305.16328": "|**2023-05-15**|**Semantic Composition in Visually Grounded Language Models**|Rohan Pandey et.al.|[2305.16328](http://arxiv.org/abs/2305.16328)|null|\n", "2304.13181": "|**2023-08-12**|**Sample-Specific Debiasing for Better Image-Text Models**|Peiqi Wang et.al.|[2304.13181](http://arxiv.org/abs/2304.13181)|null|\n", "2304.10311": "|**2023-04-20**|**Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining**|Qin Chao et.al.|[2304.10311](http://arxiv.org/abs/2304.10311)|null|\n", "2304.08587": "|**2023-06-19**|**Grounding Classical Task Planners via Vision-Language Models**|Xiaohan Zhang et.al.|[2304.08587](http://arxiv.org/abs/2304.08587)|null|\n", "2304.06712": "|**2023-08-18**|**What does CLIP know about a red circle? Visual prompt engineering for VLMs**|Aleksandar Shtedritski et.al.|[2304.06712](http://arxiv.org/abs/2304.06712)|null|\n", "2304.05645": "|**2024-07-15**|**WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language**|Zhenxiang Lin et.al.|[2304.05645](http://arxiv.org/abs/2304.05645)|**[link](https://github.com/4dvlab/wildrefer)**|\n", "2304.02560": "|**2024-03-29**|**VicTR: Video-conditioned Text Representations for Activity Recognition**|Kumara Kahatapitiya et.al.|[2304.02560](http://arxiv.org/abs/2304.02560)|null|\n", "2303.17517": "|**2023-03-30**|**Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples**|Hyeonggon Ryu et.al.|[2303.17517](http://arxiv.org/abs/2303.17517)|null|\n", "2303.16894": "|**2023-12-05**|**ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance**|Zoey Guo et.al.|[2303.16894](http://arxiv.org/abs/2303.16894)|**[link](https://github.com/ivan-tang-3d/viewrefer3d)**|\n", "2303.13483": "|**2023-03-23**|**NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations**|Joy Hsu et.al.|[2303.13483](http://arxiv.org/abs/2303.13483)|null|\n", "2303.13186": "|**2023-03-23**|**ScanERU: Interactive 3D Visual Grounding based on Embodied Reference Understanding**|Ziyang Lu et.al.|[2303.13186](http://arxiv.org/abs/2303.13186)|**[link](https://github.com/mrlearnedtoad/scaneru)**|\n", "2303.12027": "|**2023-03-21**|**Joint Visual Grounding and Tracking with Natural Language Specification**|Li Zhou et.al.|[2303.12027](http://arxiv.org/abs/2303.12027)|**[link](https://github.com/lizhou-cs/jointnlt)**|\n", "2303.10699": "|**2023-03-19**|**FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering**|Weizhe Lin et.al.|[2303.10699](http://arxiv.org/abs/2303.10699)|null|\n", "2303.07618": "|**2023-03-14**|**Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment**|Zhihao Chen et.al.|[2303.07618](http://arxiv.org/abs/2303.07618)|null|\n", "2303.07216": "|**2023-03-14**|**Parallel Vertex Diffusion for Unified Visual Grounding**|Zesen Cheng et.al.|[2303.07216](http://arxiv.org/abs/2303.07216)|null|\n", "2303.06674": "|**2023-08-17**|**Universal Instance Perception as Object Discovery and Retrieval**|Bin Yan et.al.|[2303.06674](http://arxiv.org/abs/2303.06674)|**[link](https://github.com/MasterBin-IIAU/UNINEXT)**|\n", "2303.06378": "|**2023-05-17**|**Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos**|Teng Wang et.al.|[2303.06378](http://arxiv.org/abs/2303.06378)|**[link](https://github.com/zjr2000/gvl)**|\n", "2303.05499": "|**2024-07-19**|**Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection**|Shilong Liu et.al.|[2303.05499](http://arxiv.org/abs/2303.05499)|**[link](https://github.com/idea-research/groundingdino)**|\n", "2302.12766": "|**2023-02-24**|**Language-Driven Representation Learning for Robotics**|Siddharth Karamcheti et.al.|[2302.12766](http://arxiv.org/abs/2302.12766)|**[link](https://github.com/siddk/voltron-evaluation)**|\n", "2302.12610": "|**2024-10-31**|**A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter**|Kechun Xu et.al.|[2302.12610](http://arxiv.org/abs/2302.12610)|**[link](https://github.com/xukechun/Vision-Language-Grasping)**|\n", "2302.12189": "|**2023-09-25**|**HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales**|Michele Cafagna et.al.|[2302.12189](http://arxiv.org/abs/2302.12189)|**[link](https://github.com/michelecafagna26/hl-dataset)**|\n", "2302.11252": "|**2023-02-22**|**Focusing On Targets For Improving Weakly Supervised Visual Grounding**|Viet-Quoc Pham et.al.|[2302.11252](http://arxiv.org/abs/2302.11252)|null|\n", "2302.09027": "|**2023-02-17**|**CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension**|Zhi Zhang et.al.|[2302.09027](http://arxiv.org/abs/2302.09027)|**[link](https://github.com/fightingfighting/ck-transformer)**|\n", "2302.00765": "|**2023-02-01**|**Visually Grounded Keyword Detection and Localisation for Low-Resource Languages**|Kayode Kolawole Olaleye et.al.|[2302.00765](http://arxiv.org/abs/2302.00765)|null|\n", "2301.13823": "|**2023-06-13**|**Grounding Language Models to Images for Multimodal Inputs and Outputs**|Jing Yu Koh et.al.|[2301.13823](http://arxiv.org/abs/2301.13823)|**[link](https://github.com/kohjingyu/fromage)**|\n", "2301.09045": "|**2023-02-20**|**Champion Solution for the WSDM2023 Toloka VQA Challenge**|Shengyi Gao et.al.|[2301.09045](http://arxiv.org/abs/2301.09045)|**[link](https://github.com/czczup/vit-adapter)**|\n", "2301.08571": "|**2023-01-20**|**Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences**|Xudong Hong et.al.|[2301.08571](http://arxiv.org/abs/2301.08571)|null|\n", "2212.09737": "|**2023-06-07**|**Position-guided Text Prompt for Vision-Language Pre-training**|Alex Jinpeng Wang et.al.|[2212.09737](http://arxiv.org/abs/2212.09737)|**[link](https://github.com/sail-sg/ptp)**|\n", "2212.05561": "|**2023-03-09**|**Using Multiple Instance Learning to Build Multimodal Representations**|Peiqi Wang et.al.|[2212.05561](http://arxiv.org/abs/2212.05561)|null|\n", "2212.00843": "|**2022-12-01**|**Focus! Relevant and Sufficient Context Selection for News Image Captioning**|Mingyang Zhou et.al.|[2212.00843](http://arxiv.org/abs/2212.00843)|null|\n", "2212.00836": "|**2022-12-01**|**UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2212.00836](http://arxiv.org/abs/2212.00836)|null|\n", "2211.15516": "|**2022-11-30**|**DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding**|Shilong Liu et.al.|[2211.15516](http://arxiv.org/abs/2211.15516)|**[link](https://github.com/idea-research/dq-detr)**|\n", "2211.14739": "|**2022-11-27**|**MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding**|Meihuizi Jia et.al.|[2211.14739](http://arxiv.org/abs/2211.14739)|null|\n", "2211.14241": "|**2022-11-25**|**Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding**|Eslam Mohamed Bakr et.al.|[2211.14241](http://arxiv.org/abs/2211.14241)|**[link](https://github.com/eslambakr/LAR-Look-Around-and-Refer)**|\n", "2211.12054": "|**2023-03-25**|**Visually Grounded Commonsense Knowledge Acquisition**|Yuan Yao et.al.|[2211.12054](http://arxiv.org/abs/2211.12054)|**[link](https://github.com/thunlp/clever)**|\n", "2211.08086": "|**2022-11-15**|**Visually Grounded VQA by Lattice-based Retrieval**|Daniel Reich et.al.|[2211.08086](http://arxiv.org/abs/2211.08086)|**[link](https://github.com/dreichCSL/GQA_generalization_splits)**|\n", "2211.07912": "|**2022-11-15**|**YORO -- Lightweight End to End Visual Grounding**|Chih-Hui Ho et.al.|[2211.07912](http://arxiv.org/abs/2211.07912)|**[link](https://github.com/chihhuiho/yoro)**|\n", "2210.13431": "|**2023-03-25**|**Instruction-Following Agents with Multimodal Transformer**|Hao Liu et.al.|[2210.13431](http://arxiv.org/abs/2210.13431)|**[link](https://github.com/lhao499/instructrl)**|\n", "2210.12997": "|**2022-10-24**|**Are Current Decoding Strategies Capable of Facing the Challenges of Visual Dialogue?**|Amit Kumar Chaudhary et.al.|[2210.12997](http://arxiv.org/abs/2210.12997)|null|\n", "2210.12828": "|**2022-10-23**|**Towards Pragmatic Production Strategies for Natural Language Generation Tasks**|Mario Giulianelli et.al.|[2210.12828](http://arxiv.org/abs/2210.12828)|null|\n", "2210.12634": "|**2022-10-23**|**RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing Data**|Yang Zhan et.al.|[2210.12634](http://arxiv.org/abs/2210.12634)|**[link](https://github.com/zhanyang-nwpu/rsvg-pytorch)**|\n", "2210.12565": "|**2022-10-22**|**A Visual Tour Of Current Challenges In Multimodal Language Models**|Shashank Sonkar et.al.|[2210.12565](http://arxiv.org/abs/2210.12565)|null|\n", "2210.12513": "|**2023-06-09**|**Learning Point-Language Hierarchical Alignment for 3D Visual Grounding**|Jiaming Chen et.al.|[2210.12513](http://arxiv.org/abs/2210.12513)|**[link](https://github.com/ppjmchen/ham)**|\n", "2210.10775": "|**2022-10-19**|**TOIST: Task Oriented Instance Segmentation Transformer with Noun-Pronoun Distillation**|Pengfei Li et.al.|[2210.10775](http://arxiv.org/abs/2210.10775)|**[link](https://github.com/air-discover/toist)**|\n", "2210.09263": "|**2022-10-17**|**Vision-Language Pre-training: Basics, Recent Advances, and Future Trends**|Zhe Gan et.al.|[2210.09263](http://arxiv.org/abs/2210.09263)|**[link](https://github.com/computer-vision-in-the-wild/cvinw_readings)**|\n", "2210.05487": "|**2023-02-13**|**Like a bilingual baby: The advantage of visually grounding a bilingual language model**|Khai-Nguyen Nguyen et.al.|[2210.05487](http://arxiv.org/abs/2210.05487)|null|\n", "2210.04600": "|**2022-10-12**|**YFACC: A Yor\u00f9b\u00e1 speech-image dataset for cross-lingual keyword localisation through visual grounding**|Kayode Olaleye et.al.|[2210.04600](http://arxiv.org/abs/2210.04600)|null|\n", "2210.04183": "|**2023-06-14**|**MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning**|Zijia Zhao et.al.|[2210.04183](http://arxiv.org/abs/2210.04183)|null|\n", "2210.04135": "|**2023-10-30**|**VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment**|Shraman Pramanick et.al.|[2210.04135](http://arxiv.org/abs/2210.04135)|**[link](https://github.com/ShramanPramanick/VoLTA)**|\n", "2210.03787": "|**2022-10-07**|**Learning a Visually Grounded Memory Assistant**|Meera Hahn et.al.|[2210.03787](http://arxiv.org/abs/2210.03787)|null|\n", "2210.03416": "|**2022-10-07**|**Detailed Annotations of Chest X-Rays via CT Projection for Report Understanding**|Constantin Seibold et.al.|[2210.03416](http://arxiv.org/abs/2210.03416)|null|\n", "2210.03112": "|**2023-04-17**|**A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning**|Aishwarya Kamath et.al.|[2210.03112](http://arxiv.org/abs/2210.03112)|null|\n", "2210.02953": "|**2022-10-06**|**Video Referring Expression Comprehension via Transformer with Content-aware Query**|Ji Jiang et.al.|[2210.02953](http://arxiv.org/abs/2210.02953)|null|\n", "2210.02302": "|**2022-10-05**|**GLAD: Grounded Layered Autonomous Driving for Complex Service Tasks**|Yan Ding et.al.|[2210.02302](http://arxiv.org/abs/2210.02302)|null|\n", "2210.00858": "|**2023-05-07**|**Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach**|Georgios Tziafas et.al.|[2210.00858](http://arxiv.org/abs/2210.00858)|**[link](https://github.com/gtziafas/hots)**|\n", "2210.00215": "|**2023-03-13**|**Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement**|Zirui Zhao et.al.|[2210.00215](http://arxiv.org/abs/2210.00215)|null|\n", "2209.14941": "|**2023-04-24**|**EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**|Yanmin Wu et.al.|[2209.14941](http://arxiv.org/abs/2209.14941)|**[link](https://github.com/yanmin-wu/eda)**|\n", "2209.13959": "|**2023-10-26**|**Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding**|Fengyuan Shi et.al.|[2209.13959](http://arxiv.org/abs/2209.13959)|null|\n", "2209.12028": "|**2022-09-24**|**Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline**|Lichen Zhao et.al.|[2209.12028](http://arxiv.org/abs/2209.12028)|**[link](https://github.com/zlccccc/3dgqa)**|\n", "2209.10126": "|**2022-09-21**|**Exploring Modulated Detection Transformer as a Tool for Action Recognition in Videos**|Tom\u00e1s Crisol et.al.|[2209.10126](http://arxiv.org/abs/2209.10126)|**[link](https://github.com/bhi-research/ava_mdetr)**|\n", "2209.08425": "|**2022-09-17**|**Introspective Learning : A Two-Stage Approach for Inference in Neural Networks**|Mohit Prabhushankar et.al.|[2209.08425](http://arxiv.org/abs/2209.08425)|**[link](https://github.com/olivesgatech/introspective-learning)**|\n", "2212.03864": "|**2022-09-15**|**Can Offline Reinforcement Learning Help Natural Language Understanding?**|Ziqi Zhang et.al.|[2212.03864](http://arxiv.org/abs/2212.03864)|null|\n", "2209.06515": "|**2022-09-19**|**Learning to Evaluate Performance of Multi-modal Semantic Localization**|Zhiqiang Yuan et.al.|[2209.06515](http://arxiv.org/abs/2209.06515)|**[link](https://github.com/xiaoyuan1996/semanticlocalizationmetrics)**|\n", "2209.03714": "|**2022-11-21**|**Visual Grounding of Inter-lingual Word-Embeddings**|Wafaa Mohammed et.al.|[2209.03714](http://arxiv.org/abs/2209.03714)|null|\n", "2208.13628": "|**2022-10-05**|**Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment**|Mustafa Shukor et.al.|[2208.13628](http://arxiv.org/abs/2208.13628)|**[link](https://github.com/mshukor/vicha)**|\n", "2208.10353": "|**2022-08-22**|**Neuro-Symbolic Visual Dialog**|Adnen Abdessaied et.al.|[2208.10353](http://arxiv.org/abs/2208.10353)|**[link](https://github.com/adnenabdessaied/NSVD)**|\n", "2208.09374": "|**2022-08-19**|**VLMAE: Vision-Language Masked Autoencoder**|Sunan He et.al.|[2208.09374](http://arxiv.org/abs/2208.09374)|null|\n", "2208.02515": "|**2022-09-19**|**Fine-Grained Semantically Aligned Vision-Language Pre-Training**|Juncheng Li et.al.|[2208.02515](http://arxiv.org/abs/2208.02515)|**[link](https://github.com/yyjmjc/loupe)**|\n", "2208.00361": "|**2022-10-27**|**One for All: One-stage Referring Expression Comprehension with Dynamic Reasoning**|Zhipeng Zhang et.al.|[2208.00361](http://arxiv.org/abs/2208.00361)|null|\n", "2207.13325": "|**2022-07-27**|**SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding**|Mengxue Qu et.al.|[2207.13325](http://arxiv.org/abs/2207.13325)|**[link](https://github.com/qumengxue/siri-vg)**|\n", "2207.10400": "|**2022-08-17**|**Correspondence Matters for Video Referring Expression Comprehension**|Meng Cao et.al.|[2207.10400](http://arxiv.org/abs/2207.10400)|**[link](https://github.com/mengcaopku/dcnet)**|\n", "2207.03482": "|**2022-11-29**|**Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection**|Hanoona Rasheed et.al.|[2207.03482](http://arxiv.org/abs/2207.03482)|null|\n", "2207.02639": "|**2022-07-06**|**Adversarial Robustness of Visual Dialog**|Lu Yu et.al.|[2207.02639](http://arxiv.org/abs/2207.02639)|null|\n", "2207.01821": "|**2023-05-27**|**Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases**|Zhihao Yuan et.al.|[2207.01821](http://arxiv.org/abs/2207.01821)|null|\n", "2207.00857": "|**2022-07-02**|**Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition**|Guangzhi Sun et.al.|[2207.00857](http://arxiv.org/abs/2207.00857)|null|\n", "2206.15462": "|**2024-01-07**|**Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations**|Ziyan Yang et.al.|[2206.15462](http://arxiv.org/abs/2206.15462)|**[link](https://github.com/uvavision/amc-grounding)**|\n", "2206.15381": "|**2023-10-31**|**How direct is the link between words and images?**|Hassan Shahmohammadi et.al.|[2206.15381](http://arxiv.org/abs/2206.15381)|null|\n", "2206.11352": "|**2022-06-22**|**Doubly Reparameterized Importance Weighted Structure Learning for Scene Graph Generation**|Daqi Liu et.al.|[2206.11352](http://arxiv.org/abs/2206.11352)|null|\n", "2207.05703": "|**2022-06-21**|**Tell Me the Evidence? Dual Visual-Linguistic Interaction for Answer Grounding**|Junwen Pan et.al.|[2207.05703](http://arxiv.org/abs/2207.05703)|null|\n", "2206.09114": "|**2022-06-22**|**Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution**|Chonghan Chen et.al.|[2206.09114](http://arxiv.org/abs/2206.09114)|null|\n", "2206.08919": "|**2022-06-17**|**VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix**|Teng Wang et.al.|[2206.08919](http://arxiv.org/abs/2206.08919)|null|\n", "2206.08823": "|**2023-10-31**|**Language with Vision: a Study on Grounded Word and Sentence Embeddings**|Hassan Shahmohammadi et.al.|[2206.08823](http://arxiv.org/abs/2206.08823)|**[link](https://github.com/hazel1994/visually_grounded_word_embeddings_2)**|\n", "2206.08358": "|**2023-01-09**|**MixGen: A New Multi-Modal Data Augmentation**|Xiaoshuai Hao et.al.|[2206.08358](http://arxiv.org/abs/2206.08358)|**[link](https://github.com/amazon-research/mix-generation)**|\n", "2206.08172": "|**2022-06-16**|**RefCrowd: Grounding the Target in Crowd with Referring Expressions**|Heqian Qiu et.al.|[2206.08172](http://arxiv.org/abs/2206.08172)|null|\n", "2206.07643": "|**2022-11-18**|**Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone**|Zi-Yi Dou et.al.|[2206.07643](http://arxiv.org/abs/2206.07643)|**[link](https://github.com/microsoft/fiber)**|\n", "2206.06930": "|**2022-06-14**|**Comprehending and Ordering Semantics for Image Captioning**|Yehao Li et.al.|[2206.06930](http://arxiv.org/abs/2206.06930)|**[link](https://github.com/yehli/xmodaler)**|\n", "2206.06619": "|**2022-06-14**|**TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer**|Jiajun Deng et.al.|[2206.06619](http://arxiv.org/abs/2206.06619)|**[link](https://github.com/djiajunustc/TransVG)**|\n", "2206.03139": "|**2022-06-07**|**Intra-agent speech permits zero-shot task acquisition**|Chen Yan et.al.|[2206.03139](http://arxiv.org/abs/2206.03139)|null|\n", "2205.12616": "|**2022-05-25**|**Guiding Visual Question Answering with Attention Priors**|Thao Minh Le et.al.|[2205.12616](http://arxiv.org/abs/2205.12616)|null|\n", "2205.12502": "|**2023-03-02**|**The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training**|Gi-Cheon Kang et.al.|[2205.12502](http://arxiv.org/abs/2205.12502)|**[link](https://github.com/gicheonkang/gst-visdial)**|\n", "2205.12089": "|**2022-07-10**|**Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution**|Georgios Tziafas et.al.|[2205.12089](http://arxiv.org/abs/2205.12089)|null|\n", "2205.12005": "|**2022-05-25**|**mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections**|Chenliang Li et.al.|[2205.12005](http://arxiv.org/abs/2205.12005)|**[link](https://github.com/alibaba/AliceMind/tree/main/mPLUG)**|\n", "2205.11169": "|**2022-11-22**|**PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models**|Yuan Yao et.al.|[2205.11169](http://arxiv.org/abs/2205.11169)|**[link](https://github.com/thunlp/pevl)**|\n", "2205.07017": "|**2022-05-14**|**Importance Weighted Structure Learning for Scene Graph Generation**|Daqi Liu et.al.|[2205.07017](http://arxiv.org/abs/2205.07017)|null|\n", "2205.04725": "|**2022-05-12**|**Weakly-supervised segmentation of referring expressions**|Robin Strudel et.al.|[2205.04725](http://arxiv.org/abs/2205.04725)|null|\n", "2205.03774": "|**2022-05-08**|**RoViST:Learning Robust Metrics for Visual Storytelling**|Eileen Wang et.al.|[2205.03774](http://arxiv.org/abs/2205.03774)|**[link](https://github.com/usydnlp/rovist)**|\n", "2205.02655": "|**2022-05-30**|**Language Models Can See: Plugging Visual Controls in Text Generation**|Yixuan Su et.al.|[2205.02655](http://arxiv.org/abs/2205.02655)|**[link](https://github.com/yxuansu/magic)**|\n", "2205.00272": "|**2022-06-08**|**Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning**|Li Yang et.al.|[2205.00272](http://arxiv.org/abs/2205.00272)|**[link](https://github.com/yangli18/vltvg)**|\n", "2204.10448": "|**2022-04-22**|**Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering**|Yu-Jung Heo et.al.|[2204.10448](http://arxiv.org/abs/2204.10448)|**[link](https://github.com/yujungheo/kbvqa-public)**|\n", "2204.09957": "|**2024-03-12**|**Self-paced Multi-grained Cross-modal Interaction Modeling for Referring Expression Comprehension**|Peihan Miao et.al.|[2204.09957](http://arxiv.org/abs/2204.09957)|null|\n", "2204.07913": "|**2023-09-14**|**A Survivor in the Era of Large-Scale Pretraining: An Empirical Study of One-Stage Referring Expression Comprehension**|Gen Luo et.al.|[2204.07913](http://arxiv.org/abs/2204.07913)|**[link](https://github.com/luogen1996/simrec)**|\n", "2204.07316": "|**2022-05-03**|**XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding**|Chan-Jan Hsu et.al.|[2204.07316](http://arxiv.org/abs/2204.07316)|null|\n", "2204.07302": "|**2022-04-15**|**Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning**|Feilong Chen et.al.|[2204.07302](http://arxiv.org/abs/2204.07302)|null|\n", "2204.06272": "|**2022-04-13**|**3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection**|Junyu Luo et.al.|[2204.06272](http://arxiv.org/abs/2204.06272)|**[link](https://github.com/fjhzhixi/3d-sps)**|\n", "2204.05991": "|**2022-05-02**|**ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension**|Sanjay Subramanian et.al.|[2204.05991](http://arxiv.org/abs/2204.05991)|**[link](https://github.com/allenai/reclip)**|\n", "2204.02174": "|**2022-04-05**|**Multi-View Transformer for 3D Visual Grounding**|Shijia Huang et.al.|[2204.02174](http://arxiv.org/abs/2204.02174)|**[link](https://github.com/sega-hsj/mvt-3dvg)**|\n", "2203.17273": "|**2022-08-09**|**FindIt: Generalized Localization with Natural Language Queries**|Weicheng Kuo et.al.|[2203.17273](http://arxiv.org/abs/2203.17273)|null|\n", "2203.16682": "|**2022-03-30**|**To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo**|Yiran Luo et.al.|[2203.16682](http://arxiv.org/abs/2203.16682)|**[link](https://github.com/fpsluozi/tofindwaldo)**|\n", "2203.16265": "|**2022-07-24**|**SeqTR: A Simple yet Universal Network for Visual Grounding**|Chaoyang Zhu et.al.|[2203.16265](http://arxiv.org/abs/2203.16265)|**[link](https://github.com/sean-zhuh/seqtr)**|\n", "2203.15442": "|**2022-03-29**|**Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding**|Jiabo Ye et.al.|[2203.15442](http://arxiv.org/abs/2203.15442)|**[link](https://github.com/lukeforeveryoung/qrnet)**|\n", "2203.15081": "|**2023-06-20**|**Word Discovery in Visually Grounded, Self-Supervised Speech Models**|Puyuan Peng et.al.|[2203.15081](http://arxiv.org/abs/2203.15081)|**[link](https://github.com/kamperh/vqwordseg)**|\n", "2203.13838": "|**2022-03-25**|**Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas**|Raphael Schumann et.al.|[2203.13838](http://arxiv.org/abs/2203.13838)|**[link](https://github.com/raphael-sch/map2seq_vln)**|\n", "2203.10444": "|**2023-05-26**|**VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning**|Wenjia Xu et.al.|[2203.10444](http://arxiv.org/abs/2203.10444)|**[link](https://github.com/wenjiaxu/vgse)**|\n", "2203.09773": "|**2024-01-19**|**Local-Global Context Aware Transformer for Language-Guided Video Segmentation**|Chen Liang et.al.|[2203.09773](http://arxiv.org/abs/2203.09773)|**[link](https://github.com/leonnnop/locater)**|\n", "2203.08481": "|**2022-03-22**|**Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding**|Haojun Jiang et.al.|[2203.08481](http://arxiv.org/abs/2203.08481)|**[link](https://github.com/leaplabthu/pseudo-q)**|\n", "2203.06937": "|**2022-03-14**|**Modelling word learning and recognition using visually grounded speech**|Danny Merkx et.al.|[2203.06937](http://arxiv.org/abs/2203.06937)|**[link](https://github.com/DannyMerkx/speech2image)**|\n", "2203.06382": "|**2023-06-02**|**Differentiated Relevances Embedding for Group-based Referring Expression Comprehension**|Fuhai Chen et.al.|[2203.06382](http://arxiv.org/abs/2203.06382)|null|\n", "2203.06107": "|**2022-03-11**|**REX: Reasoning-aware and Grounded Explanation**|Shi Chen et.al.|[2203.06107](http://arxiv.org/abs/2203.06107)|**[link](https://github.com/szzexpoi/rex)**|\n", "2203.05186": "|**2023-08-21**|**Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding**|Yang Jiao et.al.|[2203.05186](http://arxiv.org/abs/2203.05186)|null|\n", "2202.10667": "|**2022-02-24**|**Visually Grounded Task and Motion Planning for Mobile Manipulation**|Xiaohan Zhang et.al.|[2202.10667](http://arxiv.org/abs/2202.10667)|null|\n", "2202.10292": "|**2022-02-21**|**Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge**|Danny Merkx et.al.|[2202.10292](http://arxiv.org/abs/2202.10292)|**[link](https://github.com/DannyMerkx/speech2image)**|\n", "2202.08512": "|**2022-02-17**|**Visual Ground Truth Construction as Faceted Classification**|Fausto Giunchiglia et.al.|[2202.08512](http://arxiv.org/abs/2202.08512)|null|\n", "2202.03543": "|**2022-03-02**|**Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling**|Puyuan Peng et.al.|[2202.03543](http://arxiv.org/abs/2202.03543)|**[link](https://github.com/jasonppy/fast-vgs-family)**|\n", "2202.03052": "|**2022-06-01**|**OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework**|Peng Wang et.al.|[2202.03052](http://arxiv.org/abs/2202.03052)|**[link](https://github.com/ofa-sys/ofa)**|\n", "2202.01993": "|**2022-04-08**|**Grounding Answers for Visual Questions Asked by Visually Impaired People**|Chongyan Chen et.al.|[2202.01993](http://arxiv.org/abs/2202.01993)|**[link](https://github.com/ccychongyanchen/vizwizvqagroundingcrowdsourcing)**|\n", "2202.01107": "|**2022-02-02**|**Keyword localisation in untranscribed speech using visually grounded speech models**|Kayode Olaleye et.al.|[2202.01107](http://arxiv.org/abs/2202.01107)|**[link](https://github.com/kayodeolaleye/keyword_localisation_speech)**|\n", "2201.11697": "|**2022-01-27**|**Constrained Structure Learning for Scene Graph Generation**|Daqi Liu et.al.|[2201.11697](http://arxiv.org/abs/2201.11697)|null|\n", "2112.15324": "|**2021-12-31**|**Deconfounded Visual Grounding**|Jianqiang Huang et.al.|[2112.15324](http://arxiv.org/abs/2112.15324)|null|\n", "2112.09583": "|**2021-12-23**|**Align and Prompt: Video-and-Language Pre-training with Entity Prompts**|Dongxu Li et.al.|[2112.09583](http://arxiv.org/abs/2112.09583)|**[link](https://github.com/salesforce/alpro)**|\n", "2112.07133": "|**2023-05-11**|**CLIP-Lite: Information Efficient Visual Representation Learning with Language Supervision**|Aman Shrivastava et.al.|[2112.07133](http://arxiv.org/abs/2112.07133)|**[link](https://github.com/4m4n5/CLIP-Lite)**|\n", "2112.01551": "|**2022-07-22**|**D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2112.01551](http://arxiv.org/abs/2112.01551)|null|\n", "2111.13131": "|**2021-11-25**|**Scene Graph Generation with Geometric Context**|Vishal Kumar et.al.|[2111.13131](http://arxiv.org/abs/2111.13131)|null|\n", "2111.12872": "|**2022-04-04**|**Less is More: Generating Grounded Navigation Instructions from Landmarks**|Su Wang et.al.|[2111.12872](http://arxiv.org/abs/2111.12872)|null|\n", "2111.12085": "|**2022-07-27**|**UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling**|Zhengyuan Yang et.al.|[2111.12085](http://arxiv.org/abs/2111.12085)|**[link](https://github.com/microsoft/UniTAB)**|\n", "2111.07180": "|**2021-11-13**|**Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning**|Yizhen Zhang et.al.|[2111.07180](http://arxiv.org/abs/2111.07180)|null|\n", "2111.05759": "|**2022-07-18**|**Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation**|Chuang Lin et.al.|[2111.05759](http://arxiv.org/abs/2111.05759)|**[link](https://github.com/clin1223/mtvm)**|\n", "2110.10206": "|**2022-11-30**|**Evaluating and Improving Interactions with Hazy Oracles**|Stephan J. Lemmer et.al.|[2110.10206](http://arxiv.org/abs/2110.10206)|null|\n", "2110.09779": "|**2021-10-19**|**Open-domain clarification question generation without question examples**|Julia White et.al.|[2110.09779](http://arxiv.org/abs/2110.09779)|null|\n", "2110.08797": "|**2023-09-14**|**Towards Language-guided Visual Recognition via Dynamic Convolutions**|Gen Luo et.al.|[2110.08797](http://arxiv.org/abs/2110.08797)|**[link](https://github.com/luogen1996/laconvnet)**|\n", "2110.07575": "|**2021-10-14**|**Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset**|Ian Palmer et.al.|[2110.07575](http://arxiv.org/abs/2110.07575)|**[link](https://github.com/iapalm/Spoken-ObjectNet)**|\n", "2110.02577": "|**2021-10-06**|**Efficient Multi-Modal Embeddings from Structured Data**|Anita L. Ver\u0151 et.al.|[2110.02577](http://arxiv.org/abs/2110.02577)|null|\n", "2109.14115": "|**2021-09-29**|**Visually Grounded Concept Composition**|Bowen Zhang et.al.|[2109.14115](http://arxiv.org/abs/2109.14115)|null|\n", "2109.13238": "|**2021-10-21**|**Visually Grounded Reasoning across Languages and Cultures**|Fangyu Liu et.al.|[2109.13238](http://arxiv.org/abs/2109.13238)|**[link](https://github.com/marvl-challenge/marvl-code)**|\n", "2109.11797": "|**2022-05-20**|**CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models**|Yuan Yao et.al.|[2109.11797](http://arxiv.org/abs/2109.11797)|**[link](https://github.com/thunlp/cpt)**|\n", "2109.10613": "|**2021-09-22**|**COVR: A test-bed for Visually Grounded Compositional Generalization with real images**|Ben Bogin et.al.|[2109.10613](http://arxiv.org/abs/2109.10613)|**[link](https://github.com/benbogin/covr-dataset)**|\n", "2109.10571": "|**2021-09-22**|**Audio-Visual Grounding Referring Expression for Robotic Manipulation**|Yefei Wang et.al.|[2109.10571](http://arxiv.org/abs/2109.10571)|null|\n", "2109.09790": "|**2021-09-20**|**Dependency Induction Through the Lens of Visual Perception**|Ruisi Su et.al.|[2109.09790](http://arxiv.org/abs/2109.09790)|**[link](https://github.com/ruisi-su/concrete_dep)**|\n", "2109.08478": "|**2021-09-17**|**Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation**|Feilong Chen et.al.|[2109.08478](http://arxiv.org/abs/2109.08478)|**[link](https://github.com/zyang-ur/onestage_grounding)**|\n", "2109.08186": "|**2022-03-02**|**Fast-Slow Transformer for Visually Grounding Speech**|Puyuan Peng et.al.|[2109.08186](http://arxiv.org/abs/2109.08186)|**[link](https://github.com/jasonppy/fast-vgs-family)**|\n", "2109.06122": "|**2022-11-08**|**Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering**|Jihyung Kil et.al.|[2109.06122](http://arxiv.org/abs/2109.06122)|**[link](https://github.com/heendung/simpleaug)**|\n", "2109.04988": "|**2021-09-10**|**Panoptic Narrative Grounding**|C. Gonz\u00e1lez et.al.|[2109.04988](http://arxiv.org/abs/2109.04988)|**[link](https://github.com/bcv-uniandes/png)**|\n", "2109.04869": "|**2022-03-02**|**PlaTe: Visually-Grounded Planning with Transformers in Procedural Tasks**|Jiankai Sun et.al.|[2109.04869](http://arxiv.org/abs/2109.04869)|null|\n", "2109.04600": "|**2021-09-10**|**EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation**|Yanjun Gao et.al.|[2109.04600](http://arxiv.org/abs/2109.04600)|null|\n", "2109.03892": "|**2022-03-25**|**Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models**|Steven Y. Feng et.al.|[2109.03892](http://arxiv.org/abs/2109.03892)|**[link](https://github.com/styfeng/visctg)**|\n", "2109.03084": "|**2021-09-07**|**Learning grounded word meaning representations on similarity graphs**|Mariella Dimiccoli et.al.|[2109.03084](http://arxiv.org/abs/2109.03084)|**[link](https://github.com/mdimiccoli/hm-sge)**|\n", "2108.11092": "|**2024-01-08**|**INVIGORATE: Interactive Visual Grounding and Grasping in Clutter**|Hanbo Zhang et.al.|[2108.11092](http://arxiv.org/abs/2108.11092)|null|\n", "2108.07253": "|**2021-08-17**|**Who's Waldo? Linking People Across Text and Images**|Claire Yuqing Cui et.al.|[2108.07253](http://arxiv.org/abs/2108.07253)|**[link](https://github.com/clairecyq/whos-waldo)**|\n", "2108.05308": "|**2022-02-02**|**A Better Loss for Visual-Textual Grounding**|Davide Rigoni et.al.|[2108.05308](http://arxiv.org/abs/2108.05308)|**[link](https://github.com/drigoni/Loss_VT_Grounding)**|\n", "2108.02388": "|**2021-08-11**|**TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding**|Dailan He et.al.|[2108.02388](http://arxiv.org/abs/2108.02388)|null|\n", "2108.00205": "|**2021-07-31**|**Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding**|Heng Zhao et.al.|[2108.00205](http://arxiv.org/abs/2108.00205)|null|\n", "2107.06546": "|**2021-07-14**|**ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition**|Afra Alishahi et.al.|[2107.06546](http://arxiv.org/abs/2107.06546)|**[link](https://github.com/bhigy/zr-2021vg_baseline)**|\n", "2107.04658": "|**2021-07-09**|**Using Depth for Improving Referring Expression Comprehension in Real-World Environments**|Fethiye Irmak Dogan et.al.|[2107.04658](http://arxiv.org/abs/2107.04658)|null|\n", "2107.03438": "|**2021-11-04**|**LanguageRefer: Spatial-Language Model for 3D Visual Grounding**|Junha Roh et.al.|[2107.03438](http://arxiv.org/abs/2107.03438)|null|\n", "2107.02681": "|**2021-10-19**|**VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer**|Zineng Tang et.al.|[2107.02681](http://arxiv.org/abs/2107.02681)|**[link](https://github.com/zinengtang/VidLanKD)**|\n", "2108.02562": "|**2021-07-05**|**Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models**|Khazar Khorrami et.al.|[2108.02562](http://arxiv.org/abs/2108.02562)|null|\n", "2106.14476": "|**2021-06-28**|**Adventurer's Treasure Hunt: A Transparent System for Visually Grounded Compositional Visual Question Answering based on Scene Graphs**|Daniel Reich et.al.|[2106.14476](http://arxiv.org/abs/2106.14476)|null|\n", "2106.14321": "|**2022-09-30**|**Draw Me a Flower: Processing and Grounding Abstraction in Natural Language**|Royi Lachmy et.al.|[2106.14321](http://arxiv.org/abs/2106.14321)|null|\n", "2106.08859": "|**2021-06-23**|**Attention-Based Keyword Localisation in Speech using Visual Grounding**|Kayode Olaleye et.al.|[2106.08859](http://arxiv.org/abs/2106.08859)|null|\n", "2106.08648": "|**2021-06-16**|**Semantic sentence similarity: size does not always matter**|Danny Merkx et.al.|[2106.08648](http://arxiv.org/abs/2106.08648)|**[link](https://github.com/DannyMerkx/speech2image)**|\n", "2106.04403": "|**2021-06-09**|**SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation**|Ioannis Kazakos et.al.|[2106.04403](http://arxiv.org/abs/2106.04403)|**[link](https://github.com/imatge-upc/synthref)**|\n", "2106.03089": "|**2021-07-14**|**Referring Transformer: A One-step Approach to Multi-task Visual Grounding**|Muchen Li et.al.|[2106.03089](http://arxiv.org/abs/2106.03089)|**[link](https://github.com/ubc-vision/RefTR)**|\n", "2105.11589": "|**2022-03-16**|**VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator**|Ayush Shrivastava et.al.|[2105.11589](http://arxiv.org/abs/2105.11589)|**[link](https://github.com/alexa/visitron)**|\n", "2105.11541": "|**2021-05-24**|**Learning Better Visual Dialog Agents with Pretrained Visual-Linguistic Representation**|Tao Tu et.al.|[2105.11541](http://arxiv.org/abs/2105.11541)|**[link](https://github.com/amazon-research/read-up)**|\n", "2105.11450": "|**2021-09-22**|**SAT: 2D Semantics Assisted Training for 3D Visual Grounding**|Zhengyuan Yang et.al.|[2105.11450](http://arxiv.org/abs/2105.11450)|**[link](https://github.com/zyang-ur/SAT)**|\n", "2105.05964": "|**2021-05-12**|**Connecting What to Say With Where to Look by Modeling Human Attention Traces**|Zihang Meng et.al.|[2105.05964](http://arxiv.org/abs/2105.05964)|**[link](https://github.com/facebookresearch/connect-caption-and-trace)**|\n", "2105.05069": "|**2021-05-11**|**Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols**|Rishi Hazra et.al.|[2105.05069](http://arxiv.org/abs/2105.05069)|null|\n", "2105.04281": "|**2022-03-14**|**Visual Grounding with Transformers**|Ye Du et.al.|[2105.04281](http://arxiv.org/abs/2105.04281)|**[link](https://github.com/usr922/VGTR)**|\n", "2105.03943": "|**2021-05-16**|**gComm: An environment for investigating generalization in Grounded Language Acquisition**|Rishi Hazra et.al.|[2105.03943](http://arxiv.org/abs/2105.03943)|**[link](https://github.com/SonuDixit/gComm)**|\n", "2105.02061": "|**2021-05-05**|**Proposal-free One-stage Referring Expression via Grid-Word Cross-Attention**|Wei Suo et.al.|[2105.02061](http://arxiv.org/abs/2105.02061)|null|\n", "2104.13225": "|**2021-10-06**|**Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques**|Grzegorz Chrupa\u0142a et.al.|[2104.13225](http://arxiv.org/abs/2104.13225)|null|\n", "2104.12763": "|**2021-10-12**|**MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding**|Aishwarya Kamath et.al.|[2104.12763](http://arxiv.org/abs/2104.12763)|**[link](https://github.com/ashkamath/mdetr)**|\n", "2104.11832": "|**2021-12-14**|**Playing Lottery Tickets with Vision and Language**|Zhe Gan et.al.|[2104.11832](http://arxiv.org/abs/2104.11832)|null|\n", "2104.10156": "|**2021-04-20**|**Understanding Synonymous Referring Expressions via Contrastive Features**|Yi-Wen Chen et.al.|[2104.10156](http://arxiv.org/abs/2104.10156)|**[link](https://github.com/wenz116/RefContrast)**|\n", "2104.08560": "|**2021-04-17**|**Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task Feasibility in Interactive Visual Environments**|Andrea Burns et.al.|[2104.08560](http://arxiv.org/abs/2104.08560)|**[link](https://github.com/aburns4/MoTIF)**|\n", "2104.08541": "|**2022-01-14**|**TransVG: End-to-End Visual Grounding with Transformers**|Jiajun Deng et.al.|[2104.08541](http://arxiv.org/abs/2104.08541)|**[link](https://github.com/djiajunustc/TransVG)**|\n", "2104.07500": "|**2021-09-13**|**Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training**|Hassan Shahmohammadi et.al.|[2104.07500](http://arxiv.org/abs/2104.07500)|**[link](https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings)**|\n", "2104.04386": "|**2021-04-09**|**Look Before You Leap: Learning Landmark Features for One-Stage Visual Grounding**|Binbin Huang et.al.|[2104.04386](http://arxiv.org/abs/2104.04386)|**[link](https://github.com/svip-lab/LBYLNet)**|\n", "2104.02026": "|**2021-04-05**|**Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation**|Yapeng Tian et.al.|[2104.02026](http://arxiv.org/abs/2104.02026)|**[link](https://github.com/YapengTian/CCOL-CVPR21)**|\n", "2103.13942": "|**2021-03-25**|**Visual Grounding Strategies for Text-Only Natural Language Processing**|Damien Sileo et.al.|[2103.13942](http://arxiv.org/abs/2103.13942)|null|\n", "2103.12989": "|**2021-03-24**|**Relation-aware Instance Refinement for Weakly Supervised Visual Grounding**|Yongfei Liu et.al.|[2103.12989](http://arxiv.org/abs/2103.12989)|**[link](https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch)**|\n", "2103.12944": "|**2021-03-24**|**Scene-Intuitive Agent for Remote Embodied Visual Grounding**|Xiangru Lin et.al.|[2103.12944](http://arxiv.org/abs/2103.12944)|null|\n", "2103.12346": "|**2021-03-23**|**Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos**|Sijie Song et.al.|[2103.12346](http://arxiv.org/abs/2103.12346)|null|\n", "2103.10191": "|**2021-03-18**|**Decoupled Spatial Temporal Graphs for Generic Visual Grounding**|Qianyu Feng et.al.|[2103.10191](http://arxiv.org/abs/2103.10191)|null|\n", "2103.09720": "|**2021-03-31**|**Few-Shot Visual Grounding for Natural Human-Robot Interaction**|Giorgos Tziafas et.al.|[2103.09720](http://arxiv.org/abs/2103.09720)|null|\n", "2103.07894": "|**2021-03-17**|**Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images**|Haolin Liu et.al.|[2103.07894](http://arxiv.org/abs/2103.07894)|null|\n", "2103.07679": "|**2021-04-14**|**OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene Grounding**|Ke-Jyun Wang et.al.|[2103.07679](http://arxiv.org/abs/2103.07679)|**[link](https://github.com/lluma/OCID-Ref)**|\n", "2103.01910": "|**2022-06-16**|**MultiSubs: A Large-scale Multimodal and Multilingual Dataset**|Josiah Wang et.al.|[2103.01910](http://arxiv.org/abs/2103.01910)|**[link](https://github.com/josiahwang/multisubs-eval)**|\n", "2103.01128": "|**2021-07-29**|**InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring**|Zhihao Yuan et.al.|[2103.01128](http://arxiv.org/abs/2103.01128)|**[link](https://github.com/CurryYuan/InstanceRefer)**|\n", "2102.02779": "|**2021-05-23**|**Unifying Vision-and-Language Tasks via Text Generation**|Jaemin Cho et.al.|[2102.02779](http://arxiv.org/abs/2102.02779)|**[link](https://github.com/j-min/VL-T5)**|\n", "2102.01916": "|**2021-11-08**|**Answer Questions with Right Image Regions: A Visual Attention Regularization Approach**|Yibing Liu et.al.|[2102.01916](http://arxiv.org/abs/2102.01916)|**[link](https://github.com/BierOne/VQA-AttReg)**|\n", "2101.10044": "|**2021-04-20**|**Cross-lingual Visual Pre-training for Multimodal Machine Translation**|Ozan Caglayan et.al.|[2101.10044](http://arxiv.org/abs/2101.10044)|null|\n", "2101.07891": "|**2021-01-19**|**A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment**|Homagni Saha et.al.|[2101.07891](http://arxiv.org/abs/2101.07891)|**[link](https://github.com/Homagn/MOVILAN)**|\n", "2101.12338": "|**2021-01-14**|**Enabling Robots to Draw and Tell: Towards Visually Grounded Multimodal Description Generation**|Ting Han et.al.|[2101.12338](http://arxiv.org/abs/2101.12338)|null|\n", "2101.01169": "|**2022-01-19**|**Transformers in Vision: A Survey**|Salman Khan et.al.|[2101.01169](http://arxiv.org/abs/2101.01169)|null|\n", "2012.15814": "|**2021-06-08**|**Language-Mediated, Object-Centric Representation Learning**|Ruocheng Wang et.al.|[2012.15814](http://arxiv.org/abs/2012.15814)|null|\n", "2012.15454": "|**2020-12-31**|**Text-Free Image-to-Speech Synthesis Using Learned Segmental Units**|Wei-Ning Hsu et.al.|[2012.15454](http://arxiv.org/abs/2012.15454)|null|\n", "2012.10890": "|**2020-12-20**|**PPGN: Phrase-Guided Proposal Generation Network For Referring Expression Comprehension**|Chao Yang et.al.|[2012.10890](http://arxiv.org/abs/2012.10890)|null|\n", "2012.08977": "|**2022-03-14**|**Visually Grounding Language Instruction for History-Dependent Manipulation**|Hyemin Ahn et.al.|[2012.08977](http://arxiv.org/abs/2012.08977)|null|\n", "2012.07277": "|**2021-03-30**|**Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs**|Yifeng Zhu et.al.|[2012.07277](http://arxiv.org/abs/2012.07277)|null|\n", "2012.05011": "|**2023-01-27**|**Intrinsically Motivated Compositional Language Emergence**|Rishi Hazra et.al.|[2012.05011](http://arxiv.org/abs/2012.05011)|**[link](https://github.com/SonuDixit/gComm)**|\n", "2012.04630": "|**2020-12-08**|**CASTing Your Model: Learning to Localize Improves Self-Supervised Representations**|Ramprasaath R. Selvaraju et.al.|[2012.04630](http://arxiv.org/abs/2012.04630)|null|\n", "2011.14204": "|**2020-11-28**|**Class-agnostic Object Detection**|Ayush Jaiswal et.al.|[2011.14204](http://arxiv.org/abs/2011.14204)|null|\n", "2011.10909": "|**2020-11-22**|**Video SemNet: Memory-Augmented Video Semantic Network**|Prashanth Vijayaraghavan et.al.|[2011.10909](http://arxiv.org/abs/2011.10909)|null|\n", "2011.07660": "|**2020-11-15**|**ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments**|Hyounghun Kim et.al.|[2011.07660](http://arxiv.org/abs/2011.07660)|null|\n", "2011.04554": "|**2020-11-09**|**Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts**|Ece Takmaz et.al.|[2011.04554](http://arxiv.org/abs/2011.04554)|null|\n", "2011.03775": "|**2021-03-30**|**Text-to-Image Generation Grounded by Fine-Grained User Attention**|Jing Yu Koh et.al.|[2011.03775](http://arxiv.org/abs/2011.03775)|null|\n", "2010.15288": "|**2020-10-29**|**Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks**|Masood S. Mortazavi et.al.|[2010.15288](http://arxiv.org/abs/2010.15288)|null|\n", "2010.10038": "|**2020-12-01**|**SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency**|Sameer Dharur et.al.|[2010.10038](http://arxiv.org/abs/2010.10038)|**[link](https://github.com/sameerdharur/sorting-vqa)**|\n", "2010.06775": "|**2020-10-14**|**Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision**|Hao Tan et.al.|[2010.06775](http://arxiv.org/abs/2010.06775)|**[link](https://github.com/airsplay/vokenization)**|\n", "2010.03768": "|**2021-03-14**|**ALFWorld: Aligning Text and Embodied Environments for Interactive Learning**|Mohit Shridhar et.al.|[2010.03768](http://arxiv.org/abs/2010.03768)|**[link](https://github.com/alfworld/alfworld)**|\n", "2010.03644": "|**2020-10-07**|**Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations**|Wanrong Zhu et.al.|[2010.03644](http://arxiv.org/abs/2010.03644)|null|\n", "2010.03127": "|**2020-10-07**|**A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions**|Takuma Udagawa et.al.|[2010.03127](http://arxiv.org/abs/2010.03127)|**[link](https://github.com/Alab-NII/onecommon)**|\n", "2010.02949": "|**2020-10-06**|**Learning to Represent Image and Text with Denotation Graph**|Bowen Zhang et.al.|[2010.02949](http://arxiv.org/abs/2010.02949)|null|\n", "2010.02806": "|**2020-10-07**|**Textual Supervision for Visually Grounded Spoken Language Understanding**|Bertrand Higy et.al.|[2010.02806](http://arxiv.org/abs/2010.02806)|**[link](https://github.com/bhigy/textual-supervision)**|\n", "2009.14259": "|**2020-10-26**|**Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions**|Peter A. Jansen et.al.|[2009.14259](http://arxiv.org/abs/2009.14259)|**[link](https://github.com/cognitiveailab/alfred-gpt2)**|\n", "2009.12524": "|**2020-09-26**|**Neural Twins Talk**|Zanyar Zohourianshahzadi et.al.|[2009.12524](http://arxiv.org/abs/2009.12524)|**[link](https://github.com/zanyarz/NeuralTwinsTalk)**|\n", "2009.12404": "|**2020-09-25**|**Visually Grounded Compound PCFGs**|Yanpeng Zhao et.al.|[2009.12404](http://arxiv.org/abs/2009.12404)|**[link](https://github.com/zhaoyanpeng/vpcfg)**|\n", "2009.11278": "|**2020-09-23**|**X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers**|Jaemin Cho et.al.|[2009.11278](http://arxiv.org/abs/2009.11278)|**[link](https://github.com/allenai/x-lxmert)**|\n", "2009.08792": "|**2020-09-18**|**Commands 4 Autonomous Vehicles (C4AV) Workshop Summary**|Thierry Deruyttere et.al.|[2009.08792](http://arxiv.org/abs/2009.08792)|null|\n", "2009.07310": "|**2020-10-13**|**Simultaneous Machine Translation with Visual Context**|Ozan Caglayan et.al.|[2009.07310](http://arxiv.org/abs/2009.07310)|**[link](https://github.com/ImperialNLP/pysimt)**|\n", "2009.06066": "|**2020-09-13**|**Cosine meets Softmax: A tough-to-beat baseline for visual grounding**|Nivedita Rufus et.al.|[2009.06066](http://arxiv.org/abs/2009.06066)|**[link](https://github.com/niveditarufus/CMSVG)**|\n", "2009.05684": "|**2020-12-11**|**AttnGrounder: Talking to Cars with Attention**|Vivek Mittal et.al.|[2009.05684](http://arxiv.org/abs/2009.05684)|**[link](https://github.com/i-m-vivek/AttnGrounder)**|\n", "2008.01059": "|**2020-08-03**|**Improving One-stage Visual Grounding by Recursive Sub-query Construction**|Zhengyuan Yang et.al.|[2008.01059](http://arxiv.org/abs/2008.01059)|**[link](https://github.com/zyang-ur/ReSC)**|\n", "2007.12750": "|**2020-07-24**|**Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data**|Michael Cogswell et.al.|[2007.12750](http://arxiv.org/abs/2007.12750)|**[link](https://github.com/mcogswell/dialog_without_dialog)**|\n", "2007.12146": "|**2020-12-23**|**Spatially Aware Multimodal Transformers for TextVQA**|Yash Kant et.al.|[2007.12146](http://arxiv.org/abs/2007.12146)|**[link](https://github.com/yashkant/sam-textvqa)**|\n", "2007.11668": "|**2020-07-22**|**Analogical Reasoning for Visually Grounded Language Acquisition**|Bo Wu et.al.|[2007.11668](http://arxiv.org/abs/2007.11668)|null|\n", "2007.09554": "|**2020-12-07**|**Referring Expression Comprehension: A Survey of Methods and Datasets**|Yanyuan Qiao et.al.|[2007.09554](http://arxiv.org/abs/2007.09554)|null|\n", "2007.08814": "|**2020-07-21**|**Visual Relation Grounding in Videos**|Junbin Xiao et.al.|[2007.08814](http://arxiv.org/abs/2007.08814)|**[link](https://github.com/doc-doc/vRGV)**|\n", "2007.06198": "|**2020-07-18**|**Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder**|Gouthaman KV et.al.|[2007.06198](http://arxiv.org/abs/2007.06198)|null|\n", "2007.04670": "|**2020-07-10**|**Multi-Granularity Modularized Network for Abstract Visual Reasoning**|Xiangru Tang et.al.|[2007.04670](http://arxiv.org/abs/2007.04670)|null|\n", "2007.03876": "|**2020-07-08**|**Audio-Visual Understanding of Passenger Intents for In-Cabin Conversational Agents**|Eda Okur et.al.|[2007.03876](http://arxiv.org/abs/2007.03876)|null|\n", "2007.01951": "|**2021-04-25**|**Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation**|Liwei Wang et.al.|[2007.01951](http://arxiv.org/abs/2007.01951)|**[link](https://github.com/jhuang81/weak-sup-visual-grounding)**|\n", "2007.00229": "|**2021-02-04**|**Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation**|Wanrong Zhu et.al.|[2007.00229](http://arxiv.org/abs/2007.00229)|null|\n", "2006.09199": "|**2021-06-29**|**AVLnet: Learning Audio-Visual Language Representations from Instructional Videos**|Andrew Rouditchenko et.al.|[2006.09199](http://arxiv.org/abs/2006.09199)|null|\n", "2006.08387": "|**2020-10-20**|**Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech**|William N. Havard et.al.|[2006.08387](http://arxiv.org/abs/2006.08387)|null|\n", "2006.06195": "|**2020-10-22**|**Large-Scale Adversarial Training for Vision-and-Language Representation Learning**|Zhe Gan et.al.|[2006.06195](http://arxiv.org/abs/2006.06195)|**[link](https://github.com/zhegan27/LXMERT-AdvTrain)**|\n", "2006.01629": "|**2020-08-17**|**Give Me Something to Eat: Referring Expression Comprehension with Commonsense Knowledge**|Peng Wang et.al.|[2006.01629](http://arxiv.org/abs/2006.01629)|null|\n", "2006.00512": "|**2020-05-31**|**Learning to Recognise Words using Visually Grounded Speech**|Sebastiaan Scholten et.al.|[2006.00512](http://arxiv.org/abs/2006.00512)|null|\n", "2005.01678": "|**2020-05-18**|**What is Learned in Visually Grounded Neural Syntax Acquisition**|Noriyuki Kojima et.al.|[2005.01678](http://arxiv.org/abs/2005.01678)|**[link](https://github.com/lil-lab/vgnsl_analysis)**|\n", "2005.00785": "|**2020-11-17**|**Visually Grounded Continual Learning of Compositional Phrases**|Xisen Jin et.al.|[2005.00785](http://arxiv.org/abs/2005.00785)|**[link](https://github.com/INK-USC/VG-CCL)**|\n", "2005.00619": "|**2021-04-13**|**Probing Contextual Language Models for Common Ground with Visual Representations**|Gabriel Ilharco et.al.|[2005.00619](http://arxiv.org/abs/2005.00619)|null|\n", "2004.14973": "|**2020-05-01**|**Improving Vision-and-Language Navigation with Image-Text Pairs from the Web**|Arjun Majumdar et.al.|[2004.14973](http://arxiv.org/abs/2004.14973)|**[link](https://github.com/arjunmajum/vln-bert)**|\n", "2004.14025": "|**2020-10-07**|**Multi-View Attention Network for Visual Dialog**|Sungjin Park et.al.|[2004.14025](http://arxiv.org/abs/2004.14025)|**[link](https://github.com/taesunwhang/MVAN-VisDial)**|\n", "2004.13664": "|**2020-06-29**|**Visual Grounding of Learned Physical Models**|Yunzhu Li et.al.|[2004.13664](http://arxiv.org/abs/2004.13664)|null|\n", "2004.13294": "|**2020-04-28**|**A deep learning-based framework for segmenting invisible clinical target volumes with estimated uncertainties for post-operative prostate cancer radiotherapy**|Anjali Balagopal et.al.|[2004.13294](http://arxiv.org/abs/2004.13294)|null|\n", "2004.13278": "|**2020-11-02**|**VD-BERT: A Unified Vision and Dialog Transformer with BERT**|Yue Wang et.al.|[2004.13278](http://arxiv.org/abs/2004.13278)|**[link](https://github.com/salesforce/VD-BERT)**|\n", "2004.12070": "|**2020-10-11**|**Deep Multimodal Neural Architecture Search**|Zhou Yu et.al.|[2004.12070](http://arxiv.org/abs/2004.12070)|**[link](https://github.com/MILVLG/mmnas)**|\n", "2004.05704": "|**2024-04-23**|**Visual Grounding Methods for VQA are Working for the Wrong Reasons!**|Robik Shrestha et.al.|[2004.05704](http://arxiv.org/abs/2004.05704)|**[link](https://github.com/erobic/negative_analysis_of_grounding)**|\n", "2003.13942": "|**2020-03-31**|**Spatio-Temporal Graph for Video Captioning with Knowledge Distillation**|Boxiao Pan et.al.|[2003.13942](http://arxiv.org/abs/2003.13942)|null|\n", "2003.08813": "|**2020-03-19**|**Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation**|Gen Luo et.al.|[2003.08813](http://arxiv.org/abs/2003.08813)|**[link](https://github.com/luogen1996/MCN)**|\n", "2003.08717": "|**2021-05-26**|**Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding**|Thierry Deruyttere et.al.|[2003.08717](http://arxiv.org/abs/2003.08717)|null|\n", "2003.08027": "|**2020-03-20**|**MUTATT: Visual-Textual Mutual Guidance for Referring Expression Comprehension**|Shuai Wang et.al.|[2003.08027](http://arxiv.org/abs/2003.08027)|null|\n", "2003.05078": "|**2020-03-26**|**Visual Grounding in Video for Unsupervised Word Translation**|Gunnar A. Sigurdsson et.al.|[2003.05078](http://arxiv.org/abs/2003.05078)|**[link](https://github.com/gsig/visual-grounding)**|\n", "2003.00403": "|**2020-03-01**|**Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension**|Zhenfang Chen et.al.|[2003.00403](http://arxiv.org/abs/2003.00403)|null|\n", "2002.10340": "|**2020-07-18**|**Guessing State Tracking for Visual Dialogue**|Wei Pang et.al.|[2002.10340](http://arxiv.org/abs/2002.10340)|null|\n", "2002.09604": "|**2020-02-22**|**Emergent Communication with World Models**|Alexander I. Cowen-Rivers et.al.|[2002.09604](http://arxiv.org/abs/2002.09604)|null|\n", "1912.10132": "|**2019-12-20**|**Exploring Context, Attention and Audio Features for Audio Visual Scene-Aware Dialog**|Shachi H Kumar et.al.|[1912.10132](http://arxiv.org/abs/1912.10132)|null|\n", "1912.10131": "|**2019-12-20**|**Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog**|Shachi H Kumar et.al.|[1912.10131](http://arxiv.org/abs/1912.10131)|null|\n", "1912.08360": "|**2019-12-18**|**DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog**|Feilong Chen et.al.|[1912.08360](http://arxiv.org/abs/1912.08360)|**[link](https://github.com/phellonchen/DMRM)**|\n", "1912.06208": "|**2019-12-12**|**Shaping representations through communication: community size effect in artificial learning systems**|Olivier Tieleman et.al.|[1912.06208](http://arxiv.org/abs/1912.06208)|null|\n", "1912.03478": "|**2019-12-07**|**A Real-time Global Inference Network for One-stage Referring Expression Comprehension**|Yiyi Zhou et.al.|[1912.03478](http://arxiv.org/abs/1912.03478)|**[link](https://github.com/luogen1996/Real-time-Global-Inference-Network)**|\n", "1912.03098": "|**2020-07-20**|**Connecting Vision and Language with Localized Narratives**|Jordi Pont-Tuset et.al.|[1912.03098](http://arxiv.org/abs/1912.03098)|**[link](https://github.com/google/localized-narratives)**|\n", "1912.02379": "|**2020-03-31**|**Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline**|Vishvak Murahari et.al.|[1912.02379](http://arxiv.org/abs/1912.02379)|**[link](https://github.com/vmurahari3/visdial-bert)**|\n", "1912.02315": "|**2020-04-24**|**12-in-1: Multi-Task Vision and Language Representation Learning**|Jiasen Lu et.al.|[1912.02315](http://arxiv.org/abs/1912.02315)|**[link](https://github.com/facebookresearch/vilbert-multi-task)**|\n", "1912.02256": "|**2019-12-04**|**Compositional Temporal Visual Grounding of Natural Language Event Descriptions**|Jonathan C. Stroud et.al.|[1912.02256](http://arxiv.org/abs/1912.02256)|null|\n", "1912.00076": "|**2019-11-29**|**OptiBox: Breaking the Limits of Proposals for Visual Grounding**|Zicong Fan et.al.|[1912.00076](http://arxiv.org/abs/1912.00076)|null|\n", "1911.09602": "|**2020-02-14**|**Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech**|David Harwath et.al.|[1911.09602](http://arxiv.org/abs/1911.09602)|null|\n", "1911.09042": "|**2019-11-23**|**Learning Cross-modal Context Graph for Visual Grounding**|Yongfei Liu et.al.|[1911.09042](http://arxiv.org/abs/1911.09042)|**[link](https://github.com/youngfly11/LCMCG-PyTorch)**|\n", "1911.03743": "|**2019-11-09**|**A perspective on multi-agent communication for information fusion**|Homagni Saha et.al.|[1911.03743](http://arxiv.org/abs/1911.03743)|null|\n", "1911.03219": "|**2019-11-08**|**Language Grounding through Social Interactions and Curiosity-Driven Multi-Goal Learning**|Nicolas Lair et.al.|[1911.03219](http://arxiv.org/abs/1911.03219)|null|\n", "1911.02683": "|**2020-06-08**|**Shaping Visual Representations with Language for Few-shot Classification**|Jesse Mu et.al.|[1911.02683](http://arxiv.org/abs/1911.02683)|**[link](https://worksheets.codalab.org/worksheets/0x55ed347e3beb4fa6971d3a226e48fc92)**|\n", "1911.02133": "|**2019-11-05**|**Contextual Grounding of Natural Language Entities in Images**|Farley Lai et.al.|[1911.02133](http://arxiv.org/abs/1911.02133)|**[link](https://gitlab.com/necla-ml/grounding)**|\n", "1911.02103": "|**2019-11-05**|**Recurrent Instance Segmentation using Sequences of Referring Expressions**|Alba Herrera-Palacio et.al.|[1911.02103](http://arxiv.org/abs/1911.02103)|null|\n", "1909.11740": "|**2020-07-17**|**UNITER: UNiversal Image-TExt Representation Learning**|Yen-Chun Chen et.al.|[1909.11740](http://arxiv.org/abs/1909.11740)|**[link](https://github.com/ChenRocks/UNITER)**|\n", "1909.08782": "|**2019-09-19**|**Large-scale representation learning from visually grounded untranscribed speech**|Gabriel Ilharco et.al.|[1909.08782](http://arxiv.org/abs/1909.08782)|null|\n", "1909.08491": "|**2019-09-18**|**Word Recognition, Competition, and Activation in a Model of Visually Grounded Speech**|William N. Havard et.al.|[1909.08491](http://arxiv.org/abs/1909.08491)|null|\n", "1909.08164": "|**2019-09-18**|**Dynamic Graph Attention for Referring Expression Comprehension**|Sibei Yang et.al.|[1909.08164](http://arxiv.org/abs/1909.08164)|null|\n", "1909.07072": "|**2020-04-27**|**A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension**|Yue Liao et.al.|[1909.07072](http://arxiv.org/abs/1909.07072)|null|\n", "1909.04499": "|**2019-09-10**|**Countering Language Drift via Visual Grounding**|Jason Lee et.al.|[1909.04499](http://arxiv.org/abs/1909.04499)|null|\n", "1909.03795": "|**2019-09-09**|**Language learning using Speech to Image retrieval**|Danny Merkx et.al.|[1909.03795](http://arxiv.org/abs/1909.03795)|**[link](https://github.com/DannyMerkx/speech2image)**|\n", "1908.10285": "|**2019-08-27**|**Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts**|Sandro Pezzelle et.al.|[1908.10285](http://arxiv.org/abs/1908.10285)|null|\n", "1908.08530": "|**2020-02-18**|**VL-BERT: Pre-training of Generic Visual-Linguistic Representations**|Weijie Su et.al.|[1908.08530](http://arxiv.org/abs/1908.08530)|**[link](https://github.com/jackroos/VL-BERT)**|\n", "1908.06354": "|**2019-08-18**|**A Fast and Accurate One-Stage Approach to Visual Grounding**|Zhengyuan Yang et.al.|[1908.06354](http://arxiv.org/abs/1908.06354)|**[link](https://github.com/zyang-ur/onestage_grounding)**|\n", "1908.04107": "|**2019-08-19**|**Multimodal Unified Attention Networks for Vision-and-Language Interactions**|Zhou Yu et.al.|[1908.04107](http://arxiv.org/abs/1908.04107)|null|\n", "1908.03557": "|**2019-08-09**|**VisualBERT: A Simple and Performant Baseline for Vision and Language**|Liunian Harold Li et.al.|[1908.03557](http://arxiv.org/abs/1908.03557)|null|\n", "1908.02943": "|**2019-08-08**|**Towards Generating Stylized Image Captions via Adversarial Training**|Omid Mohamad Nezami et.al.|[1908.02943](http://arxiv.org/abs/1908.02943)|null|\n", "1908.02265": "|**2019-08-06**|**ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks**|Jiasen Lu et.al.|[1908.02265](http://arxiv.org/abs/1908.02265)|null|\n", "1907.07507": "|**2019-07-24**|**Differentiable Disentanglement Filter: an Application Agnostic Core Concept Discovery Probe**|Guntis Barzdins et.al.|[1907.07507](http://arxiv.org/abs/1907.07507)|null|\n", "1907.04355": "|**2019-07-09**|**Transfer Learning from Audio-Visual Grounding to Speech Recognition**|Wei-Ning Hsu et.al.|[1907.04355](http://arxiv.org/abs/1907.04355)|null|\n", "1907.02022": "|**2019-11-26**|**Chasing Ghosts: Instruction Following as Bayesian State Tracking**|Peter Anderson et.al.|[1907.02022](http://arxiv.org/abs/1907.02022)|**[link](https://github.com/batra-mlp-lab/vln-chasing-ghosts)**|\n", "1907.01166": "|**2019-07-02**|**Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems**|Hung Le et.al.|[1907.01166](http://arxiv.org/abs/1907.01166)|**[link](https://github.com/henryhungle/MTN)**|\n", "1907.00477": "|**2019-12-28**|**Analyzing Utility of Visual Context in Multimodal Speech Recognition Under Noisy Conditions**|Tejas Srinivasan et.al.|[1907.00477](http://arxiv.org/abs/1907.00477)|null|\n", "1906.06147": "|**2019-07-28**|**Grounding Object Detections With Transcriptions**|Yasufumi Moriya et.al.|[1906.06147](http://arxiv.org/abs/1906.06147)|null|\n", "1906.03952": "|**2019-06-10**|**Multimodal Logical Inference System for Visual-Textual Entailment**|Riko Suzuki et.al.|[1906.03952](http://arxiv.org/abs/1906.03952)|null|\n", "1906.03561": "|**2020-04-10**|**Joint Visual Grounding with Language Scene Graphs**|Daqing Liu et.al.|[1906.03561](http://arxiv.org/abs/1906.03561)|null|\n", "1906.02890": "|**2019-09-24**|**Visually Grounded Neural Syntax Acquisition**|Haoyue Shi et.al.|[1906.02890](http://arxiv.org/abs/1906.02890)|null|\n", "1906.01784": "|**2019-06-05**|**Learning to Compose and Reason with Language Tree Structures for Visual Grounding**|Richang Hong et.al.|[1906.01784](http://arxiv.org/abs/1906.01784)|null|\n", "1906.01530": "|**2019-06-26**|**The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue**|Janosch Haber et.al.|[1906.01530](http://arxiv.org/abs/1906.01530)|null|\n", "1904.10947": "|**2019-08-30**|**On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval**|Ankita Pasad et.al.|[1904.10947](http://arxiv.org/abs/1904.10947)|null|\n", "1904.07078": "|**2019-04-15**|**Semantic query-by-example speech search using visual grounding**|Herman Kamper et.al.|[1904.07078](http://arxiv.org/abs/1904.07078)|null|\n", "1904.06038": "|**2019-04-12**|**Evaluating the Representational Hub of Language and Vision Models**|Ravi Shekhar et.al.|[1904.06038](http://arxiv.org/abs/1904.06038)|null|\n", "1904.03589": "|**2019-07-01**|**Modularized Textual Grounding for Counterfactual Resilience**|Zhiyuan Fang et.al.|[1904.03589](http://arxiv.org/abs/1904.03589)|**[link](https://github.com/jacobswan1/MTG-pytorch)**|\n", "1904.02794": "|**2019-04-11**|**VQD: Visual Query Detection in Natural Scenes**|Manoj Acharya et.al.|[1904.02794](http://arxiv.org/abs/1904.02794)|null|\n", "1904.02225": "|**2019-04-03**|**Revisiting Visual Grounding**|Erik Conser et.al.|[1904.02225](http://arxiv.org/abs/1904.02225)|null|\n", "1903.11649": "|**2019-10-15**|**Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment**|Samyak Datta et.al.|[1903.11649](http://arxiv.org/abs/1903.11649)|null|\n", "1903.11393": "|**2019-03-27**|**Learning semantic sentence representations from visually grounded language without lexical knowledge**|Danny Merkx et.al.|[1903.11393](http://arxiv.org/abs/1903.11393)|**[link](https://github.com/DannyMerkx/caption2image)**|\n", "1902.09368": "|**2019-08-29**|**Dual Attention Networks for Visual Reference Resolution in Visual Dialog**|Gi-Cheon Kang et.al.|[1902.09368](http://arxiv.org/abs/1902.09368)|**[link](https://github.com/gicheonkang/DAN-VisDial)**|\n", "1902.09326": "|**2019-04-17**|**Making History Matter: History-Advantage Sequence Training for Visual Dialog**|Tianhao Yang et.al.|[1902.09326](http://arxiv.org/abs/1902.09326)|null|\n", "1902.08213": "|**2019-02-21**|**Towards Visually Grounded Sub-Word Speech Unit Discovery**|David Harwath et.al.|[1902.08213](http://arxiv.org/abs/1902.08213)|null|\n", "1902.04213": "|**2019-03-17**|**You Only Look & Listen Once: Towards Fast and Accurate Visual Grounding**|Chaorui Deng et.al.|[1902.04213](http://arxiv.org/abs/1902.04213)|null|\n", "1902.03751": "|**2019-10-28**|**Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded**|Ramprasaath R. Selvaraju et.al.|[1902.03751](http://arxiv.org/abs/1902.03751)|null|\n", "1902.03052": "|**2019-02-08**|**Models of Visually Grounded Speech Signal Pay Attention To Nouns: a Bilingual Experiment on English and Japanese**|William N. Havard et.al.|[1902.03052](http://arxiv.org/abs/1902.03052)|**[link](https://github.com/William-N-Havard/VGS-dataset-metadata)**|\n", "1901.00850": "|**2019-04-06**|**CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions**|Runtao Liu et.al.|[1901.00850](http://arxiv.org/abs/1901.00850)|null|\n", "1812.09244": "|**2019-06-05**|**Symbolic inductive bias for visually grounded learning of spoken language**|Grzegorz Chrupa\u0142a et.al.|[1812.09244](http://arxiv.org/abs/1812.09244)|**[link](https://github.com/gchrupala/symbolic-bias)**|\n", "1812.08407": "|**2018-12-20**|**Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog**|Shachi H Kumar et.al.|[1812.08407](http://arxiv.org/abs/1812.08407)|null|\n", "1812.04794": "|**2018-12-12**|**Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks**|Peng Wang et.al.|[1812.04794](http://arxiv.org/abs/1812.04794)|null|\n", "1812.03426": "|**2018-12-09**|**Real-Time Referring Expression Comprehension by Single-Stage Grounding Network**|Xinpeng Chen et.al.|[1812.03426](http://arxiv.org/abs/1812.03426)|null|\n", "1812.03299": "|**2019-10-21**|**Learning to Assemble Neural Module Tree Networks for Visual Grounding**|Daqing Liu et.al.|[1812.03299](http://arxiv.org/abs/1812.03299)|null|\n", "1812.02664": "|**2019-04-06**|**Recursive Visual Attention in Visual Dialog**|Yulei Niu et.al.|[1812.02664](http://arxiv.org/abs/1812.02664)|**[link](https://github.com/yuleiniu/rva)**|\n", "1812.00500": "|**2018-12-03**|**Multi-task Learning of Hierarchical Vision-Language Representation**|Duy-Kien Nguyen et.al.|[1812.00500](http://arxiv.org/abs/1812.00500)|null|\n", "1811.10740": "|**2018-12-01**|**Mixture of Regression Experts in fMRI Encoding**|Subba Reddy Oota et.al.|[1811.10740](http://arxiv.org/abs/1811.10740)|null|\n", "1811.10582": "|**2019-01-21**|**Visual Entailment Task for Visually-Grounded Language Learning**|Ning Xie et.al.|[1811.10582](http://arxiv.org/abs/1811.10582)|**[link](https://github.com/necla-ml/SNLI-VE)**|\n", "1811.10561": "|**2018-11-26**|**CLEAR: A Dataset for Compositional Language and Elementary Acoustic Reasoning**|Jerome Abdelnour et.al.|[1811.10561](http://arxiv.org/abs/1811.10561)|**[link](https://github.com/IGLU-CHISTERA/CLEAR-dataset-generation)**|\n", "1811.06529": "|**2018-11-16**|**On transfer learning using a MAC model variant**|Vincent Marois et.al.|[1811.06529](http://arxiv.org/abs/1811.06529)|null|\n", "1811.05013": "|**2018-11-12**|**Blindfold Baselines for Embodied QA**|Ankesh Anand et.al.|[1811.05013](http://arxiv.org/abs/1811.05013)|**[link](https://github.com/ankeshanand/blindfold-baselines-eqa)**|\n", "1811.04498": "|**2018-11-11**|**Product Title Refinement via Multi-Modal Generative Adversarial Learning**|Jianguo Zhang et.al.|[1811.04498](http://arxiv.org/abs/1811.04498)|null|\n", "1810.11954": "|**2018-10-20**|**A Knowledge-Grounded Multimodal Search-Based Conversational Agent**|Shubham Agarwal et.al.|[1810.11954](http://arxiv.org/abs/1810.11954)|**[link](https://github.com/shubhamagarwal92/mmd)**|\n", "1810.03649": "|**2018-11-08**|**Overcoming Language Priors in Visual Question Answering with Adversarial Regularization**|Sainandan Ramakrishnan et.al.|[1810.03649](http://arxiv.org/abs/1810.03649)|null|\n", "1809.04560": "|**2018-10-17**|**Game-Based Video-Context Dialogue**|Ramakanth Pasunuru et.al.|[1809.04560](http://arxiv.org/abs/1809.04560)|**[link](https://github.com/ramakanth-pasunuru/video-dialogue)**|\n", "1809.03408": "|**2019-03-15**|**Beyond task success: A closer look at jointly learning to see, ask, and GuessWhat**|Ravi Shekhar et.al.|[1809.03408](http://arxiv.org/abs/1809.03408)|null|\n", "1809.01816": "|**2018-09-06**|**Visual Coreference Resolution in Visual Dialog using Neural Module Networks**|Satwik Kottur et.al.|[1809.01816](http://arxiv.org/abs/1809.01816)|null|\n", "1808.04359": "|**2018-09-06**|**Community Regularization of Visually-Grounded Dialog**|Akshat Agarwal et.al.|[1808.04359](http://arxiv.org/abs/1808.04359)|**[link](https://github.com/agakshat/visualdialog-pytorch)**|\n", "1808.00265": "|**2018-08-01**|**Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining**|Yundong Zhang et.al.|[1808.00265](http://arxiv.org/abs/1808.00265)|null|\n", "1806.10348": "|**2018-06-27**|**Learning Visually-Grounded Semantics from Contrastive Adversarial Samples**|Haoyue Shi et.al.|[1806.10348](http://arxiv.org/abs/1806.10348)|**[link](https://github.com/ExplorerFreda/VSE-C)**|\n", "1806.05645": "|**2018-06-14**|**Grounded Textual Entailment**|Hoa Trong Vu et.al.|[1806.05645](http://arxiv.org/abs/1806.05645)|**[link](https://github.com/claudiogreco/coling18-gte)**|\n", "1806.05030": "|**2018-06-13**|**Visually grounded cross-lingual keyword spotting in speech**|Herman Kamper et.al.|[1806.05030](http://arxiv.org/abs/1806.05030)|null|\n", "1806.04284": "|**2018-06-12**|**iParaphrasing: Extracting Visually Grounded Paraphrases via an Image**|Chenhui Chu et.al.|[1806.04284](http://arxiv.org/abs/1806.04284)|**[link](https://github.com/ids-cv/coling_iparaphrasing)**|\n", "1806.03831": "|**2018-06-11**|**Interactive Visual Grounding of Referring Expressions for Human-Robot Interaction**|Mohit Shridhar et.al.|[1806.03831](http://arxiv.org/abs/1806.03831)|null|\n", "1805.07030": "|**2018-05-18**|**SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text**|Alexander Mathews et.al.|[1805.07030](http://arxiv.org/abs/1805.07030)|**[link](https://github.com/computationalmedia/semstyle)**|\n", "1805.06960": "|**2018-06-12**|**Ask No More: Deciding when to guess in referential visual dialogue**|Ravi Shekhar et.al.|[1805.06960](http://arxiv.org/abs/1805.06960)|null|\n", "1805.03508": "|**2018-05-09**|**Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding**|Zhou Yu et.al.|[1805.03508](http://arxiv.org/abs/1805.03508)|**[link](https://github.com/xiangchenchao/ddpn)**|\n", "1804.08454": "|**2018-12-21**|**Attention Based Natural Language Grounding by Navigating Virtual Environment**|Akilesh B et.al.|[1804.08454](http://arxiv.org/abs/1804.08454)|**[link](https://github.com/rl-lang-grounding/rl-lang-ground)**|\n", "1804.01720": "|**2018-04-06**|**Finding beans in burgers: Deep semantic-visual embedding with localization**|Martin Engilberge et.al.|[1804.01720](http://arxiv.org/abs/1804.01720)|**[link](https://github.com/technicolor-research/dsve-loc)**|\n", "1803.06506": "|**2018-11-16**|**Learning Unsupervised Visual Grounding Through Semantic Self-Supervision**|Syed Ashar Javed et.al.|[1803.06506](http://arxiv.org/abs/1803.06506)|null|\n", "1802.03803": "|**2018-04-03**|**FlipDial: A Generative Model for Two-Way Visual Dialogue**|Daniela Massiceti et.al.|[1802.03803](http://arxiv.org/abs/1802.03803)|null|\n", "1801.08186": "|**2018-03-27**|**MAttNet: Modular Attention Network for Referring Expression Comprehension**|Licheng Yu et.al.|[1801.08186](http://arxiv.org/abs/1801.08186)|**[link](https://github.com/lichengunc/MAttNet)**|\n", "1712.06228": "|**2017-12-18**|**Visual Explanations from Hadamard Product in Multimodal Deep Networks**|Jin-Hwa Kim et.al.|[1712.06228](http://arxiv.org/abs/1712.06228)|null|\n", "1712.03449": "|**2017-12-09**|**Modulating and attending the source image during encoding improves Multimodal Translation**|Jean-Benoit Delbrouck et.al.|[1712.03449](http://arxiv.org/abs/1712.03449)|**[link](https://github.com/jbdel/mmt_cbn)**|\n", "1712.00609": "|**2017-12-02**|**Improving Visually Grounded Sentence Representations with Self-Attention**|Kang Min Yoo et.al.|[1712.00609](http://arxiv.org/abs/1712.00609)|null|\n", "1712.00576": "|**2017-12-02**|**Interactive Reinforcement Learning for Object Grounding via Self-Talking**|Yan Zhu et.al.|[1712.00576](http://arxiv.org/abs/1712.00576)|null|\n", "1711.11017": "|**2017-11-29**|**HoME: a Household Multimodal Environment**|Simon Brodeur et.al.|[1711.11017](http://arxiv.org/abs/1711.11017)|null|\n", "1711.10837": "|**2017-11-29**|**Curriculum Q-Learning for Visual Vocabulary Acquisition**|Ahmed H. Zaidi et.al.|[1711.10837](http://arxiv.org/abs/1711.10837)|null|\n", "1711.08664": "|**2017-11-23**|**Self-view Grounding Given a Narrated 360\u00b0 Video**|Shih-Han Chou et.al.|[1711.08664](http://arxiv.org/abs/1711.08664)|null|\n", "1711.07280": "|**2018-04-05**|**Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments**|Peter Anderson et.al.|[1711.07280](http://arxiv.org/abs/1711.07280)|**[link](https://github.com/peteanderson80/Matterport3DSimulator)**|\n", "1711.06370": "|**2017-11-17**|**Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries**|Bohan Zhuang et.al.|[1711.06370](http://arxiv.org/abs/1711.06370)|null|\n", "1710.01949": "|**2018-10-31**|**Semantic speech retrieval with a visually grounded model of untranscribed speech**|Herman Kamper et.al.|[1710.01949](http://arxiv.org/abs/1710.01949)|**[link](https://github.com/kamperh/semantic_flickraudio)**|\n", "1709.10431": "|**2017-09-29**|**The BURCHAK corpus: a Challenge Data Set for Interactive Learning of Visually Grounded Word Meanings**|Yanchao Yu et.al.|[1709.10431](http://arxiv.org/abs/1709.10431)|null|\n", "1709.10426": "|**2017-09-29**|**Training an adaptive dialogue policy for interactive learning of visually grounded word meanings**|Yanchao Yu et.al.|[1709.10426](http://arxiv.org/abs/1709.10426)|null|\n", "1709.10423": "|**2017-09-29**|**Learning how to learn: an adaptive dialogue agent for incrementally learning visually grounded word meanings**|Yanchao Yu et.al.|[1709.10423](http://arxiv.org/abs/1709.10423)|null|\n", "1709.07992": "|**2018-08-06**|**Visual Reference Resolution using Attention Memory for Visual Dialog**|Paul Hongsuck Seo et.al.|[1709.07992](http://arxiv.org/abs/1709.07992)|null|\n", "1707.08435": "|**2020-11-23**|**SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set**|William Havard et.al.|[1707.08435](http://arxiv.org/abs/1707.08435)|**[link](https://github.com/William-N-Havard/SpeechCoco)**|\n", "1707.06320": "|**2018-06-04**|**Learning Visually Grounded Sentence Representations**|Douwe Kiela et.al.|[1707.06320](http://arxiv.org/abs/1707.06320)|null|\n", "1707.01009": "|**2017-12-16**|**Visually Grounded Word Embeddings and Richer Visual Features for Improving Multimodal Neural Machine Translation**|Jean-Benoit Delbrouck et.al.|[1707.01009](http://arxiv.org/abs/1707.01009)|null|\n", "1705.10762": "|**2018-11-09**|**Generative Models of Visually Grounded Imagination**|Ramakrishna Vedantam et.al.|[1705.10762](http://arxiv.org/abs/1705.10762)|null|\n", "1705.04350": "|**2017-07-07**|**Imagination improves Multimodal Translation**|Desmond Elliott et.al.|[1705.04350](http://arxiv.org/abs/1705.04350)|null|\n", "1705.01371": "|**2017-05-03**|**Weakly-supervised Visual Grounding of Phrases with Linguistic Structures**|Fanyi Xiao et.al.|[1705.01371](http://arxiv.org/abs/1705.01371)|null|\n", "1704.03470": "|**2018-05-01**|**Learning Two-Branch Neural Networks for Image-Text Matching Tasks**|Liwei Wang et.al.|[1704.03470](http://arxiv.org/abs/1704.03470)|null|\n", "1703.08136": "|**2017-05-25**|**Visually grounded learning of keyword prediction from untranscribed speech**|Herman Kamper et.al.|[1703.08136](http://arxiv.org/abs/1703.08136)|**[link](https://github.com/kamperh/recipe_vision_speech_flickr)**|\n", "1703.05423": "|**2017-03-15**|**End-to-end optimization of goal-driven and visually grounded dialogue systems**|Florian Strub et.al.|[1703.05423](http://arxiv.org/abs/1703.05423)|null|\n", "1702.01991": "|**2017-06-30**|**Representations of language in a model of visually grounded speech signal**|Grzegorz Chrupa\u0142a et.al.|[1702.01991](http://arxiv.org/abs/1702.01991)|**[link](https://github.com/gchrupala/visually-grounded-speech)**|\n", "1701.08251": "|**2017-04-20**|**Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation**|Nasrin Mostafazadeh et.al.|[1701.08251](http://arxiv.org/abs/1701.08251)|null|\n", "1701.02426": "|**2017-04-12**|**Scene Graph Generation by Iterative Message Passing**|Danfei Xu et.al.|[1701.02426](http://arxiv.org/abs/1701.02426)|null|\n", "1612.09542": "|**2017-04-17**|**A Joint Speaker-Listener-Reinforcer Model for Referring Expressions**|Licheng Yu et.al.|[1612.09542](http://arxiv.org/abs/1612.09542)|null|\n", "1612.07833": "|**2016-12-22**|**Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task**|Nan Ding et.al.|[1612.07833](http://arxiv.org/abs/1612.07833)|null|\n", "1612.06530": "|**2017-05-29**|**Automatic Generation of Grounded Visual Questions**|Shijie Zhang et.al.|[1612.06530](http://arxiv.org/abs/1612.06530)|null|\n", "1610.03342": "|**2016-10-11**|**From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning**|Lieke Gelderloos et.al.|[1610.03342](http://arxiv.org/abs/1610.03342)|null|\n", "1606.08390": "|**2016-11-22**|**Revisiting Visual Question Answering Baselines**|Allan Jabri et.al.|[1606.08390](http://arxiv.org/abs/1606.08390)|null|\n", "1606.01847": "|**2016-09-24**|**Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**|Akira Fukui et.al.|[1606.01847](http://arxiv.org/abs/1606.01847)|**[link](https://github.com/akirafukui/vqa-mcb)**|\n", "1512.06974": "|**2016-04-12**|**Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels**|Ishan Misra et.al.|[1512.06974](http://arxiv.org/abs/1512.06974)|null|\n", "1511.07067": "|**2016-06-29**|**Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes**|Satwik Kottur et.al.|[1511.07067](http://arxiv.org/abs/1511.07067)|null|\n", "1506.03694": "|**2015-06-19**|**Learning language through pictures**|Grzegorz Chrupa\u0142a et.al.|[1506.03694](http://arxiv.org/abs/1506.03694)|**[link](https://github.com/gchrupala/imaginet)**|\n", "1107.0031": "|**2011-06-30**|**Grounded Semantic Composition for Visual Scenes**|P. Gorniak et.al.|[1107.0031](http://arxiv.org/abs/1107.0031)|null|\n", "2412.15190": "|**2024-12-19**|**EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues**|Sagar Soni et.al.|[2412.15190](http://arxiv.org/abs/2412.15190)|null|\n", "2412.14672": "|**2024-12-19**|**FiVL: A Framework for Improved Vision-Language Alignment**|Estelle Aflalo et.al.|[2412.14672](http://arxiv.org/abs/2412.14672)|null|\n", "2412.13654": "|**2024-12-18**|**GAGS: Granularity-Aware Feature Distillation for Language Gaussian Splatting**|Yuning Peng et.al.|[2412.13654](http://arxiv.org/abs/2412.13654)|null|\n"}, "RVOS": {"2412.01136": "|**2024-12-02**|**Referring Video Object Segmentation via Language-aligned Track Selection**|Seongchan Kim et.al.|[2412.01136](http://arxiv.org/abs/2412.01136)|null|\n", "2411.17646": "|**2024-11-26**|**SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation**|Claudia Cuttano et.al.|[2411.17646](http://arxiv.org/abs/2411.17646)|**[link](https://github.com/claudiacuttano/samwise)**|\n", "2409.05847": "|**2024-09-09**|**LSVOS Challenge Report: Large-scale Complex and Long Video Object Segmentation**|Henghui Ding et.al.|[2409.05847](http://arxiv.org/abs/2409.05847)|null|\n", "2408.12447": "|**2024-08-22**|**The 2nd Solution for LSVOS Challenge RVOS Track: Spatial-temporal Refinement for Consistent Semantic Segmentation**|Tuyen Tran et.al.|[2408.12447](http://arxiv.org/abs/2408.12447)|null|\n", "2408.10541": "|**2024-08-20**|**The Instance-centric Transformer for the RVOS Track of LSVOS Challenge: 3rd Place Solution**|Bin Cao et.al.|[2408.10541](http://arxiv.org/abs/2408.10541)|null|\n", "2408.10129": "|**2024-08-24**|**UNINEXT-Cutie: The 1st Solution for LSVOS Challenge RVOS Track**|Hao Fang et.al.|[2408.10129](http://arxiv.org/abs/2408.10129)|null|\n", "2407.07402": "|**2024-07-10**|**ActionVOS: Actions as Prompts for Video Object Segmentation**|Liangyang Ouyang et.al.|[2407.07402](http://arxiv.org/abs/2407.07402)|**[link](https://github.com/ut-vision/actionvos)**|\n", "2406.13939": "|**2024-06-20**|**2nd Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation**|Bin Cao et.al.|[2406.13939](http://arxiv.org/abs/2406.13939)|null|\n", "2406.12834": "|**2024-06-23**|**GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation**|Ci-Siang Lin et.al.|[2406.12834](http://arxiv.org/abs/2406.12834)|null|\n", "2406.07043": "|**2024-06-11**|**1st Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation**|Mingqi Gao et.al.|[2406.07043](http://arxiv.org/abs/2406.07043)|**[link](https://github.com/tapall-ai/mevis_track_solution_2024)**|\n", "2406.04842": "|**2024-06-07**|**3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation**|Feiyu Pan et.al.|[2406.04842](http://arxiv.org/abs/2406.04842)|null|\n", "2405.10610": "|**2024-09-22**|**Harnessing Vision-Language Pretrained Models with Temporal-Aware Adaptation for Referring Video Object Segmentation**|Zikun Zhou et.al.|[2405.10610](http://arxiv.org/abs/2405.10610)|null|\n", "2403.19407": "|**2024-10-11**|**Temporally Consistent Referring Video Object Segmentation with Hybrid Memory**|Bo Miao et.al.|[2403.19407](http://arxiv.org/abs/2403.19407)|**[link](https://github.com/bo-miao/HTR)**|\n", "2403.12042": "|**2024-07-06**|**Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation**|Zixin Zhu et.al.|[2403.12042](http://arxiv.org/abs/2403.12042)|**[link](https://github.com/buxiangzhiren/vd-it)**|\n", "2401.00663": "|**2024-01-01**|**1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation**|Zhuoyan Luo et.al.|[2401.00663](http://arxiv.org/abs/2401.00663)|**[link](https://github.com/robertluo1/iccv2023_rvos_challenge)**|\n", "2312.17448": "|**2023-12-29**|**Tracking with Human-Intent Reasoning**|Jiawen Zhu et.al.|[2312.17448](http://arxiv.org/abs/2312.17448)|**[link](https://github.com/jiawen-zhu/trackgpt)**|\n", "2312.15715": "|**2023-12-25**|**UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces**|Jiannan Wu et.al.|[2312.15715](http://arxiv.org/abs/2312.15715)|**[link](https://github.com/foundationvision/uniref)**|\n", "2309.11933": "|**2023-09-21**|**Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation**|Ping Li et.al.|[2309.11933](http://arxiv.org/abs/2309.11933)|null|\n", "2309.03473": "|**2023-09-07**|**Temporal Collection and Distribution for Referring Video Object Segmentation**|Jiajin Tang et.al.|[2309.03473](http://arxiv.org/abs/2309.03473)|null|\n", "2309.02041": "|**2023-09-05**|**Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples**|Guanghui Li et.al.|[2309.02041](http://arxiv.org/abs/2309.02041)|**[link](https://github.com/hengliusky/few_shot_rvos)**|\n", "2308.08544": "|**2023-08-16**|**MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions**|Henghui Ding et.al.|[2308.08544](http://arxiv.org/abs/2308.08544)|**[link](https://github.com/henghuiding/MeViS)**|\n", "2308.04162": "|**2023-08-08**|**EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation**|Jiajun Chen et.al.|[2308.04162](http://arxiv.org/abs/2308.04162)|**[link](https://github.com/lab206/epcformer)**|\n", "2308.02162": "|**2023-12-15**|**Learning Referring Video Object Segmentation from Weak Annotation**|Wangbo Zhao et.al.|[2308.02162](http://arxiv.org/abs/2308.02162)|null|\n", "2307.13537": "|**2023-07-25**|**Spectrum-guided Multi-granularity Referring Video Object Segmentation**|Bo Miao et.al.|[2307.13537](http://arxiv.org/abs/2307.13537)|**[link](https://github.com/bo-miao/sgmg)**|\n", "2307.09356": "|**2023-07-18**|**OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation**|Dongming Wu et.al.|[2307.09356](http://arxiv.org/abs/2307.09356)|**[link](https://github.com/wudongming97/onlinerefer)**|\n", "2307.00997": "|**2024-09-03**|**RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation**|Yonglin Li et.al.|[2307.00997](http://arxiv.org/abs/2307.00997)|**[link](https://github.com/lancasterli/refsam)**|\n", "2307.00536": "|**2023-09-17**|**Bidirectional Correlation-Driven Inter-Frame Interaction Transformer for Referring Video Object Segmentation**|Meng Lan et.al.|[2307.00536](http://arxiv.org/abs/2307.00536)|null|\n", "2306.08736": "|**2024-04-02**|**LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation**|Linfeng Yuan et.al.|[2306.08736](http://arxiv.org/abs/2306.08736)|**[link](https://github.com/linfengyuan1997/losh)**|\n", "2305.17011": "|**2023-05-26**|**SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation**|Zhuoyan Luo et.al.|[2305.17011](http://arxiv.org/abs/2305.17011)|**[link](https://github.com/RobertLuo1/NeurIPS2023_SOC)**|\n", "2305.16318": "|**2023-12-12**|**Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation**|Shilin Yan et.al.|[2305.16318](http://arxiv.org/abs/2305.16318)|**[link](https://github.com/opengvlab/mutr)**|\n", "2212.14679": "|**2022-12-27**|**1st Place Solution for YouTubeVOS Challenge 2022: Referring Video Object Segmentation**|Zhiwei Hu et.al.|[2212.14679](http://arxiv.org/abs/2212.14679)|**[link](https://github.com/zhiweihhh/cvpr2022-rvos-challenge)**|\n", "2207.12622": "|**2022-07-26**|**Multi-Attention Network for Compressed Video Referring Object Segmentation**|Weidong Chen et.al.|[2207.12622](http://arxiv.org/abs/2207.12622)|**[link](https://github.com/dexianghong/manet)**|\n", "2207.01203": "|**2023-08-18**|**Towards Robust Referring Video Object Segmentation with Cyclic Relational Consensus**|Xiang Li et.al.|[2207.01203](http://arxiv.org/abs/2207.01203)|**[link](https://github.com/lxa9867/R2VOS)**|\n", "2206.12035": "|**2022-06-24**|**The Second Place Solution for The 4th Large-scale Video Object Segmentation Challenge--Track 3: Referring Video Object Segmentation**|Leilei Cao et.al.|[2206.12035](http://arxiv.org/abs/2206.12035)|null|\n", "2206.03789": "|**2022-06-08**|**Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation**|Zihan Ding et.al.|[2206.03789](http://arxiv.org/abs/2206.03789)|**[link](https://github.com/dzh19990407/lbdt)**|\n", "2203.09773": "|**2024-01-19**|**Local-Global Context Aware Transformer for Language-Guided Video Segmentation**|Chen Liang et.al.|[2203.09773](http://arxiv.org/abs/2203.09773)|**[link](https://github.com/leonnnop/locater)**|\n", "2201.00487": "|**2022-03-13**|**Language as Queries for Referring Video Object Segmentation**|Jiannan Wu et.al.|[2201.00487](http://arxiv.org/abs/2201.00487)|**[link](https://github.com/wjn922/referformer)**|\n", "2111.14821": "|**2022-04-03**|**End-to-End Referring Video Object Segmentation with Multimodal Transformers**|Adam Botach et.al.|[2111.14821](http://arxiv.org/abs/2111.14821)|**[link](https://github.com/mttr2021/MTTR)**|\n", "2106.01061": "|**2024-01-19**|**Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation**|Chen Liang et.al.|[2106.01061](http://arxiv.org/abs/2106.01061)|null|\n"}, "RMOT": {}, "AVL": {"2412.09878": "|**2024-12-13**|**SonicBoom: Contact Localization Using Array of Microphones**|Moonyoung Lee et.al.|[2412.09878](http://arxiv.org/abs/2412.09878)|null|\n", "2412.06209": "|**2024-12-09**|**Sound2Vision: Generating Diverse Visuals from Audio through Cross-Modal Latent Alignment**|Kim Sung-Bin et.al.|[2412.06209](http://arxiv.org/abs/2412.06209)|null|\n", "2410.19595": "|**2024-10-25**|**Mask-Weighted Spatial Likelihood Coding for Speaker-Independent Joint Localization and Mask Estimation**|Jakob Kienegger et.al.|[2410.19595](http://arxiv.org/abs/2410.19595)|null|\n", "2410.01020": "|**2024-10-01**|**A Critical Assessment of Visual Sound Source Localization Models Including Negative Audio**|Xavier Juanola et.al.|[2410.01020](http://arxiv.org/abs/2410.01020)|**[link](https://github.com/xavijuanola/vssl_eval)**|\n", "2409.20031": "|**2024-09-30**|**Adaptive high-precision sound source localization at low frequencies based on convolutional neural network**|Wenbo Ma et.al.|[2409.20031](http://arxiv.org/abs/2409.20031)|null|\n", "2409.07224": "|**2024-09-11**|**Analytic Class Incremental Learning for Sound Source Localization with Privacy Protection**|Xinyuan Qian et.al.|[2409.07224](http://arxiv.org/abs/2409.07224)|null|\n", "2409.05034": "|**2024-09-08**|**TF-Mamba: A Time-Frequency Network for Sound Source Localization**|Yang Xiao et.al.|[2409.05034](http://arxiv.org/abs/2409.05034)|null|\n", "2409.04843": "|**2024-09-07**|**Leveraging Moving Sound Source Trajectories for Universal Sound Separation**|Donghang Wu et.al.|[2409.04843](http://arxiv.org/abs/2409.04843)|null|\n", "2408.16448": "|**2024-08-29**|**Enhancing Sound Source Localization via False Negative Elimination**|Zengjie Song et.al.|[2408.16448](http://arxiv.org/abs/2408.16448)|**[link](https://github.com/zjsong/sacl)**|\n", "2408.15771": "|**2024-08-28**|**wav2pos: Sound Source Localization using Masked Autoencoders**|Axel Berg et.al.|[2408.15771](http://arxiv.org/abs/2408.15771)|**[link](https://github.com/axeber01/wav2pos)**|\n", "2408.13904": "|**2024-08-25**|**The effect of self-motion and room familiarity on sound source localization in virtual environments**|Niklas Isserstedt et.al.|[2408.13904](http://arxiv.org/abs/2408.13904)|null|\n", "2409.06709": "|**2024-08-25**|**Unveiling Visual Biases in Audio-Visual Localization Benchmarks**|Liangyu Chen et.al.|[2409.06709](http://arxiv.org/abs/2409.06709)|null|\n", "2408.05364": "|**2024-08-09**|**Spherical World-Locking for Audio-Visual Localization in Egocentric Videos**|Heeseung Yun et.al.|[2408.05364](http://arxiv.org/abs/2408.05364)|null|\n", "2407.21721": "|**2024-07-31**|**Open-Vocabulary Audio-Visual Semantic Segmentation**|Ruohao Guo et.al.|[2407.21721](http://arxiv.org/abs/2407.21721)|null|\n", "2407.13676": "|**2024-07-18**|**Aligning Sight and Sound: Advanced Sound Source Localization Through Audio-Visual Alignment**|Arda Senocak et.al.|[2407.13676](http://arxiv.org/abs/2407.13676)|**[link](https://github.com/kaistmm/SSLalignment)**|\n", "2406.16058": "|**2024-06-23**|**Text-Queried Target Sound Event Localization**|Jinzheng Zhao et.al.|[2406.16058](http://arxiv.org/abs/2406.16058)|null|\n", "2406.07914": "|**2024-06-14**|**Can Large Language Models Understand Spatial Audio?**|Changli Tang et.al.|[2406.07914](http://arxiv.org/abs/2406.07914)|null|\n", "2406.07663": "|**2024-06-11**|**Broadband MEMS Microphone Arrays with Reduced Aperture Through 3D-Printed Waveguides**|Dennis Laurijssen et.al.|[2406.07663](http://arxiv.org/abs/2406.07663)|null|\n", "2405.19813": "|**2024-05-30**|**SLAM-based Joint Calibration of Multiple Asynchronous Microphone Arrays and Sound Source Localization**|Jiang Wang et.al.|[2405.19813](http://arxiv.org/abs/2405.19813)|**[link](https://github.com/aislab-sustech/calibration_of_multi_mic_arrays)**|\n", "2405.07021": "|**2024-05-11**|**IPDnet: A Universal Direct-Path IPD Estimation Network for Sound Source Localization**|Yabo Wang et.al.|[2405.07021](http://arxiv.org/abs/2405.07021)|**[link](https://github.com/audio-westlakeu/fn-ssl)**|\n", "2405.02991": "|**2024-05-09**|**Steered Response Power for Sound Source Localization: A Tutorial Review**|Eric Grinstein et.al.|[2405.02991](http://arxiv.org/abs/2405.02991)|**[link](https://github.com/egrinstein/xsrp)**|\n", "2404.19615": "|**2024-04-30**|**SemiPL: A Semi-supervised Method for Event Sound Source Localization**|Yue Li et.al.|[2404.19615](http://arxiv.org/abs/2404.19615)|**[link](https://github.com/ly245422/sspl)**|\n", "2404.01751": "|**2024-07-07**|**T-VSL: Text-Guided Visual Sound Source Localization in Mixtures**|Tanvir Mahmud et.al.|[2404.01751](http://arxiv.org/abs/2404.01751)|**[link](https://github.com/enyac-group/t-vsl)**|\n", "2404.01611": "|**2024-04-02**|**Audio Simulation for Sound Source Localization in Virtual Evironment**|Yi Di Yuan et.al.|[2404.01611](http://arxiv.org/abs/2404.01611)|null|\n", "2403.20130": "|**2024-03-29**|**Sound event localization and classification using WASN in Outdoor Environment**|Dongzhe Zhang et.al.|[2403.20130](http://arxiv.org/abs/2403.20130)|null|\n", "2403.17514": "|**2024-03-26**|**Speaker Distance Estimation in Enclosures from Single-Channel Audio**|Michael Neri et.al.|[2403.17514](http://arxiv.org/abs/2403.17514)|**[link](https://github.com/michaelneri/audio-distance-estimation)**|\n", "2403.17420": "|**2024-03-26**|**Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge**|Dongjin Kim et.al.|[2403.17420](http://arxiv.org/abs/2403.17420)|**[link](https://github.com/visualaikhu/noprior_multissl)**|\n", "2403.09455": "|**2024-03-14**|**The Neural-SRP method for positional sound source localization**|Eric Grinstein et.al.|[2403.09455](http://arxiv.org/abs/2403.09455)|**[link](https://github.com/egrinstein/gnn_ssl)**|\n", "2402.06586": "|**2024-02-09**|**Analytical model for the relation between signal bandwidth and spatial resolution in Steered-Response Power Phase Transform (SRP-PHAT) maps**|Guillermo Garcia-Barrios et.al.|[2402.06586](http://arxiv.org/abs/2402.06586)|null|\n", "2402.06411": "|**2024-02-09**|**Exploiting spatial diversity for increasing the robustness of sound source localization systems against reverberation**|Guillermo Garcia-Barrios et.al.|[2402.06411](http://arxiv.org/abs/2402.06411)|null|\n", "2402.03867": "|**2024-02-06**|**Binaural sound source localization using a hybrid time and frequency domain model**|Gil Geva et.al.|[2402.03867](http://arxiv.org/abs/2402.03867)|null|\n", "2312.04846": "|**2023-12-08**|**Sound Source Localization for a Source inside a Structure using Ac-CycleGAN**|Shunsuke Kita et.al.|[2312.04846](http://arxiv.org/abs/2312.04846)|null|\n", "2311.12305": "|**2024-01-28**|**Eliminating Quantization Errors in Classification-Based Sound Source Localization**|Linfeng Feng et.al.|[2311.12305](http://arxiv.org/abs/2311.12305)|**[link](https://github.com/linfeng-feng/uld)**|\n", "2311.07175": "|**2023-11-13**|**Research and experimental verification on low-frequency long-range sound propagation characteristics under ice-covered and range-dependent marine environment in the Arctic**|Jinbao Weng et.al.|[2311.07175](http://arxiv.org/abs/2311.07175)|null|\n", "2311.04066": "|**2023-11-07**|**Can CLIP Help Sound Source Localization?**|Sooyoung Park et.al.|[2311.04066](http://arxiv.org/abs/2311.04066)|**[link](https://github.com/swimmiing/ACL-SSL)**|\n", "2311.01052": "|**2023-11-16**|**Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis**|Victor Letzelter et.al.|[2311.01052](http://arxiv.org/abs/2311.01052)|**[link](https://github.com/victorletzelter/code-rmcl)**|\n", "2310.19063": "|**2024-01-27**|**Feature Aggregation in Joint Sound Classification and Localization Neural Networks**|Brendan Healy et.al.|[2310.19063](http://arxiv.org/abs/2310.19063)|null|\n", "2310.18709": "|**2024-11-02**|**Audio-Visual Instance Segmentation**|Ruohao Guo et.al.|[2310.18709](http://arxiv.org/abs/2310.18709)|**[link](https://github.com/ruohaoguo/avis)**|\n", "2310.15130": "|**2024-08-16**|**Novel-View Acoustic Synthesis from 3D Reconstructed Rooms**|Byeongjoo Ahn et.al.|[2310.15130](http://arxiv.org/abs/2310.15130)|**[link](https://github.com/apple/ml-nvas3d)**|\n", "2309.10724": "|**2023-09-19**|**Sound Source Localization is All about Cross-Modal Alignment**|Arda Senocak et.al.|[2309.10724](http://arxiv.org/abs/2309.10724)|null|\n", "2309.07929": "|**2024-02-02**|**Prompting Segmentation with Sound Is Generalizable Audio-Visual Source Localizer**|Yaoting Wang et.al.|[2309.07929](http://arxiv.org/abs/2309.07929)|**[link](https://github.com/GeWu-Lab/Generalizable-Audio-Visual-Segmentation)**|\n", "2308.14611": "|**2023-08-28**|**Data-driven 3D Room Geometry Inference with a Linear Loudspeaker Array and a Single Microphone**|Cagdas Tuna et.al.|[2308.14611](http://arxiv.org/abs/2308.14611)|null|\n", "2308.06087": "|**2023-08-18**|**Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization**|Sung Jin Um et.al.|[2308.06087](http://arxiv.org/abs/2308.06087)|**[link](https://github.com/visualaikhu/sira-ssl)**|\n", "2308.04767": "|**2023-08-09**|**Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization**|Tianyu Liu et.al.|[2308.04767](http://arxiv.org/abs/2308.04767)|**[link](https://github.com/tahy1/avin)**|\n", "2308.04169": "|**2023-08-08**|**Dual input neural networks for positional sound source localization**|Eric Grinstein et.al.|[2308.04169](http://arxiv.org/abs/2308.04169)|**[link](https://github.com/egrinstein/di_nn)**|\n", "2307.12129": "|**2023-07-22**|**Estimating speaker direction on a humanoid robot with binaural acoustic signals**|Pranav Barot et.al.|[2307.12129](http://arxiv.org/abs/2307.12129)|null|\n", "2306.16081": "|**2023-06-28**|**Graph neural networks for sound source localization on distributed microphone networks**|Eric Grinstein et.al.|[2306.16081](http://arxiv.org/abs/2306.16081)|**[link](https://github.com/egrinstein/gnn_ssl)**|\n", "2305.19610": "|**2023-05-31**|**FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization**|Yabo Wang et.al.|[2305.19610](http://arxiv.org/abs/2305.19610)|**[link](https://github.com/audio-westlakeu/fn-ssl)**|\n", "2305.01836": "|**2023-05-03**|**AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation**|Shentong Mo et.al.|[2305.01836](http://arxiv.org/abs/2305.01836)|null|\n", "2304.07512": "|**2023-04-15**|**Soft Label Coding for End-to-end Sound Source Localization With Ad-hoc Microphone Arrays**|Linfeng Feng et.al.|[2304.07512](http://arxiv.org/abs/2304.07512)|null|\n", "2303.17490": "|**2023-03-30**|**Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment**|Kim Sung-Bin et.al.|[2303.17490](http://arxiv.org/abs/2303.17490)|null|\n", "2303.17056": "|**2023-03-29**|**Audio-Visual Grouping Network for Sound Localization from Mixtures**|Shentong Mo et.al.|[2303.17056](http://arxiv.org/abs/2303.17056)|**[link](https://github.com/stonemo/avgn)**|\n", "2301.12808": "|**2023-01-30**|**Real-Time Acoustic Perception for Automotive Applications**|Jun Yin et.al.|[2301.12808](http://arxiv.org/abs/2301.12808)|null|\n", "2212.03470": "|**2022-12-10**|**Improving trajectory localization accuracy via direction-of-arrival derivative estimation**|Ruchi Pandey et.al.|[2212.03470](http://arxiv.org/abs/2212.03470)|null|\n", "2211.11381": "|**2022-11-21**|**LISA: Localized Image Stylization with Audio via Implicit Neural Representation**|Seung Hyun Lee et.al.|[2211.11381](http://arxiv.org/abs/2211.11381)|null|\n", "2211.08367": "|**2023-04-14**|**FlowGrad: Using Motion for Visual Sound Source Localization**|Rajsuryan Singh et.al.|[2211.08367](http://arxiv.org/abs/2211.08367)|**[link](https://github.com/rrrajjjj/flowgrad)**|\n", "2211.03019": "|**2022-11-06**|**Hear The Flow: Optical Flow-Based Self-Supervised Visual Sound Source Localization**|Dennis Fedorishin et.al.|[2211.03019](http://arxiv.org/abs/2211.03019)|**[link](https://github.com/denfed/heartheflow)**|\n", "2211.01966": "|**2022-11-03**|**MarginNCE: Robust Sound Localization with a Negative Margin**|Sooyoung Park et.al.|[2211.01966](http://arxiv.org/abs/2211.01966)|null|\n", "2210.14581": "|**2022-10-26**|**Deep Learning Based Audio-Visual Multi-Speaker DOA Estimation Using Permutation-Free Loss Function**|Qing Wang et.al.|[2210.14581](http://arxiv.org/abs/2210.14581)|null|\n", "2210.14536": "|**2023-06-05**|**Position tracking of a varying number of sound sources with sliding permutation invariant training**|David Diaz-Guerra et.al.|[2210.14536](http://arxiv.org/abs/2210.14536)|null|\n", "2210.08484": "|**2022-10-16**|**End-to-end Two-dimensional Sound Source Localization With Ad-hoc Microphone Arrays**|Yijun Gong et.al.|[2210.08484](http://arxiv.org/abs/2210.08484)|null|\n", "2210.05600": "|**2022-10-11**|**Observability Analysis of Graph SLAM-Based Joint Calibration of Multiple Microphone Arrays and Sound Source Localization**|Yuanzheng He et.al.|[2210.05600](http://arxiv.org/abs/2210.05600)|null|\n", "2209.09634": "|**2022-08-30**|**A Closer Look at Weakly-Supervised Audio-Visual Source Localization**|Shentong Mo et.al.|[2209.09634](http://arxiv.org/abs/2209.09634)|**[link](https://github.com/stonemo/slavc)**|\n", "2206.12273": "|**2022-06-24**|**Iterative Sound Source Localization for Unknown Number of Sources**|Yanjie Fu et.al.|[2206.12273](http://arxiv.org/abs/2206.12273)|**[link](https://github.com/fyjneverfollows/issl)**|\n", "2203.13412": "|**2022-03-25**|**Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes**|Zengjie Song et.al.|[2203.13412](http://arxiv.org/abs/2203.13412)|**[link](https://github.com/zjsong/sspl)**|\n", "2203.03166": "|**2022-04-06**|**HRTF measurement for accurate sound localization cues**|Gyeong-Tae Lee et.al.|[2203.03166](http://arxiv.org/abs/2203.03166)|**[link](https://github.com/han-saram/hrtf-hats-kaist)**|\n", "2202.13974": "|**2022-02-28**|**SmartBelt: A Wearable Microphone Array for Sound Source Localization with Haptic Feedback**|Simon Michaud et.al.|[2202.13974](http://arxiv.org/abs/2202.13974)|null|\n", "2202.07859": "|**2022-02-16**|**SRP-DNN: Learning Direct-Path Phase Difference for Multiple Moving Sound Source Localization**|Bing Yang et.al.|[2202.07859](http://arxiv.org/abs/2202.07859)|null|\n", "2202.07841": "|**2022-02-16**|**Learning Deep Direct-Path Relative Transfer Function for Binaural Sound Source Localization**|Bing Yang et.al.|[2202.07841](http://arxiv.org/abs/2202.07841)|null|\n", "2202.06406": "|**2022-02-13**|**Visual Sound Localization in the Wild by Cross-Modal Interference Erasing**|Xian Liu et.al.|[2202.06406](http://arxiv.org/abs/2202.06406)|**[link](https://github.com/alvinliu0/visual-sound-localization-in-the-wild)**|\n", "2111.08567": "|**2021-11-05**|**Joint Learning of Visual-Audio Saliency Prediction and Sound Source Localization on Multi-face Videos**|Minglang Qiao et.al.|[2111.08567](http://arxiv.org/abs/2111.08567)|**[link](https://github.com/MinglangQiao/MVVA-Database)**|\n", "2111.00030": "|**2021-10-29**|**Differentiable Tracking-Based Training of Deep Learning Sound Source Localizers**|Sharath Adavanne et.al.|[2111.00030](http://arxiv.org/abs/2111.00030)|**[link](https://github.com/sharathadavanne/hungarian-net)**|\n", "2109.14797": "|**2021-10-02**|**Emergency Vehicles Audio Detection and Localization in Autonomous Driving**|Hongyi Sun et.al.|[2109.14797](http://arxiv.org/abs/2109.14797)|null|\n", "2109.03465": "|**2022-06-17**|**A Survey of Sound Source Localization with Deep Learning Methods**|Pierre-Amaury Grumiaux et.al.|[2109.03465](http://arxiv.org/abs/2109.03465)|null|\n", "2108.01246": "|**2021-08-03**|**AcousticFusion: Fusing Sound Source Localization to Visual SLAM in Dynamic Environments**|Tianwei Zhang et.al.|[2108.01246](http://arxiv.org/abs/2108.01246)|null|\n", "2106.00180": "|**2021-06-01**|**Dual Normalization Multitasking for Audio-Visual Sounding Object Localization**|Tokuhiro Nishikawa et.al.|[2106.00180](http://arxiv.org/abs/2106.00180)|null|\n", "2105.01897": "|**2021-05-05**|**Improved feature extraction for CRNN-based multiple sound source localization**|Pierre-Amaury Grumiaux et.al.|[2105.01897](http://arxiv.org/abs/2105.01897)|null|\n", "2104.13347": "|**2021-04-27**|**BeamLearning: an end-to-end Deep Learning approach for the angular localization of sound sources using raw multichannel acoustic pressure data**|Hadrien Pujol et.al.|[2104.13347](http://arxiv.org/abs/2104.13347)|null|\n", "2104.06401": "|**2022-07-09**|**Self-supervised object detection from audio-visual correspondence**|Triantafyllos Afouras et.al.|[2104.06401](http://arxiv.org/abs/2104.06401)|null|\n", "2103.06049": "|**2021-03-10**|**Search Disaster Victims using Sound Source Localization**|Abhish Khanal et.al.|[2103.06049](http://arxiv.org/abs/2103.06049)|**[link](https://github.com/subash-timilsina/Sound-Source-Localization)**|\n", "2103.03954": "|**2022-05-11**|**ODAS: Open embeddeD Audition System**|Fran\u00e7ois Grondin et.al.|[2103.03954](http://arxiv.org/abs/2103.03954)|null|\n", "2012.05533": "|**2021-03-17**|**Data-Efficient Framework for Real-world Multiple Sound Source 2D Localization**|Guillaume Le Moing et.al.|[2012.05533](http://arxiv.org/abs/2012.05533)|null|\n", "2012.05908": "|**2021-03-16**|**Ensemble of Discriminators for Domain Adaptation in Multiple Sound Source 2D Localization**|Guillaume Le Moing et.al.|[2012.05908](http://arxiv.org/abs/2012.05908)|null|\n", "2012.05515": "|**2020-12-10**|**Learning Multiple Sound Source 2D Localization**|Guillaume Le Moing et.al.|[2012.05515](http://arxiv.org/abs/2012.05515)|null|\n", "2012.03574": "|**2020-12-07**|**Reverberant Sound Localization with a Robot Head Based on Direct-Path Relative Transfer Function**|Xiaofei Li et.al.|[2012.03574](http://arxiv.org/abs/2012.03574)|null|\n", "2010.16140": "|**2020-10-30**|**Beamforming for measurements under disturbed propagation conditions using numerically calculated Green's functions**|Marius Lehmann et.al.|[2010.16140](http://arxiv.org/abs/2010.16140)|null|\n", "2010.14420": "|**2021-02-15**|**SSLIDE: Sound Source Localization for Indoors based on Deep Learning**|Yifan Wu et.al.|[2010.14420](http://arxiv.org/abs/2010.14420)|null|\n", "2008.05789": "|**2020-08-13**|**Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning**|Ying Cheng et.al.|[2008.05789](http://arxiv.org/abs/2008.05789)|null|\n", "2007.13976": "|**2020-07-28**|**Self-supervised Neural Audio-Visual Sound Source Localization via Probabilistic Spatial Modeling**|Yoshiki Masuyama et.al.|[2007.13976](http://arxiv.org/abs/2007.13976)|null|\n", "2007.06355": "|**2020-07-14**|**Multiple Sound Sources Localization from Coarse to Fine**|Rui Qian et.al.|[2007.06355](http://arxiv.org/abs/2007.06355)|**[link](https://github.com/shvdiwnkozbw/Multi-Source-Sound-Localization)**|\n", "2007.05722": "|**2020-07-11**|**Do We Need Sound for Sound Source Localization?**|Takashi Oya et.al.|[2007.05722](http://arxiv.org/abs/2007.05722)|null|\n", "2007.03274": "|**2020-07-07**|**Multi-Tones' Phase Coding (MTPC) of Interaural Time Difference by Spiking Neural Network**|Zihan Pan et.al.|[2007.03274](http://arxiv.org/abs/2007.03274)|null|\n", "2006.15647": "|**2020-06-28**|**I can attend a meeting too! Towards a human-like telepresence avatar robot to attend meeting on your behalf**|Hrishav Bakul Barua et.al.|[2006.15647](http://arxiv.org/abs/2006.15647)|null|\n", "2002.01440": "|**2020-02-12**|**Audio-Visual Calibration with Polynomial Regression for 2-D Projection Using SVD-PHAT**|Francois Grondin et.al.|[2002.01440](http://arxiv.org/abs/2002.01440)|null|\n", "1912.04979": "|**2019-12-10**|**Advances in Online Audio-Visual Meeting Transcription**|Takuya Yoshioka et.al.|[1912.04979](http://arxiv.org/abs/1912.04979)|null|\n", "1911.12616": "|**2019-11-28**|**Performance Comparison of UCA and UCCA based Real-time Sound Source Localization Systems using Circular Harmonics SRP Method**|Zhe Zhang et.al.|[1911.12616](http://arxiv.org/abs/1911.12616)|null|\n", "1911.09649": "|**2019-11-20**|**Learning to Localize Sound Sources in Visual Scenes: Analysis and Applications**|Arda Senocak et.al.|[1911.09649](http://arxiv.org/abs/1911.09649)|null|\n", "1909.09944": "|**2019-10-25**|**Watch, Listen and Tell: Multi-modal Weakly Supervised Dense Event Captioning**|Tanzila Rahman et.al.|[1909.09944](http://arxiv.org/abs/1909.09944)|null|\n", "1909.06998": "|**2019-09-16**|**Real-time 3-D Mapping with Estimating Acoustic Materials**|Taeyoung Kim et.al.|[1909.06998](http://arxiv.org/abs/1909.06998)|null|\n", "1909.01008": "|**2020-10-21**|**The LOCATA Challenge: Acoustic Source Localization and Tracking**|Christine Evers et.al.|[1909.01008](http://arxiv.org/abs/1909.01008)|**[link](https://github.com/cevers/sap_locata_eval)**|\n", "1907.12621": "|**2019-07-29**|**Fast and Robust 3-D Sound Source Localization with DSVD-PHAT**|Francois Grondin et.al.|[1907.12621](http://arxiv.org/abs/1907.12621)|null|\n", "1907.04655": "|**2019-07-03**|**Audio-Based Search and Rescue with a Drone: Highlights from the IEEE Signal Processing Cup 2019 Student Competition**|Antoine Deleforge et.al.|[1907.04655](http://arxiv.org/abs/1907.04655)|null|\n", "1907.01169": "|**2019-07-02**|**Can a Robot Hear the Shape and Dimensions of a Room?**|Linh Nguyen et.al.|[1907.01169](http://arxiv.org/abs/1907.01169)|null|\n", "1906.11913": "|**2019-06-27**|**Multiple Sound Source Localization with SVD-PHAT**|Francois Grondin et.al.|[1906.11913](http://arxiv.org/abs/1906.11913)|null|\n", "1906.08968": "|**2019-06-21**|**Mirage: 2D Source Localization Using Microphone Pair Augmentation with Echoes**|Diego Di Carlo et.al.|[1906.08968](http://arxiv.org/abs/1906.08968)|null|\n", "1906.08847": "|**2019-06-20**|**A Signal Subspace Rotation Method for Localization of Multiple Wideband Sound Sources**|Kainan Chen et.al.|[1906.08847](http://arxiv.org/abs/1906.08847)|null|\n", "1902.09179": "|**2019-02-25**|**Robust Sound Source Localization considering Similarity of Back-Propagation Signals**|Inkyu An et.al.|[1902.09179](http://arxiv.org/abs/1902.09179)|null|\n", "1902.05446": "|**2019-02-13**|**Enhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source Localization**|Jorge et.al.|[1902.05446](http://arxiv.org/abs/1902.05446)|null|\n", "1812.05901": "|**2018-12-14**|**Evaluation of an open-source implementation of the SRP-PHAT algorithm within the 2018 LOCATA challenge**|Romain Lebarbenchon et.al.|[1812.05901](http://arxiv.org/abs/1812.05901)|null|\n", "1812.03914": "|**2018-12-10**|**A Computationally Efficient and Practically Feasible Two Microphones Blind Speech Separation Method**|Chandan K A Reddy et.al.|[1812.03914](http://arxiv.org/abs/1812.03914)|null|\n", "1812.02399": "|**2018-12-06**|**Binaural Source Localization based on Modulation-Domain Features and Decision Pooling**|Semih A\u011fcaer et.al.|[1812.02399](http://arxiv.org/abs/1812.02399)|null|\n", "1812.00115": "|**2018-12-01**|**Lightweight and Optimized Sound Source Localization and Tracking Methods for Open and Closed Microphone Array Configurations**|Francois Grondin et.al.|[1812.00115](http://arxiv.org/abs/1812.00115)|null|\n", "1811.11785": "|**2019-02-11**|**SVD-PHAT: A Fast Sound Source Localization Method**|Francois Grondin et.al.|[1811.11785](http://arxiv.org/abs/1811.11785)|null|\n", "1811.08482": "|**2019-08-20**|**Proceedings of the LOCATA Challenge Workshop -- a satellite event of IWAENC 2018**|Heinrich W. Loellmann et.al.|[1811.08482](http://arxiv.org/abs/1811.08482)|null|\n", "1810.04080": "|**2018-12-04**|**TRAMP: Tracking by a Real-time AMbisonic-based Particle filter**|Sr\u0111an Kiti\u0107 et.al.|[1810.04080](http://arxiv.org/abs/1810.04080)|null|\n", "1809.07549": "|**2018-09-20**|**Evaluating MCC-PHAT for the LOCATA Challenge - Task 1 and Task 3**|Shoufeng Lin et.al.|[1809.07549](http://arxiv.org/abs/1809.07549)|null|\n", "1808.06429": "|**2018-08-20**|**Deep Residual Network for Sound Source Localization in the Time Domain**|Dmitry Suvorov et.al.|[1808.06429](http://arxiv.org/abs/1808.06429)|null|\n", "1804.05111": "|**2020-06-28**|**Multi-Sound-Source Localization Using Machine Learning for Small Autonomous Unmanned Vehicles with a Self-Rotating Bi-Microphone Array**|Deepak Gala et.al.|[1804.05111](http://arxiv.org/abs/1804.05111)|null|\n", "1804.03641": "|**2018-10-09**|**Audio-Visual Scene Analysis with Self-Supervised Multisensory Features**|Andrew Owens et.al.|[1804.03641](http://arxiv.org/abs/1804.03641)|null|\n", "1804.03372": "|**2018-04-10**|**Realtime Active Sound Source Localization for Unmanned Ground Robots Using a Self-Rotational Bi-Microphone Array**|Deepak Gala et.al.|[1804.03372](http://arxiv.org/abs/1804.03372)|null|\n", "1803.03849": "|**2018-03-10**|**Learning to Localize Sound Source in Visual Scenes**|Arda Senocak et.al.|[1803.03849](http://arxiv.org/abs/1803.03849)|null|\n", "1802.04479": "|**2018-02-13**|**Phased Microphone Array for Sound Source Localization with Deep Learning**|Wei Ma et.al.|[1802.04479](http://arxiv.org/abs/1802.04479)|null|\n", "1801.03740": "|**2018-08-28**|**Direction of Arrival with One Microphone, a few LEGOs, and Non-Negative Matrix Factorization**|Dalia El Badawy et.al.|[1801.03740](http://arxiv.org/abs/1801.03740)|**[link](https://github.com/swing-research/scatsense)**|\n", "1712.07814": "|**2017-12-21**|**Indoor Sound Source Localization with Probabilistic Neural Network**|Yingxiang Sun et.al.|[1712.07814](http://arxiv.org/abs/1712.07814)|null|\n", "1711.11565": "|**2018-02-26**|**Deep Neural Networks for Multiple Speaker Detection and Localization**|Weipeng He et.al.|[1711.11565](http://arxiv.org/abs/1711.11565)|null|\n", "1711.07791": "|**2017-11-21**|**Reflection-Aware Sound Source Localization**|Inkyu An et.al.|[1711.07791](http://arxiv.org/abs/1711.07791)|null|\n", "1710.10948": "|**2017-10-27**|**Sound Source Localization in a Multipath Environment Using Convolutional Neural Networks**|Eric L. Ferguson et.al.|[1710.10948](http://arxiv.org/abs/1710.10948)|null|\n", "1612.06287": "|**2016-12-14**|**VAST : The Virtual Acoustic Space Traveler Dataset**|Cl\u00e9ment Gaultier et.al.|[1612.06287](http://arxiv.org/abs/1612.06287)|null|\n", "1609.09747": "|**2017-03-20**|**Hearing in a shoe-box : binaural source position and wall absorption estimation using virtually supervised learning**|Saurabh Kataria et.al.|[1609.09747](http://arxiv.org/abs/1609.09747)|null|\n", "1609.09743": "|**2016-09-30**|**Rectified binaural ratio: A complex T-distributed feature for robust sound localization**|Antoine Deleforge et.al.|[1609.09743](http://arxiv.org/abs/1609.09743)|null|\n", "1608.05179": "|**2016-08-29**|**Improving the Efficiency of DAMAS for Sound Source Localization via Wavelet Compression Computational Grid**|Wei Ma et.al.|[1608.05179](http://arxiv.org/abs/1608.05179)|null|\n", "1602.08629": "|**2016-02-27**|**Localization of Simultaneous Moving Sound Sources for Mobile Robot Using a Frequency-Domain Steered Beamformer Approach**|Jean-Marc Valin et.al.|[1602.08629](http://arxiv.org/abs/1602.08629)|null|\n", "1604.01642": "|**2016-02-27**|**Robust 3D Localization and Tracking of Sound Sources Using Beamforming and Particle Filtering**|Jean-Marc Valin et.al.|[1604.01642](http://arxiv.org/abs/1604.01642)|null|\n", "1602.08213": "|**2016-02-26**|**Robust Sound Source Localization Using a Microphone Array on a Mobile Robot**|Jean-Marc Valin et.al.|[1602.08213](http://arxiv.org/abs/1602.08213)|null|\n", "1602.08139": "|**2016-02-25**|**Robust Localization and Tracking of Simultaneous Moving Sound Sources Using Beamforming and Particle Filtering**|Jean-Marc Valin et.al.|[1602.08139](http://arxiv.org/abs/1602.08139)|null|\n", "1509.03205": "|**2016-06-27**|**Estimation of the Direct-Path Relative Transfer Function for Supervised Sound-Source Localization**|Xiaofei Li et.al.|[1509.03205](http://arxiv.org/abs/1509.03205)|null|\n", "1508.03148": "|**2015-08-13**|**Semi-Supervised Sound Source Localization Based on Manifold Regularization**|Bracha Laufer-Goldshtein et.al.|[1508.03148](http://arxiv.org/abs/1508.03148)|null|\n", "1502.07577": "|**2015-03-03**|**Sampling Sparse Signals on the Sphere: Algorithms and Applications**|Ivan Dokmanic et.al.|[1502.07577](http://arxiv.org/abs/1502.07577)|null|\n", "1502.03163": "|**2015-02-11**|**Gaussian Process Models for HRTF based Sound-Source Localization and Active-Learning**|Yuancheng Luo et.al.|[1502.03163](http://arxiv.org/abs/1502.03163)|null|\n", "1407.2351": "|**2015-02-16**|**Efficient Steered-Response Power Methods for Sound Source Localization Using Microphone Arrays**|Markus V. S. Lima et.al.|[1407.2351](http://arxiv.org/abs/1407.2351)|null|\n", "1311.1047": "|**2014-02-12**|**A Geometric Approach to Sound Source Localization from Time-Delay Estimates**|Xavier Alameda-Pineda et.al.|[1311.1047](http://arxiv.org/abs/1311.1047)|null|\n"}, "AVS": {"2412.11248": "|**2024-12-17**|**Multimodal Class-aware Semantic Enhancement Network for Audio-Visual Video Parsing**|Pengcheng Zhao et.al.|[2412.11248](http://arxiv.org/abs/2412.11248)|null|\n", "2412.08161": "|**2024-12-11**|**Collaborative Hybrid Propagator for Temporal Misalignment in Audio-Visual Segmentation**|Kexin Li et.al.|[2412.08161](http://arxiv.org/abs/2412.08161)|null|\n", "2411.02236": "|**2024-11-04**|**3D Audio-Visual Segmentation**|Artem Sokolov et.al.|[2411.02236](http://arxiv.org/abs/2411.02236)|null|\n", "2408.01708": "|**2024-08-03**|**AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation**|Zili Wang et.al.|[2408.01708](http://arxiv.org/abs/2408.01708)|**[link](https://github.com/markxcloud/avesformer)**|\n", "2407.16638": "|**2024-07-23**|**Unveiling and Mitigating Bias in Audio Visual Segmentation**|Peiwen Sun et.al.|[2407.16638](http://arxiv.org/abs/2407.16638)|null|\n", "2407.11820": "|**2024-09-12**|**Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation**|Juncheng Ma et.al.|[2407.11820](http://arxiv.org/abs/2407.11820)|**[link](https://github.com/GeWu-Lab/Stepping-Stones)**|\n", "2407.10957": "|**2024-07-15**|**Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes**|Yaoting Wang et.al.|[2407.10957](http://arxiv.org/abs/2407.10957)|null|\n", "2407.10947": "|**2024-07-15**|**Can Textual Semantics Mitigate Sounding Object Segmentation Preference?**|Yaoting Wang et.al.|[2407.10947](http://arxiv.org/abs/2407.10947)|**[link](https://github.com/gewu-lab/sounding-object-segmentation-preference)**|\n", "2407.08126": "|**2024-07-11**|**Label-anticipated Event Disentanglement for Audio-Visual Video Parsing**|Jinxing Zhou et.al.|[2407.08126](http://arxiv.org/abs/2407.08126)|null|\n", "2407.05358": "|**2024-09-29**|**CPM: Class-conditional Prompting Machine for Audio-visual Segmentation**|Yuanhong Chen et.al.|[2407.05358](http://arxiv.org/abs/2407.05358)|null|\n", "2407.02004": "|**2024-07-03**|**SAVE: Segment Audio-Visual Easy way using Segment Anything Model**|Khanh-Binh Nguyen et.al.|[2407.02004](http://arxiv.org/abs/2407.02004)|null|\n", "2406.06163": "|**2024-06-10**|**Extending Segment Anything Model into Auditory and Temporal Dimensions for Audio-Visual Segmentation**|Juhyeong Seon et.al.|[2406.06163](http://arxiv.org/abs/2406.06163)|**[link](https://github.com/Sunjuhyeong/SAM_STBAVA)**|\n", "2406.02345": "|**2024-06-04**|**Progressive Confident Masking Attention Network for Audio-Visual Segmentation**|Yuxuan Wang et.al.|[2406.02345](http://arxiv.org/abs/2406.02345)|null|\n", "2403.14203": "|**2024-03-21**|**Unsupervised Audio-Visual Segmentation with Modality Alignment**|Swapnil Bhosale et.al.|[2403.14203](http://arxiv.org/abs/2403.14203)|null|\n", "2403.11074": "|**2024-03-17**|**Audio-Visual Segmentation via Unlabeled Frame Exploitation**|Jinxiang Liu et.al.|[2403.11074](http://arxiv.org/abs/2403.11074)|null|\n", "2402.02327": "|**2024-02-06**|**Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues**|Tianxiang Chen et.al.|[2402.02327](http://arxiv.org/abs/2402.02327)|null|\n", "2312.06462": "|**2024-04-07**|**Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation**|Qi Yang et.al.|[2312.06462](http://arxiv.org/abs/2312.06462)|**[link](https://github.com/yannqi/COMBO-AVS)**|\n", "2312.01017": "|**2023-12-02**|**Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling**|Shentong Mo et.al.|[2312.01017](http://arxiv.org/abs/2312.01017)|**[link](https://github.com/stonemo/deepavfusion)**|\n", "2311.15080": "|**2023-11-25**|**Weakly-Supervised Audio-Visual Segmentation**|Shentong Mo et.al.|[2311.15080](http://arxiv.org/abs/2311.15080)|null|\n", "2310.08303": "|**2023-10-12**|**Multimodal Variational Auto-encoder based Audio-Visual Segmentation**|Yuxin Mao et.al.|[2310.08303](http://arxiv.org/abs/2310.08303)|**[link](https://github.com/opennlplab/mmvae-avs)**|\n", "2310.06259": "|**2024-07-17**|**Cross-modal Cognitive Consensus guided Audio-Visual Segmentation**|Zhaofeng Shi et.al.|[2310.06259](http://arxiv.org/abs/2310.06259)|**[link](https://github.com/zhaofengshi/avs-c3n)**|\n", "2309.09501": "|**2023-09-18**|**Discovering Sounding Objects by Audio Queries for Audio Visual Segmentation**|Shaofei Huang et.al.|[2309.09501](http://arxiv.org/abs/2309.09501)|null|\n", "2309.06728": "|**2023-09-13**|**Leveraging Foundation models for Unsupervised Audio-Visual Segmentation**|Swapnil Bhosale et.al.|[2309.06728](http://arxiv.org/abs/2309.06728)|null|\n", "2308.10175": "|**2023-08-20**|**BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge**|Chen Liu et.al.|[2308.10175](http://arxiv.org/abs/2308.10175)|null|\n", "2308.08288": "|**2023-12-19**|**Improving Audio-Visual Segmentation with Bidirectional Generation**|Dawei Hao et.al.|[2308.08288](http://arxiv.org/abs/2308.08288)|**[link](https://github.com/opennlplab/avs-bidirectional)**|\n", "2308.05421": "|**2023-08-10**|**Progressive Spatio-temporal Perception for Audio-Visual Question Answering**|Guangyao Li et.al.|[2308.05421](http://arxiv.org/abs/2308.05421)|**[link](https://github.com/gewu-lab/pstp-net)**|\n", "2307.16620": "|**2023-08-01**|**Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics**|Chen Liu et.al.|[2307.16620](http://arxiv.org/abs/2307.16620)|null|\n", "2307.16579": "|**2023-07-31**|**Contrastive Conditional Latent Diffusion for Audio-visual Segmentation**|Yuxin Mao et.al.|[2307.16579](http://arxiv.org/abs/2307.16579)|null|\n", "2307.13236": "|**2023-07-25**|**Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation**|Jinxiang Liu et.al.|[2307.13236](http://arxiv.org/abs/2307.13236)|null|\n", "2307.01146": "|**2023-12-18**|**AVSegFormer: Audio-Visual Segmentation with Transformer**|Shengyi Gao et.al.|[2307.01146](http://arxiv.org/abs/2307.01146)|**[link](https://github.com/vvvb-github/avsegformer)**|\n", "2305.11019": "|**2023-10-07**|**Annotation-free Audio-Visual Segmentation**|Jinxiang Liu et.al.|[2305.11019](http://arxiv.org/abs/2305.11019)|null|\n", "2305.07223": "|**2023-12-26**|**Transavs: End-To-End Audio-Visual Segmentation With Transformer**|Yuhang Ling et.al.|[2305.07223](http://arxiv.org/abs/2305.07223)|null|\n", "2305.01836": "|**2023-05-03**|**AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation**|Shentong Mo et.al.|[2305.01836](http://arxiv.org/abs/2305.01836)|null|\n", "2304.05930": "|**2024-09-17**|**MED-VT++: Unifying Multimodal Learning with a Multiscale Encoder-Decoder Video Transformer**|Rezaul Karim et.al.|[2304.05930](http://arxiv.org/abs/2304.05930)|null|\n", "2304.02970": "|**2024-08-14**|**Unraveling Instance Associations: A Closer Look for Audio-Visual Segmentation**|Yuanhong Chen et.al.|[2304.02970](http://arxiv.org/abs/2304.02970)|**[link](https://github.com/cyh-0/CAVP)**|\n", "2301.13190": "|**2023-01-30**|**Audio-Visual Segmentation with Semantics**|Jinxing Zhou et.al.|[2301.13190](http://arxiv.org/abs/2301.13190)|**[link](https://github.com/opennlplab/avsbench)**|\n", "2207.05042": "|**2023-02-17**|**Audio-Visual Segmentation**|Jinxing Zhou et.al.|[2207.05042](http://arxiv.org/abs/2207.05042)|**[link](https://github.com/opennlplab/avsbench)**|\n", "2104.00239": "|**2021-04-05**|**Positive Sample Propagation along the Audio-Visual Event Line**|Jinxing Zhou et.al.|[2104.00239](http://arxiv.org/abs/2104.00239)|**[link](https://github.com/jasongief/PSP_CVPR_2021)**|\n"}, "3D-RES": {"2412.02402": "|**2024-12-03**|**RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2412.02402](http://arxiv.org/abs/2412.02402)|**[link](https://github.com/sosppxo/rg-san)**|\n", "2407.20664": "|**2024-07-31**|**3D-GRES: Generalized 3D Referring Expression Segmentation**|Changli Wu et.al.|[2407.20664](http://arxiv.org/abs/2407.20664)|**[link](https://github.com/sosppxo/MDIN)**|\n", "2308.16632": "|**2023-08-31**|**3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2308.16632](http://arxiv.org/abs/2308.16632)|**[link](https://github.com/sosppxo/3d-stmn)**|\n"}, "3D-REC": {"2412.06613": "|**2024-12-09**|**3D Spatial Understanding in MLLMs: Disambiguation and Evaluation**|Chun-Peng Chang et.al.|[2412.06613](http://arxiv.org/abs/2412.06613)|null|\n", "2412.04383": "|**2024-12-05**|**SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding**|Rong Li et.al.|[2412.04383](http://arxiv.org/abs/2412.04383)|null|\n", "2411.18666": "|**2024-11-27**|**3D Scene Graph Guided Vision-Language Pre-training**|Hao Liu et.al.|[2411.18666](http://arxiv.org/abs/2411.18666)|null|\n", "2411.14869": "|**2024-11-27**|**BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence**|Xuewu Lin et.al.|[2411.14869](http://arxiv.org/abs/2411.14869)|**[link](https://github.com/HorizonRobotics/BIP3D)**|\n", "2411.14594": "|**2024-11-21**|**Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction Problems**|Qihao Yuan et.al.|[2411.14594](http://arxiv.org/abs/2411.14594)|**[link](https://github.com/sunsleaf/csvg)**|\n", "2411.04351": "|**2024-11-07**|**LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with Transformers**|Yeong-Seung Baek et.al.|[2411.04351](http://arxiv.org/abs/2411.04351)|null|\n", "2411.03405": "|**2024-11-05**|**Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding**|Sombit Dey et.al.|[2411.03405](http://arxiv.org/abs/2411.03405)|null|\n", "2410.15615": "|**2024-10-21**|**Joint Top-Down and Bottom-Up Frameworks for 3D Visual Grounding**|Yang Liu et.al.|[2410.15615](http://arxiv.org/abs/2410.15615)|null|\n", "2410.13860": "|**2024-10-17**|**VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding**|Runsen Xu et.al.|[2410.13860](http://arxiv.org/abs/2410.13860)|**[link](https://github.com/openrobotlab/vlm-grounder)**|\n", "2409.08102": "|**2024-09-12**|**Bayesian Self-Training for Semi-Supervised 3D Segmentation**|Ozan Unal et.al.|[2409.08102](http://arxiv.org/abs/2409.08102)|null|\n", "2408.04034": "|**2024-08-07**|**Task-oriented Sequential Grounding in 3D Scenes**|Zhuofan Zhang et.al.|[2408.04034](http://arxiv.org/abs/2408.04034)|null|\n", "2407.18244": "|**2024-07-25**|**RefMask3D: Language-Guided Transformer for 3D Referring Segmentation**|Shuting He et.al.|[2407.18244](http://arxiv.org/abs/2407.18244)|**[link](https://github.com/heshuting555/refmask3d)**|\n", "2407.14491": "|**2024-09-02**|**PD-APE: A Parallel Decoding Framework with Adaptive Position Encoding for 3D Visual Grounding**|Chenshu Hou et.al.|[2407.14491](http://arxiv.org/abs/2407.14491)|null|\n", "2407.05363": "|**2024-07-10**|**Multi-branch Collaborative Learning Network for 3D Visual Grounding**|Zhipeng Qian et.al.|[2407.05363](http://arxiv.org/abs/2407.05363)|**[link](https://github.com/qzp2018/MCLN)**|\n", "2407.01525": "|**2024-07-17**|**ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities**|Chenming Zhu et.al.|[2407.01525](http://arxiv.org/abs/2407.01525)|null|\n", "2406.09401": "|**2024-06-13**|**MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations**|Ruiyuan Lyu et.al.|[2406.09401](http://arxiv.org/abs/2406.09401)|**[link](https://github.com/openrobotlab/embodiedscan)**|\n", "2406.08907": "|**2024-06-13**|**Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding**|Yue Xu et.al.|[2406.08907](http://arxiv.org/abs/2406.08907)|null|\n", "2406.05785": "|**2024-07-22**|**A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances, and Future Directions**|Daizong Liu et.al.|[2406.05785](http://arxiv.org/abs/2406.05785)|**[link](https://github.com/liudaizong/awesome-3d-visual-grounding)**|\n", "2405.18295": "|**2024-07-06**|**Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention**|Weitai Kang et.al.|[2405.18295](http://arxiv.org/abs/2405.18295)|null|\n", "2405.15274": "|**2024-05-24**|**Talk to Parallel LiDARs: A Human-LiDAR Interaction Method Based on 3D Visual Grounding**|Yuhang Liu et.al.|[2405.15274](http://arxiv.org/abs/2405.15274)|null|\n", "2405.12821": "|**2024-07-19**|**Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension**|Runwei Guan et.al.|[2405.12821](http://arxiv.org/abs/2405.12821)|**[link](https://github.com/guanrunwei/talk2radar)**|\n", "2404.19696": "|**2024-04-30**|**Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners**|Chun Feng et.al.|[2404.19696](http://arxiv.org/abs/2404.19696)|null|\n", "2404.11064": "|**2024-12-18**|**Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework through Prompt-based Localization**|Yongdong Luo et.al.|[2404.11064](http://arxiv.org/abs/2404.11064)|**[link](https://github.com/leon1207/3dgctr)**|\n", "2403.16539": "|**2024-12-04**|**Data-Efficient 3D Visual Grounding via Order-Aware Referring**|Tung-Yu Wu et.al.|[2403.16539](http://arxiv.org/abs/2403.16539)|null|\n", "2403.08182": "|**2024-03-13**|**SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention**|Feng Xiao et.al.|[2403.08182](http://arxiv.org/abs/2403.08182)|null|\n", "2403.03077": "|**2024-12-01**|**MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding**|Chun-Peng Chang et.al.|[2403.03077](http://arxiv.org/abs/2403.03077)|**[link](https://github.com/dfki-av/mikasa-3dvg)**|\n", "2401.09340": "|**2024-09-24**|**SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding**|Baoxiong Jia et.al.|[2401.09340](http://arxiv.org/abs/2401.09340)|null|\n", "2312.09625": "|**2024-08-30**|**Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment**|Xiaoxu Xu et.al.|[2312.09625](http://arxiv.org/abs/2312.09625)|null|\n", "2312.08022": "|**2023-12-13**|**Mono3DVG: 3D Visual Grounding in Monocular Images**|Yang Zhan et.al.|[2312.08022](http://arxiv.org/abs/2312.08022)|**[link](https://github.com/zhanyang-nwpu/mono3dvg)**|\n", "2311.15383": "|**2024-03-23**|**Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding**|Zhihao Yuan et.al.|[2311.15383](http://arxiv.org/abs/2311.15383)|**[link](https://github.com/CurryYuan/ZSVG3D)**|\n", "2310.18773": "|**2023-10-28**|**CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data**|Taiki Miyanishi et.al.|[2310.18773](http://arxiv.org/abs/2310.18773)|**[link](https://github.com/atr-dbi/cityrefer)**|\n", "2310.06214": "|**2024-10-05**|**CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding**|Eslam Abdelrahman et.al.|[2310.06214](http://arxiv.org/abs/2310.06214)|**[link](https://github.com/eslambakr/CoT3D_VG)**|\n", "2309.12311": "|**2023-09-21**|**LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent**|Jianing Yang et.al.|[2309.12311](http://arxiv.org/abs/2309.12311)|**[link](https://github.com/sled-group/chat-with-nerf)**|\n", "2309.05251": "|**2023-09-11**|**Multi3DRefer: Grounding Text Description to Multiple 3D Objects**|Yiming Zhang et.al.|[2309.05251](http://arxiv.org/abs/2309.05251)|null|\n", "2309.04561": "|**2024-07-16**|**Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding**|Ozan Unal et.al.|[2309.04561](http://arxiv.org/abs/2309.04561)|null|\n", "2308.11887": "|**2023-11-20**|**A Unified Framework for 3D Point Cloud Visual Grounding**|Haojia Lin et.al.|[2308.11887](http://arxiv.org/abs/2308.11887)|**[link](https://github.com/leon1207/3dreftr)**|\n", "2307.13363": "|**2023-07-25**|**3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding**|Zehan Wang et.al.|[2307.13363](http://arxiv.org/abs/2307.13363)|null|\n", "2307.09267": "|**2023-07-18**|**Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding**|Zehan Wang et.al.|[2307.09267](http://arxiv.org/abs/2307.09267)|**[link](https://github.com/ZzZZCHS/WS-3DVG)**|\n", "2305.15765": "|**2023-05-25**|**Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving**|Wenhao Cheng et.al.|[2305.15765](http://arxiv.org/abs/2305.15765)|null|\n", "2305.13876": "|**2024-02-07**|**Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans**|Taiki Miyanishi et.al.|[2305.13876](http://arxiv.org/abs/2305.13876)|**[link](https://github.com/atr-dbi/cross3dvg)**|\n", "2304.05645": "|**2024-07-15**|**WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language**|Zhenxiang Lin et.al.|[2304.05645](http://arxiv.org/abs/2304.05645)|**[link](https://github.com/4dvlab/wildrefer)**|\n", "2303.16894": "|**2023-12-05**|**ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance**|Zoey Guo et.al.|[2303.16894](http://arxiv.org/abs/2303.16894)|**[link](https://github.com/ivan-tang-3d/viewrefer3d)**|\n", "2303.13483": "|**2023-03-23**|**NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations**|Joy Hsu et.al.|[2303.13483](http://arxiv.org/abs/2303.13483)|null|\n", "2303.13186": "|**2023-03-23**|**ScanERU: Interactive 3D Visual Grounding based on Embodied Reference Understanding**|Ziyang Lu et.al.|[2303.13186](http://arxiv.org/abs/2303.13186)|**[link](https://github.com/mrlearnedtoad/scaneru)**|\n", "2212.00836": "|**2022-12-01**|**UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2212.00836](http://arxiv.org/abs/2212.00836)|null|\n", "2211.14241": "|**2022-11-25**|**Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding**|Eslam Mohamed Bakr et.al.|[2211.14241](http://arxiv.org/abs/2211.14241)|**[link](https://github.com/eslambakr/LAR-Look-Around-and-Refer)**|\n", "2210.12513": "|**2023-06-09**|**Learning Point-Language Hierarchical Alignment for 3D Visual Grounding**|Jiaming Chen et.al.|[2210.12513](http://arxiv.org/abs/2210.12513)|**[link](https://github.com/ppjmchen/ham)**|\n", "2209.14941": "|**2023-04-24**|**EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**|Yanmin Wu et.al.|[2209.14941](http://arxiv.org/abs/2209.14941)|**[link](https://github.com/yanmin-wu/eda)**|\n", "2207.01821": "|**2023-05-27**|**Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases**|Zhihao Yuan et.al.|[2207.01821](http://arxiv.org/abs/2207.01821)|null|\n", "2204.06272": "|**2022-04-13**|**3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection**|Junyu Luo et.al.|[2204.06272](http://arxiv.org/abs/2204.06272)|**[link](https://github.com/fjhzhixi/3d-sps)**|\n", "2204.02174": "|**2022-04-05**|**Multi-View Transformer for 3D Visual Grounding**|Shijia Huang et.al.|[2204.02174](http://arxiv.org/abs/2204.02174)|**[link](https://github.com/sega-hsj/mvt-3dvg)**|\n", "2112.01551": "|**2022-07-22**|**D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2112.01551](http://arxiv.org/abs/2112.01551)|null|\n", "2108.02388": "|**2021-08-11**|**TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding**|Dailan He et.al.|[2108.02388](http://arxiv.org/abs/2108.02388)|null|\n", "2107.03438": "|**2021-11-04**|**LanguageRefer: Spatial-Language Model for 3D Visual Grounding**|Junha Roh et.al.|[2107.03438](http://arxiv.org/abs/2107.03438)|null|\n", "2105.11450": "|**2021-09-22**|**SAT: 2D Semantics Assisted Training for 3D Visual Grounding**|Zhengyuan Yang et.al.|[2105.11450](http://arxiv.org/abs/2105.11450)|**[link](https://github.com/zyang-ur/SAT)**|\n", "2103.07894": "|**2021-03-17**|**Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images**|Haolin Liu et.al.|[2103.07894](http://arxiv.org/abs/2103.07894)|null|\n", "2103.01128": "|**2021-07-29**|**InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring**|Zhihao Yuan et.al.|[2103.01128](http://arxiv.org/abs/2103.01128)|**[link](https://github.com/CurryYuan/InstanceRefer)**|\n"}, "VOS": {"2412.07072": "|**2024-12-10**|**Stable Mean Teacher for Semi-supervised Video Action Detection**|Akash Kumar et.al.|[2412.07072](http://arxiv.org/abs/2412.07072)|null|\n", "2412.04930": "|**2024-12-09**|**Video Decomposition Prior: A Methodology to Decompose Videos into Layers**|Gaurav Shrivastava et.al.|[2412.04930](http://arxiv.org/abs/2412.04930)|null|\n", "2412.01471": "|**2024-12-03**|**Multi-Granularity Video Object Segmentation**|Sangbeom Lim et.al.|[2412.01471](http://arxiv.org/abs/2412.01471)|null|\n", "2412.01136": "|**2024-12-02**|**Referring Video Object Segmentation via Language-aligned Track Selection**|Seongchan Kim et.al.|[2412.01136](http://arxiv.org/abs/2412.01136)|null|\n", "2411.19210": "|**2024-11-28**|**Track Anything Behind Everything: Zero-Shot Amodal Video Object Segmentation**|Finlay G. C. Hudson et.al.|[2411.19210](http://arxiv.org/abs/2411.19210)|null|\n", "2411.18933": "|**2024-11-28**|**Efficient Track Anything**|Yunyang Xiong et.al.|[2411.18933](http://arxiv.org/abs/2411.18933)|null|\n", "2411.17646": "|**2024-11-26**|**SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation**|Claudia Cuttano et.al.|[2411.17646](http://arxiv.org/abs/2411.17646)|**[link](https://github.com/claudiacuttano/samwise)**|\n", "2411.17576": "|**2024-12-04**|**A Distractor-Aware Memory for Visual Object Tracking with SAM2**|Jovana Videnovic et.al.|[2411.17576](http://arxiv.org/abs/2411.17576)|**[link](https://github.com/jovanavidenovic/dam4sam)**|\n", "2411.11409": "|**2024-11-18**|**IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos**|Yunong Liu et.al.|[2411.11409](http://arxiv.org/abs/2411.11409)|**[link](https://github.com/yunongLiu1/IKEA-Manuals-at-Work)**|\n", "2411.02818": "|**2024-11-05**|**LiVOS: Light Video Object Segmentation with Gated Linear Matching**|Qin Liu et.al.|[2411.02818](http://arxiv.org/abs/2411.02818)|**[link](https://github.com/uncbiag/livos)**|\n", "2410.23191": "|**2024-10-31**|**Continuous Spatio-Temporal Memory Networks for 4D Cardiac Cine MRI Segmentation**|Meng Ye et.al.|[2410.23191](http://arxiv.org/abs/2410.23191)|**[link](https://github.com/deeptag/cstm)**|\n", "2410.22451": "|**2024-10-29**|**Addressing Issues with Working Memory in Video Object Segmentation**|Clayton Bromley et.al.|[2410.22451](http://arxiv.org/abs/2410.22451)|null|\n", "2410.16268": "|**2024-12-17**|**SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree**|Shuangrui Ding et.al.|[2410.16268](http://arxiv.org/abs/2410.16268)|**[link](https://github.com/mark12ding/sam2long)**|\n", "2409.19603": "|**2024-09-29**|**One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos**|Zechen Bai et.al.|[2409.19603](http://arxiv.org/abs/2409.19603)|**[link](https://github.com/showlab/videolisa)**|\n", "2409.19342": "|**2024-09-28**|**X-Prompt: Multi-modal Visual Prompt for Video Object Segmentation**|Pinxue Guo et.al.|[2409.19342](http://arxiv.org/abs/2409.19342)|**[link](https://github.com/pinxueguo/x-prompt)**|\n", "2409.14343": "|**2024-09-22**|**Memory Matching is not Enough: Jointly Improving Memory Matching and Decoding for Video Object Segmentation**|Jintu Zheng et.al.|[2409.14343](http://arxiv.org/abs/2409.14343)|null|\n", "2409.05847": "|**2024-09-09**|**LSVOS Challenge Report: Large-scale Complex and Long Video Object Segmentation**|Henghui Ding et.al.|[2409.05847](http://arxiv.org/abs/2409.05847)|null|\n", "2409.10555": "|**2024-09-03**|**Convolutional Networks as Extremely Small Foundation Models: Visual Prompting and Theoretical Perspective**|Jianqiao Wangni et.al.|[2409.10555](http://arxiv.org/abs/2409.10555)|null|\n", "2408.16431": "|**2024-08-29**|**Discriminative Spatial-Semantic VOS Solution: 1st Place Solution for 6th LSVOS**|Deshui Miao et.al.|[2408.16431](http://arxiv.org/abs/2408.16431)|null|\n", "2408.15876": "|**2024-08-28**|**Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation**|Shaofei Huang et.al.|[2408.15876](http://arxiv.org/abs/2408.15876)|**[link](https://github.com/appletea233/al-ref-sam2)**|\n", "2408.13582": "|**2024-08-24**|**CSS-Segment: 2nd Place Report of LSVOS Challenge VOS Track**|Jinming Chai et.al.|[2408.13582](http://arxiv.org/abs/2408.13582)|null|\n", "2408.12447": "|**2024-08-22**|**The 2nd Solution for LSVOS Challenge RVOS Track: Spatial-temporal Refinement for Consistent Semantic Segmentation**|Tuyen Tran et.al.|[2408.12447](http://arxiv.org/abs/2408.12447)|null|\n", "2408.10541": "|**2024-08-20**|**The Instance-centric Transformer for the RVOS Track of LSVOS Challenge: 3rd Place Solution**|Bin Cao et.al.|[2408.10541](http://arxiv.org/abs/2408.10541)|null|\n", "2408.10469": "|**2024-08-21**|**LSVOS Challenge 3rd Place Report: SAM2 and Cutie based VOS**|Xinyu Liu et.al.|[2408.10469](http://arxiv.org/abs/2408.10469)|null|\n", "2408.10129": "|**2024-08-24**|**UNINEXT-Cutie: The 1st Solution for LSVOS Challenge RVOS Track**|Hao Fang et.al.|[2408.10129](http://arxiv.org/abs/2408.10129)|null|\n", "2408.10125": "|**2024-08-24**|**Video Object Segmentation via SAM 2: The 4th Solution for LSVOS Challenge VOS Track**|Feiyu Pan et.al.|[2408.10125](http://arxiv.org/abs/2408.10125)|null|\n", "2408.09860": "|**2024-11-20**|**3D-Aware Instance Segmentation and Tracking in Egocentric Videos**|Yash Bhalgat et.al.|[2408.09860](http://arxiv.org/abs/2408.09860)|null|\n", "2408.03923": "|**2024-08-07**|**Fast Sprite Decomposition from Animated Graphics**|Tomoyuki Suzuki et.al.|[2408.03923](http://arxiv.org/abs/2408.03923)|null|\n", "2408.03286": "|**2024-08-17**|**Biomedical SAM 2: Segment Anything in Biomedical Images and Videos**|Zhiling Yan et.al.|[2408.03286](http://arxiv.org/abs/2408.03286)|**[link](https://github.com/ZhilingYan/Biomedical-SAM-2)**|\n", "2408.00169": "|**2024-11-12**|**Strike the Balance: On-the-Fly Uncertainty based User Interactions for Long-Term Video Object Segmentation**|St\u00e9phane Vujasinovi\u0107 et.al.|[2408.00169](http://arxiv.org/abs/2408.00169)|**[link](https://github.com/vujas-eteph/lazyxmem)**|\n", "2407.15794": "|**2024-11-01**|**Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video**|Guiqiu Liao et.al.|[2407.15794](http://arxiv.org/abs/2407.15794)|**[link](https://github.com/pcasolab/vdst-net)**|\n", "2407.11714": "|**2024-07-16**|**Improving Unsupervised Video Object Segmentation via Fake Flow Generation**|Suhwan Cho et.al.|[2407.11714](http://arxiv.org/abs/2407.11714)|**[link](https://github.com/suhwan-cho/FakeFlow)**|\n", "2407.11325": "|**2024-07-16**|**VISA: Reasoning Video Object Segmentation via Large Language Models**|Cilin Yan et.al.|[2407.11325](http://arxiv.org/abs/2407.11325)|**[link](https://github.com/cilinyan/revos-api)**|\n", "2407.07760": "|**2024-07-10**|**Learning Spatial-Semantic Features for Robust Video Object Segmentation**|Xin Li et.al.|[2407.07760](http://arxiv.org/abs/2407.07760)|null|\n", "2407.07402": "|**2024-07-10**|**ActionVOS: Actions as Prompts for Video Object Segmentation**|Liangyang Ouyang et.al.|[2407.07402](http://arxiv.org/abs/2407.07402)|**[link](https://github.com/ut-vision/actionvos)**|\n", "2407.06871": "|**2024-07-09**|**Rethinking Image-to-Video Adaptation: An Object-centric Perspective**|Rui Qian et.al.|[2407.06871](http://arxiv.org/abs/2407.06871)|null|\n", "2407.06247": "|**2024-07-08**|**Context Propagation from Proposals for Semantic Video Object Segmentation**|Tinghuai Wang et.al.|[2407.06247](http://arxiv.org/abs/2407.06247)|null|\n", "2407.05916": "|**2024-07-08**|**Non-parametric Contextual Relationship Learning for Semantic Video Object Segmentation**|Tinghuai Wang et.al.|[2407.05916](http://arxiv.org/abs/2407.05916)|null|\n", "2407.05913": "|**2024-07-08**|**Submodular video object proposal selection for semantic object segmentation**|Tinghuai Wang et.al.|[2407.05913](http://arxiv.org/abs/2407.05913)|null|\n", "2406.17628": "|**2024-06-25**|**Video Inpainting Localization with Contrastive Learning**|Zijie Lou et.al.|[2406.17628](http://arxiv.org/abs/2406.17628)|**[link](https://github.com/multimediafor/vilocal)**|\n", "2406.17005": "|**2024-06-24**|**PVUW 2024 Challenge on Complex Video Understanding: Methods and Results**|Henghui Ding et.al.|[2406.17005](http://arxiv.org/abs/2406.17005)|**[link](https://github.com/henghuiding/MOSE-api)**|\n", "2406.13939": "|**2024-06-20**|**2nd Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation**|Bin Cao et.al.|[2406.13939](http://arxiv.org/abs/2406.13939)|null|\n", "2406.13576": "|**2024-06-19**|**Trusted Video Inpainting Localization via Deep Attentive Noise Learning**|Zijie Lou et.al.|[2406.13576](http://arxiv.org/abs/2406.13576)|**[link](https://github.com/multimediafor/truvil)**|\n", "2406.12834": "|**2024-06-23**|**GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation**|Ci-Siang Lin et.al.|[2406.12834](http://arxiv.org/abs/2406.12834)|null|\n", "2406.12536": "|**2024-09-19**|**ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection**|Junhao Lin et.al.|[2406.12536](http://arxiv.org/abs/2406.12536)|**[link](https://github.com/jhl-det/rgbd_video_sod)**|\n", "2406.08476": "|**2024-06-12**|**RMem: Restricted Memory Banks Improve Video Object Segmentation**|Junbao Zhou et.al.|[2406.08476](http://arxiv.org/abs/2406.08476)|null|\n", "2406.08192": "|**2024-06-12**|**2nd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex Video Object Segmentation**|Zhensong Xu et.al.|[2406.08192](http://arxiv.org/abs/2406.08192)|null|\n", "2406.07043": "|**2024-06-11**|**1st Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation**|Mingqi Gao et.al.|[2406.07043](http://arxiv.org/abs/2406.07043)|**[link](https://github.com/tapall-ai/mevis_track_solution_2024)**|\n", "2406.05485": "|**2024-06-08**|**Training-Free Robust Interactive Video Object Segmentation**|Xiaoli Wei et.al.|[2406.05485](http://arxiv.org/abs/2406.05485)|null|\n", "2406.05131": "|**2024-06-07**|**DVOS: Self-Supervised Dense-Pattern Video Object Segmentation**|Keyhan Najafian et.al.|[2406.05131](http://arxiv.org/abs/2406.05131)|null|\n", "2406.04842": "|**2024-06-07**|**3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation**|Feiyu Pan et.al.|[2406.04842](http://arxiv.org/abs/2406.04842)|null|\n", "2406.04600": "|**2024-06-07**|**1st Place Solution for MOSE Track in CVPR 2024 PVUW Workshop: Complex Video Object Segmentation**|Deshui Miao et.al.|[2406.04600](http://arxiv.org/abs/2406.04600)|null|\n", "2406.03668": "|**2024-06-06**|**3rd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex Video Object Segmentation**|Xinyu Liu et.al.|[2406.03668](http://arxiv.org/abs/2406.03668)|null|\n", "2405.19525": "|**2024-05-29**|**Lifelong Learning Using a Dynamically Growing Tree of Sub-networks for Domain Generalization in Video Object Segmentation**|Islam Osman et.al.|[2405.19525](http://arxiv.org/abs/2405.19525)|null|\n", "2405.14010": "|**2024-05-22**|**One-shot Training for Video Object Segmentation**|Baiyu Chen et.al.|[2405.14010](http://arxiv.org/abs/2405.14010)|null|\n", "2405.10610": "|**2024-09-22**|**Harnessing Vision-Language Pretrained Models with Temporal-Aware Adaptation for Referring Video Object Segmentation**|Zikun Zhou et.al.|[2405.10610](http://arxiv.org/abs/2405.10610)|null|\n", "2405.07031": "|**2024-05-11**|**Global Motion Understanding in Large-Scale Video Object Segmentation**|Volodymyr Fedynyak et.al.|[2405.07031](http://arxiv.org/abs/2405.07031)|null|\n", "2405.08715": "|**2024-05-11**|**DeVOS: Flow-Guided Deformable Transformer for Video Object Segmentation**|Volodymyr Fedynyak et.al.|[2405.08715](http://arxiv.org/abs/2405.08715)|null|\n", "2405.04042": "|**2024-05-07**|**Space-time Reinforcement Network for Video Object Segmentation**|Yadang Chen et.al.|[2405.04042](http://arxiv.org/abs/2405.04042)|null|\n", "2404.19326": "|**2024-05-01**|**LVOS: A Benchmark for Large-scale Long-term Video Object Segmentation**|Lingyi Hong et.al.|[2404.19326](http://arxiv.org/abs/2404.19326)|null|\n", "2404.13953": "|**2024-04-22**|**360VOTS: Visual Object Tracking and Segmentation in Omnidirectional Videos**|Yinzhe Xu et.al.|[2404.13953](http://arxiv.org/abs/2404.13953)|null|\n", "2404.13505": "|**2024-04-21**|**Dynamic in Static: Hybrid Visual Correspondence for Self-Supervised Video Object Segmentation**|Gensheng Pei et.al.|[2404.13505](http://arxiv.org/abs/2404.13505)|**[link](https://github.com/nust-machine-intelligence-laboratory/hvc)**|\n", "2404.06265": "|**2024-04-09**|**Spatial-Temporal Multi-level Association for Video Object Segmentation**|Deshui Miao et.al.|[2404.06265](http://arxiv.org/abs/2404.06265)|null|\n", "2404.01945": "|**2024-04-02**|**Event-assisted Low-Light Video Object Segmentation**|Hebei Li et.al.|[2404.01945](http://arxiv.org/abs/2404.01945)|**[link](https://github.com/hebeifast/eventlowlightvos)**|\n", "2403.19407": "|**2024-10-11**|**Temporally Consistent Referring Video Object Segmentation with Hybrid Memory**|Bo Miao et.al.|[2403.19407](http://arxiv.org/abs/2403.19407)|**[link](https://github.com/bo-miao/HTR)**|\n", "2403.18690": "|**2024-03-27**|**Annolid: Annotate, Segment, and Track Anything You Need**|Chen Yang et.al.|[2403.18690](http://arxiv.org/abs/2403.18690)|null|\n", "2403.17937": "|**2024-09-26**|**Efficient Video Object Segmentation via Modulated Cross-Attention Memory**|Abdelrahman Shaker et.al.|[2403.17937](http://arxiv.org/abs/2403.17937)|**[link](https://github.com/amshaker/mavos)**|\n", "2403.14598": "|**2024-03-21**|**PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model**|Zheng Zhang et.al.|[2403.14598](http://arxiv.org/abs/2403.14598)|**[link](https://github.com/zamling/psalm)**|\n", "2403.12042": "|**2024-07-06**|**Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation**|Zixin Zhu et.al.|[2403.12042](http://arxiv.org/abs/2403.12042)|**[link](https://github.com/buxiangzhiren/vd-it)**|\n", "2403.11529": "|**2024-03-18**|**Video Object Segmentation with Dynamic Query Modulation**|Hantao Zhou et.al.|[2403.11529](http://arxiv.org/abs/2403.11529)|**[link](https://github.com/zht8506/qmvos)**|\n", "2403.08682": "|**2024-03-13**|**OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework**|Wanyun Li et.al.|[2403.08682](http://arxiv.org/abs/2403.08682)|null|\n", "2403.08003": "|**2024-07-02**|**Augmenting Efficient Real-time Surgical Instrument Segmentation in Video with Point Tracking and Segment Anything**|Zijian Wu et.al.|[2403.08003](http://arxiv.org/abs/2403.08003)|**[link](https://github.com/wuzijian1997/sis-pt-sam)**|\n", "2403.06130": "|**2024-03-10**|**ClickVOS: Click Video Object Segmentation**|Pinxue Guo et.al.|[2403.06130](http://arxiv.org/abs/2403.06130)|null|\n", "2403.04258": "|**2024-03-07**|**Depth-aware Test-Time Training for Zero-shot Video Object Segmentation**|Weihuang Liu et.al.|[2403.04258](http://arxiv.org/abs/2403.04258)|**[link](https://github.com/NiFangBaAGe/DATTT)**|\n", "2402.19082": "|**2024-02-29**|**VideoMAC: Video Masked Autoencoders Meet ConvNets**|Gensheng Pei et.al.|[2402.19082](http://arxiv.org/abs/2402.19082)|**[link](https://github.com/nust-machine-intelligence-laboratory/videomac)**|\n", "2402.09883": "|**2024-02-15**|**Lester: rotoscope animation through video object segmentation and tracking**|Ruben Tous et.al.|[2402.09883](http://arxiv.org/abs/2402.09883)|**[link](https://github.com/rtous/lester)**|\n", "2402.08882": "|**2024-02-14**|**Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation**|Ge Shi et.al.|[2402.08882](http://arxiv.org/abs/2402.08882)|null|\n", "2402.05917": "|**2024-06-10**|**Point-VOS: Pointing Up Video Object Segmentation**|Idil Esen Zulfikar et.al.|[2402.05917](http://arxiv.org/abs/2402.05917)|null|\n", "2402.04921": "|**2024-03-04**|**Is Two-shot All You Need? A Label-efficient Approach for Video Segmentation in Breast Ultrasound**|Jiajun Zeng et.al.|[2402.04921](http://arxiv.org/abs/2402.04921)|null|\n", "2401.13937": "|**2024-03-18**|**Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention**|Quang-Trung Truong et.al.|[2401.13937](http://arxiv.org/abs/2401.13937)|null|\n", "2401.12480": "|**2024-02-04**|**Explore Synergistic Interaction Across Frames for Interactive Video Object Segmentation**|Kexin Li et.al.|[2401.12480](http://arxiv.org/abs/2401.12480)|null|\n", "2401.10831": "|**2024-04-10**|**Understanding Video Transformers via Universal Concept Discovery**|Matthew Kowal et.al.|[2401.10831](http://arxiv.org/abs/2401.10831)|null|\n", "2401.10229": "|**2024-10-01**|**OMG-Seg: Is One Model Good Enough For All Segmentation?**|Xiangtai Li et.al.|[2401.10229](http://arxiv.org/abs/2401.10229)|**[link](https://github.com/lxtgh/omg-seg)**|\n", "2401.00663": "|**2024-01-01**|**1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation**|Zhuoyan Luo et.al.|[2401.00663](http://arxiv.org/abs/2401.00663)|**[link](https://github.com/robertluo1/iccv2023_rvos_challenge)**|\n", "2312.17448": "|**2023-12-29**|**Tracking with Human-Intent Reasoning**|Jiawen Zhu et.al.|[2312.17448](http://arxiv.org/abs/2312.17448)|**[link](https://github.com/jiawen-zhu/trackgpt)**|\n", "2312.15715": "|**2023-12-25**|**UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces**|Jiannan Wu et.al.|[2312.15715](http://arxiv.org/abs/2312.15715)|**[link](https://github.com/foundationvision/uniref)**|\n", "2312.13008": "|**2023-12-20**|**No More Shortcuts: Realizing the Potential of Temporal Self-Supervision**|Ishan Rajendrakumar Dave et.al.|[2312.13008](http://arxiv.org/abs/2312.13008)|null|\n", "2312.09525": "|**2023-12-15**|**Hierarchical Graph Pattern Understanding for Zero-Shot VOS**|Gensheng Pei et.al.|[2312.09525](http://arxiv.org/abs/2312.09525)|**[link](https://github.com/nust-machine-intelligence-laboratory/hgpu)**|\n", "2312.08514": "|**2024-04-09**|**TAM-VT: Transformation-Aware Multi-scale Video Transformer for Segmentation and Tracking**|Raghav Goyal et.al.|[2312.08514](http://arxiv.org/abs/2312.08514)|null|\n", "2312.07169": "|**2024-04-03**|**Semi-supervised Active Learning for Video Action Detection**|Ayush Singh et.al.|[2312.07169](http://arxiv.org/abs/2312.07169)|**[link](https://github.com/akash2907/semi-sup-active-learning)**|\n", "2312.06592": "|**2023-12-11**|**Flexible visual prompts for in-context learning in computer vision**|Thomas Foster et.al.|[2312.06592](http://arxiv.org/abs/2312.06592)|**[link](https://github.com/v7labs/xmem_icl)**|\n", "2311.18837": "|**2023-11-30**|**VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models**|Zhen Xing et.al.|[2311.18837](http://arxiv.org/abs/2311.18837)|null|\n", "2311.18286": "|**2023-11-30**|**SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation**|Lingyi Hong et.al.|[2311.18286](http://arxiv.org/abs/2311.18286)|null|\n", "2311.17893": "|**2024-07-08**|**Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation**|Shuangrui Ding et.al.|[2311.17893](http://arxiv.org/abs/2311.17893)|**[link](https://github.com/shvdiwnkozbw/ssl-uvos)**|\n", "2311.14671": "|**2024-07-22**|**SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation**|Lingchen Meng et.al.|[2311.14671](http://arxiv.org/abs/2311.14671)|**[link](https://github.com/menglcool/segic)**|\n", "2311.07261": "|**2023-11-13**|**Sketch-based Video Object Segmentation: Benchmark and Analysis**|Ruolin Yang et.al.|[2311.07261](http://arxiv.org/abs/2311.07261)|null|\n", "2311.04414": "|**2023-11-11**|**Learning the What and How of Annotation in Video Object Segmentation**|Thanos Delatolas et.al.|[2311.04414](http://arxiv.org/abs/2311.04414)|null|\n", "2311.02734": "|**2023-11-05**|**ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification**|Nicolas Gorlo et.al.|[2311.02734](http://arxiv.org/abs/2311.02734)|null|\n", "2310.15115": "|**2023-10-23**|**SpVOS: Efficient Video Object Segmentation with Triple Sparse Convolution**|Weihao Lin et.al.|[2310.15115](http://arxiv.org/abs/2310.15115)|null|\n", "2310.12982": "|**2024-04-11**|**Putting the Object Back into Video Object Segmentation**|Ho Kei Cheng et.al.|[2310.12982](http://arxiv.org/abs/2310.12982)|**[link](https://github.com/hkchengrex/Cutie)**|\n", "2310.06992": "|**2024-01-25**|**Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models**|Wen-Hsuan Chu et.al.|[2310.06992](http://arxiv.org/abs/2310.06992)|null|\n", "2310.01946": "|**2023-10-03**|**CoralVOS: Dataset and Benchmark for Coral Video Segmentation**|Zheng Ziqiang et.al.|[2310.01946](http://arxiv.org/abs/2310.01946)|null|\n", "2309.15274": "|**2024-02-14**|**Memory-Efficient Continual Learning Object Segmentation for Long Video**|Amir Nazemi et.al.|[2309.15274](http://arxiv.org/abs/2309.15274)|null|\n", "2309.14786": "|**2023-09-26**|**Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation**|Suhwan Cho et.al.|[2309.14786](http://arxiv.org/abs/2309.14786)|**[link](https://github.com/suhwan-cho/tmo)**|\n", "2309.13857": "|**2023-09-25**|**Adversarial Attacks on Video Object Segmentation with Hard Region Discovery**|Ping Li et.al.|[2309.13857](http://arxiv.org/abs/2309.13857)|null|\n", "2309.12303": "|**2024-07-28**|**PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation**|Shilin Yan et.al.|[2309.12303](http://arxiv.org/abs/2309.12303)|**[link](https://github.com/shilinyan99/panovos)**|\n", "2309.11933": "|**2023-09-21**|**Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation**|Ping Li et.al.|[2309.11933](http://arxiv.org/abs/2309.11933)|null|\n", "2309.11707": "|**2023-09-21**|**Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation**|Ping Li et.al.|[2309.11707](http://arxiv.org/abs/2309.11707)|null|\n", "2309.11160": "|**2023-09-20**|**Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation**|Nian Liu et.al.|[2309.11160](http://arxiv.org/abs/2309.11160)|**[link](https://github.com/nankepan/VIPMT)**|\n", "2309.03903": "|**2023-09-07**|**Tracking Anything with Decoupled Video Segmentation**|Ho Kei Cheng et.al.|[2309.03903](http://arxiv.org/abs/2309.03903)|**[link](https://github.com/hkchengrex/Tracking-Anything-with-DEVA)**|\n", "2309.03473": "|**2023-09-07**|**Temporal Collection and Distribution for Referring Video Object Segmentation**|Jiajin Tang et.al.|[2309.03473](http://arxiv.org/abs/2309.03473)|null|\n", "2309.03247": "|**2023-10-18**|**Robust Visual Tracking by Motion Analyzing**|Mohammed Leo et.al.|[2309.03247](http://arxiv.org/abs/2309.03247)|null|\n", "2309.02041": "|**2023-09-05**|**Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples**|Guanghui Li et.al.|[2309.02041](http://arxiv.org/abs/2309.02041)|**[link](https://github.com/hengliusky/few_shot_rvos)**|\n", "2308.13505": "|**2023-08-25**|**Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation**|Jiaming Zhang et.al.|[2308.13505](http://arxiv.org/abs/2308.13505)|null|\n", "2308.13266": "|**2023-09-21**|**Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation**|Yuanyou Xu et.al.|[2308.13266](http://arxiv.org/abs/2308.13266)|**[link](https://github.com/yoxu515/mits)**|\n", "2308.11239": "|**2023-12-02**|**LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training**|Silky Singh et.al.|[2308.11239](http://arxiv.org/abs/2308.11239)|**[link](https://github.com/silky1708/locate)**|\n", "2308.09903": "|**2023-08-19**|**Scalable Video Object Segmentation with Simplified Framework**|Qiangqiang Wu et.al.|[2308.09903](http://arxiv.org/abs/2308.09903)|null|\n", "2308.08544": "|**2023-08-16**|**MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions**|Henghui Ding et.al.|[2308.08544](http://arxiv.org/abs/2308.08544)|**[link](https://github.com/henghuiding/MeViS)**|\n", "2308.06693": "|**2023-08-13**|**Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation**|Yichen Yuan et.al.|[2308.06693](http://arxiv.org/abs/2308.06693)|**[link](https://github.com/dlut-yyc/isomer)**|\n", "2308.04162": "|**2023-08-08**|**EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation**|Jiajun Chen et.al.|[2308.04162](http://arxiv.org/abs/2308.04162)|**[link](https://github.com/lab206/epcformer)**|\n", "2308.02162": "|**2023-12-15**|**Learning Referring Video Object Segmentation from Weak Annotation**|Wangbo Zhao et.al.|[2308.02162](http://arxiv.org/abs/2308.02162)|null|\n", "2307.15958": "|**2023-08-15**|**XMem++: Production-level Video Segmentation From Few Annotated Frames**|Maksym Bekuzarov et.al.|[2307.15958](http://arxiv.org/abs/2307.15958)|**[link](https://github.com/max810/XMem2)**|\n", "2307.13974": "|**2023-07-26**|**Tracking Anything in High Quality**|Jiawen Zhu et.al.|[2307.13974](http://arxiv.org/abs/2307.13974)|**[link](https://github.com/jiawen-zhu/hqtrack)**|\n", "2307.13537": "|**2023-07-25**|**Spectrum-guided Multi-granularity Referring Video Object Segmentation**|Bo Miao et.al.|[2307.13537](http://arxiv.org/abs/2307.13537)|**[link](https://github.com/bo-miao/sgmg)**|\n", "2307.09356": "|**2023-07-18**|**OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation**|Dongming Wu et.al.|[2307.09356](http://arxiv.org/abs/2307.09356)|**[link](https://github.com/wudongming97/onlinerefer)**|\n", "2307.08263": "|**2023-07-17**|**Hierarchical Spatiotemporal Transformers for Video Object Segmentation**|Jun-Sang Yoo et.al.|[2307.08263](http://arxiv.org/abs/2307.08263)|null|\n", "2307.07933": "|**2023-07-16**|**Holistic Prototype Attention Network for Few-Shot VOS**|Yin Tang et.al.|[2307.07933](http://arxiv.org/abs/2307.07933)|**[link](https://github.com/nust-machine-intelligence-laboratory/hpan)**|\n", "2307.07812": "|**2023-07-15**|**Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation**|Mennatullah Siam et.al.|[2307.07812](http://arxiv.org/abs/2307.07812)|**[link](https://github.com/msiam/mmc-multiscalememory)**|\n", "2307.04392": "|**2023-07-10**|**FODVid: Flow-guided Object Discovery in Videos**|Silky Singh et.al.|[2307.04392](http://arxiv.org/abs/2307.04392)|null|\n", "2307.02508": "|**2023-07-10**|**ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking**|Yuanyou Xu et.al.|[2307.02508](http://arxiv.org/abs/2307.02508)|null|\n", "2307.02010": "|**2023-07-10**|**ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation**|Jiahao Li et.al.|[2307.02010](http://arxiv.org/abs/2307.02010)|null|\n", "2307.01197": "|**2023-12-03**|**Segment Anything Meets Point Tracking**|Frano Raji\u010d et.al.|[2307.01197](http://arxiv.org/abs/2307.01197)|**[link](https://github.com/syscv/sam-pt)**|\n", "2307.00997": "|**2024-09-03**|**RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation**|Yonglin Li et.al.|[2307.00997](http://arxiv.org/abs/2307.00997)|**[link](https://github.com/lancasterli/refsam)**|\n", "2307.00536": "|**2023-09-17**|**Bidirectional Correlation-Driven Inter-Frame Interaction Transformer for Referring Video Object Segmentation**|Meng Lan et.al.|[2307.00536](http://arxiv.org/abs/2307.00536)|null|\n", "2306.15377": "|**2023-06-28**|**TrickVOS: A Bag of Tricks for Video Object Segmentation**|Evangelos Skartados et.al.|[2306.15377](http://arxiv.org/abs/2306.15377)|null|\n", "2306.12048": "|**2024-01-17**|**Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering**|Lin Xi et.al.|[2306.12048](http://arxiv.org/abs/2306.12048)|**[link](https://github.com/xilin1991/CluterNet)**|\n", "2306.08736": "|**2024-04-02**|**LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation**|Linfeng Yuan et.al.|[2306.08736](http://arxiv.org/abs/2306.08736)|**[link](https://github.com/linfengyuan1997/losh)**|\n", "2306.08731": "|**2024-02-01**|**EPIC Fields: Marrying 3D Geometry and Video Understanding**|Vadim Tschernezki et.al.|[2306.08731](http://arxiv.org/abs/2306.08731)|**[link](https://github.com/epic-kitchens/epic-fields-code)**|\n", "2305.17011": "|**2023-05-26**|**SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation**|Zhuoyan Luo et.al.|[2305.17011](http://arxiv.org/abs/2305.17011)|**[link](https://github.com/RobertLuo1/NeurIPS2023_SOC)**|\n", "2305.16318": "|**2023-12-12**|**Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation**|Shilin Yan et.al.|[2305.16318](http://arxiv.org/abs/2305.16318)|**[link](https://github.com/opengvlab/mutr)**|\n", "2305.14731": "|**2023-05-24**|**AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras**|Peyman Gholami et.al.|[2305.14731](http://arxiv.org/abs/2305.14731)|null|\n", "2305.14344": "|**2023-05-23**|**Siamese Masked Autoencoders**|Agrim Gupta et.al.|[2305.14344](http://arxiv.org/abs/2305.14344)|null|\n", "2305.12823": "|**2023-09-25**|**READMem: Robust Embedding Association for a Diverse Memory in Unconstrained Video Object Segmentation**|St\u00e9phane Vujasinovi\u0107 et.al.|[2305.12823](http://arxiv.org/abs/2305.12823)|**[link](https://github.com/Vujas-Eteph/READMem)**|\n", "2305.12659": "|**2024-06-06**|**UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model**|Zhenghao Zhang et.al.|[2305.12659](http://arxiv.org/abs/2305.12659)|**[link](https://github.com/alibaba/uvosam)**|\n", "2305.04470": "|**2023-06-25**|**Video Object Segmentation in Panoptic Wild Scenes**|Yuanyou Xu et.al.|[2305.04470](http://arxiv.org/abs/2305.04470)|**[link](https://github.com/yoxu515/viposeg-benchmark)**|\n", "2305.03048": "|**2023-10-04**|**Personalize Segment Anything Model with One Shot**|Renrui Zhang et.al.|[2305.03048](http://arxiv.org/abs/2305.03048)|**[link](https://github.com/zrrskywalker/personalize-sam)**|\n", "2304.11840": "|**2023-04-24**|**Robust and Efficient Memory Network for Video Object Segmentation**|Yadang Chen et.al.|[2304.11840](http://arxiv.org/abs/2304.11840)|null|\n", "2304.08025": "|**2023-04-17**|**Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping**|Long Lian et.al.|[2304.08025](http://arxiv.org/abs/2304.08025)|**[link](https://github.com/TonyLianLong/RCF-UnsupVideoSeg)**|\n", "2304.06718": "|**2023-07-11**|**Segment Everything Everywhere All at Once**|Xueyan Zou et.al.|[2304.06718](http://arxiv.org/abs/2304.06718)|null|\n", "2304.06211": "|**2023-04-13**|**Boosting Video Object Segmentation via Space-time Correspondence Learning**|Yurong Zhang et.al.|[2304.06211](http://arxiv.org/abs/2304.06211)|**[link](https://github.com/wenguanwang/vos_correspondence)**|\n", "2304.05930": "|**2024-09-17**|**MED-VT++: Unifying Multimodal Learning with a Multiscale Encoder-Decoder Video Transformer**|Rezaul Karim et.al.|[2304.05930](http://arxiv.org/abs/2304.05930)|null|\n", "2304.04259": "|**2023-04-09**|**CLVOS23: A Long Video Object Segmentation Dataset for Continual Learning**|Amir Nazemi et.al.|[2304.04259](http://arxiv.org/abs/2304.04259)|**[link](https://github.com/amir4g/clvos23)**|\n", "2304.03910": "|**2023-04-08**|**Co-attention Propagation Network for Zero-Shot Video Object Segmentation**|Gensheng Pei et.al.|[2304.03910](http://arxiv.org/abs/2304.03910)|**[link](https://github.com/nust-machine-intelligence-laboratory/hcpn)**|\n", "2304.03284": "|**2023-04-06**|**SegGPT: Segmenting Everything In Context**|Xinlong Wang et.al.|[2304.03284](http://arxiv.org/abs/2304.03284)|**[link](https://github.com/baaivision/painter)**|\n", "2304.00571": "|**2023-04-07**|**DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks**|Qiangqiang Wu et.al.|[2304.00571](http://arxiv.org/abs/2304.00571)|**[link](https://github.com/jimmy-dq/dropmae)**|\n", "2303.14384": "|**2023-03-25**|**Reliability-Hierarchical Memory Network for Scribble-Supervised Video Object Segmentation**|Zikun Zhou et.al.|[2303.14384](http://arxiv.org/abs/2303.14384)|**[link](https://github.com/mkg1204/rhmnet-for-ssvos)**|\n", "2303.13245": "|**2023-03-23**|**CrOC: Cross-View Online Clustering for Dense Visual Representation Learning**|Thomas Stegm\u00fcller et.al.|[2303.13245](http://arxiv.org/abs/2303.13245)|**[link](https://github.com/stegmuel/croc)**|\n", "2303.12078": "|**2023-03-21**|**Two-shot Video Object Segmentation**|Kun Yan et.al.|[2303.12078](http://arxiv.org/abs/2303.12078)|**[link](https://github.com/yk-pku/two-shot-video-object-segmentation)**|\n", "2303.10383": "|**2024-02-03**|**Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation**|Xiaoqi Zhao et.al.|[2303.10383](http://arxiv.org/abs/2303.10383)|**[link](https://github.com/xiaoqi-zhao-dlut/multi-source-aps-zvos)**|\n", "2303.10100": "|**2023-03-17**|**Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation**|Liulei Li et.al.|[2303.10100](http://arxiv.org/abs/2303.10100)|**[link](https://github.com/0liliulei/mask-vos)**|\n", "2303.08314": "|**2024-03-31**|**Guided Slot Attention for Unsupervised Video Object Segmentation**|Minhyeok Lee et.al.|[2303.08314](http://arxiv.org/abs/2303.08314)|**[link](https://github.com/hydragon516/gsanet)**|\n", "2303.07815": "|**2023-03-14**|**MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation**|Roy Miles et.al.|[2303.07815](http://arxiv.org/abs/2303.07815)|null|\n", "2303.04376": "|**2024-02-21**|**Tsanet: Temporal and Scale Alignment for Unsupervised Video Object Segmentation**|Seunghoon Lee et.al.|[2303.04376](http://arxiv.org/abs/2303.04376)|null|\n", "2302.14362": "|**2023-02-28**|**One-Shot Video Inpainting**|Sangjin Lee et.al.|[2302.14362](http://arxiv.org/abs/2302.14362)|null|\n", "2302.03793": "|**2023-02-07**|**Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction**|Yangxiao Lu et.al.|[2302.03793](http://arxiv.org/abs/2302.03793)|null|\n", "2302.01872": "|**2023-02-03**|**MOSE: A New Dataset for Video Object Segmentation in Complex Scenes**|Henghui Ding et.al.|[2302.01872](http://arxiv.org/abs/2302.01872)|**[link](https://github.com/henghuiding/MOSE-api)**|\n", "2301.12352": "|**2023-01-29**|**Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation**|Jialin Yuan et.al.|[2301.12352](http://arxiv.org/abs/2301.12352)|null|\n", "2301.10492": "|**2023-01-25**|**Flow-guided Semi-supervised Video Object Segmentation**|Yushan Zhang et.al.|[2301.10492](http://arxiv.org/abs/2301.10492)|null|\n", "2301.02657": "|**2023-05-10**|**TarViS: A Unified Approach for Target-based Video Segmentation**|Ali Athar et.al.|[2301.02657](http://arxiv.org/abs/2301.02657)|**[link](https://github.com/Ali2500/TarViS)**|\n", "2212.14679": "|**2022-12-27**|**1st Place Solution for YouTubeVOS Challenge 2022: Referring Video Object Segmentation**|Zhiwei Hu et.al.|[2212.14679](http://arxiv.org/abs/2212.14679)|**[link](https://github.com/zhiweihhh/cvpr2022-rvos-challenge)**|\n", "2212.08816": "|**2022-12-17**|**Improving Unsupervised Video Object Segmentation with Motion-Appearance Synergy**|Long Lian et.al.|[2212.08816](http://arxiv.org/abs/2212.08816)|null|\n", "2212.08058": "|**2022-12-15**|**Learning a Fast 3D Spectral Approach to Object Segmentation and Tracking over Space and Time**|Elena Burceanu et.al.|[2212.08058](http://arxiv.org/abs/2212.08058)|null|\n", "2212.06826": "|**2022-12-13**|**Look Before You Match: Instance Understanding Matters in Video Object Segmentation**|Junke Wang et.al.|[2212.06826](http://arxiv.org/abs/2212.06826)|null|\n", "2212.06200": "|**2023-03-28**|**Breaking the \"Object\" in Video Object Segmentation**|Pavel Tokmakov et.al.|[2212.06200](http://arxiv.org/abs/2212.06200)|null|\n", "2212.02871": "|**2022-12-06**|**Video Object of Interest Segmentation**|Siyuan Zhou et.al.|[2212.02871](http://arxiv.org/abs/2212.02871)|null|\n", "2212.02112": "|**2022-12-05**|**Learning to Learn Better for Video Object Segmentation**|Meng Lan et.al.|[2212.02112](http://arxiv.org/abs/2212.02112)|**[link](https://github.com/vitae-transformer/vos-llb)**|\n", "2211.12036": "|**2024-03-26**|**Dual Prototype Attention for Unsupervised Video Object Segmentation**|Suhwan Cho et.al.|[2211.12036](http://arxiv.org/abs/2211.12036)|**[link](https://github.com/hydragon516/dpa)**|\n", "2211.10181": "|**2023-08-18**|**LVOS: A Benchmark for Long-term Video Object Segmentation**|Lingyi Hong et.al.|[2211.10181](http://arxiv.org/abs/2211.10181)|**[link](https://github.com/LingyiHongfd/LVOS)**|\n", "2211.08352": "|**2022-11-13**|**Visual Semantic Segmentation Based on Few/Zero-Shot Learning: An Overview**|Wenqi Ren et.al.|[2211.08352](http://arxiv.org/abs/2211.08352)|null|\n", "2211.05364": "|**2022-11-21**|**Efficient Unsupervised Video Object Segmentation Network Based on Motion Guidance**|Chao Hu et.al.|[2211.05364](http://arxiv.org/abs/2211.05364)|null|\n", "2211.01783": "|**2024-09-16**|**Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks**|Matthew Kowal et.al.|[2211.01783](http://arxiv.org/abs/2211.01783)|null|\n", "2210.16795": "|**2022-10-30**|**Two-Level Temporal Relation Model for Online Video Instance Segmentation**|\u00c7a\u011fan Selim \u00c7oban et.al.|[2210.16795](http://arxiv.org/abs/2210.16795)|**[link](https://github.com/caganselim/tltm)**|\n", "2210.12733": "|**2022-10-23**|**Self-supervised Amodal Video Object Segmentation**|Jian Yao et.al.|[2210.12733](http://arxiv.org/abs/2210.12733)|null|\n", "2210.09782": "|**2022-11-28**|**Decoupling Features in Hierarchical Propagation for Video Object Segmentation**|Zongxin Yang et.al.|[2210.09782](http://arxiv.org/abs/2210.09782)|**[link](https://github.com/z-x-yang/AOT)**|\n", "2210.05567": "|**2022-10-12**|**Global Spectral Filter Memory Network for Video Object Segmentation**|Yong Liu et.al.|[2210.05567](http://arxiv.org/abs/2210.05567)|**[link](https://github.com/workforai/gsfm)**|\n", "2210.04154": "|**2022-10-09**|**Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders**|Haosen Yang et.al.|[2210.04154](http://arxiv.org/abs/2210.04154)|**[link](https://github.com/happy-hsy/motionmae)**|\n", "2209.13064": "|**2022-09-26**|**EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations**|Ahmad Darkhalil et.al.|[2209.13064](http://arxiv.org/abs/2209.13064)|**[link](https://github.com/epic-kitchens/visor-wdtcf)**|\n", "2209.12118": "|**2022-11-22**|**BURST: A Benchmark for Unifying Object Recognition, Segmentation and Tracking in Video**|Ali Athar et.al.|[2209.12118](http://arxiv.org/abs/2209.12118)|**[link](https://github.com/ali2500/burst-benchmark)**|\n", "2209.09341": "|**2022-10-19**|**A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation**|Georgy Ponimatkin et.al.|[2209.09341](http://arxiv.org/abs/2209.09341)|**[link](https://github.com/ponimatkin/ssl-vos)**|\n", "2209.03712": "|**2022-09-08**|**Unsupervised Video Object Segmentation via Prototype Memory Network**|Minhyeok Lee et.al.|[2209.03712](http://arxiv.org/abs/2209.03712)|**[link](https://github.com/Hydragon516/PMN)**|\n", "2209.03138": "|**2022-11-02**|**Treating Motion as Option to Reduce Motion Dependency in Unsupervised Video Object Segmentation**|Suhwan Cho et.al.|[2209.03138](http://arxiv.org/abs/2209.03138)|**[link](https://github.com/suhwan-cho/tmo)**|\n", "2209.03139": "|**2023-02-01**|**Pixel-Level Equalized Matching for Video Object Segmentation**|Suhwan Cho et.al.|[2209.03139](http://arxiv.org/abs/2209.03139)|null|\n", "2209.00383": "|**2023-12-05**|**TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut**|Yangtao Wang et.al.|[2209.00383](http://arxiv.org/abs/2209.00383)|null|\n", "2208.10128": "|**2022-08-22**|**SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization**|Zhihui Lin et.al.|[2208.10128](http://arxiv.org/abs/2208.10128)|**[link](https://github.com/lmm077/SWEM)**|\n", "2208.04026": "|**2022-08-08**|**Two-Stream Networks for Object Segmentation in Videos**|Hannan Lu et.al.|[2208.04026](http://arxiv.org/abs/2208.04026)|null|\n", "2208.01924": "|**2022-08-03**|**Per-Clip Video Object Segmentation**|Kwanyong Park et.al.|[2208.01924](http://arxiv.org/abs/2208.01924)|**[link](https://github.com/pkyong95/PCVOS)**|\n", "2208.01159": "|**2022-08-08**|**BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation**|Ye Yu et.al.|[2208.01159](http://arxiv.org/abs/2208.01159)|null|\n", "2207.12622": "|**2022-07-26**|**Multi-Attention Network for Compressed Video Referring Object Segmentation**|Weidong Chen et.al.|[2207.12622](http://arxiv.org/abs/2207.12622)|**[link](https://github.com/dexianghong/manet)**|\n", "2207.10661": "|**2022-07-21**|**In Defense of Online Models for Video Instance Segmentation**|Junfeng Wu et.al.|[2207.10661](http://arxiv.org/abs/2207.10661)|**[link](https://github.com/wjf5203/vnext)**|\n", "2207.10456": "|**2022-07-22**|**Semantic-Aware Fine-Grained Correspondence**|Yingdong Hu et.al.|[2207.10456](http://arxiv.org/abs/2207.10456)|**[link](https://github.com/alxead/sfc)**|\n", "2207.10258": "|**2022-07-21**|**Region Aware Video Object Segmentation with Deep Motion Modeling**|Bo Miao et.al.|[2207.10258](http://arxiv.org/abs/2207.10258)|null|\n", "2207.08485": "|**2022-07-19**|**Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation**|Gensheng Pei et.al.|[2207.08485](http://arxiv.org/abs/2207.08485)|**[link](https://github.com/NUST-Machine-Intelligence-Laboratory/HFAN)**|\n", "2207.07922": "|**2022-07-16**|**Learning Quality-aware Dynamic Memory for Video Object Segmentation**|Yong Liu et.al.|[2207.07922](http://arxiv.org/abs/2207.07922)|**[link](https://github.com/workforai/qdmn)**|\n", "2207.07115": "|**2022-07-18**|**XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model**|Ho Kei Cheng et.al.|[2207.07115](http://arxiv.org/abs/2207.07115)|**[link](https://github.com/hkchengrex/XMem)**|\n", "2207.06953": "|**2022-08-15**|**Tackling Background Distraction in Video Object Segmentation**|Suhwan Cho et.al.|[2207.06953](http://arxiv.org/abs/2207.06953)|**[link](https://github.com/suhwan-cho/tbd)**|\n", "2207.02088": "|**2022-07-05**|**SiamMask: A Framework for Fast Online Object Tracking and Segmentation**|Weiming Hu et.al.|[2207.02088](http://arxiv.org/abs/2207.02088)|null|\n", "2207.01203": "|**2023-08-18**|**Towards Robust Referring Video Object Segmentation with Cyclic Relational Consensus**|Xiang Li et.al.|[2207.01203](http://arxiv.org/abs/2207.01203)|**[link](https://github.com/lxa9867/R2VOS)**|\n", "2207.00887": "|**2022-07-02**|**Towards Robust Video Object Segmentation with Adaptive Object Calibration**|Xiaohao Xu et.al.|[2207.00887](http://arxiv.org/abs/2207.00887)|**[link](https://github.com/jerryx1110/robust-video-object-segmentation)**|\n", "2206.12035": "|**2022-06-24**|**The Second Place Solution for The 4th Large-scale Video Object Segmentation Challenge--Track 3: Referring Video Object Segmentation**|Leilei Cao et.al.|[2206.12035](http://arxiv.org/abs/2206.12035)|null|\n", "2206.09585": "|**2022-06-20**|**5th Place Solution for YouTube-VOS Challenge 2022: Video Object Segmentation**|Wangwang Yang et.al.|[2206.09585](http://arxiv.org/abs/2206.09585)|null|\n", "2206.03789": "|**2022-06-08**|**Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation**|Zihan Ding et.al.|[2206.03789](http://arxiv.org/abs/2206.03789)|**[link](https://github.com/dzh19990407/lbdt)**|\n", "2206.02846": "|**2022-06-06**|**A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information**|Matthew Kowal et.al.|[2206.02846](http://arxiv.org/abs/2206.02846)|**[link](https://github.com/YorkUCVIL/Static-Dynamic-Interpretability)**|\n", "2206.00182": "|**2022-08-05**|**Differentiable Soft-Masked Attention**|Ali Athar et.al.|[2206.00182](http://arxiv.org/abs/2206.00182)|**[link](https://github.com/Ali2500/HODOR)**|\n", "2205.08075": "|**2022-05-23**|**Collaborative Attention Memory Network for Video Object Segmentation**|Zhixing Huang et.al.|[2205.08075](http://arxiv.org/abs/2205.08075)|null|\n", "2205.03761": "|**2022-05-08**|**Recurrent Dynamic Embedding for Video Object Segmentation**|Mingxing Li et.al.|[2205.03761](http://arxiv.org/abs/2205.03761)|**[link](https://github.com/limingxing00/rde-vos-cvpr2022)**|\n", "2205.01197": "|**2022-05-02**|**Boosting Video Object Segmentation based on Scale Inconsistency**|Hengyi Wang et.al.|[2205.01197](http://arxiv.org/abs/2205.01197)|**[link](https://github.com/HengyiWang/SIRNet)**|\n", "2204.10846": "|**2022-04-22**|**Self-Supervised Video Object Segmentation via Cutout Prediction and Tagging**|Jyoti Kini et.al.|[2204.10846](http://arxiv.org/abs/2204.10846)|null|\n", "2204.06626": "|**2022-04-13**|**Adaptive Memory Management for Video Object Segmentation**|Ali Pourganjalikhan et.al.|[2204.06626](http://arxiv.org/abs/2204.06626)|**[link](https://github.com/alipga/AMM_VOS)**|\n", "2204.02791": "|**2024-01-16**|**Implicit Motion-Compensated Network for Unsupervised Video Object Segmentation**|Lin Xi et.al.|[2204.02791](http://arxiv.org/abs/2204.02791)|**[link](https://github.com/xilin1991/IMCNet)**|\n", "2203.15312": "|**2022-10-24**|**In-N-Out Generative Learning for Dense Unsupervised Video Segmentation**|Xiao Pan et.al.|[2203.15312](http://arxiv.org/abs/2203.15312)|**[link](https://github.com/pansanity666/INO_VOS)**|\n", "2203.14308": "|**2023-07-16**|**Temporal Transductive Inference for Few-Shot Video Object Segmentation**|Mennatullah Siam et.al.|[2203.14308](http://arxiv.org/abs/2203.14308)|**[link](https://github.com/msiam/tti_fsvos)**|\n", "2203.11442": "|**2024-05-28**|**Scalable Video Object Segmentation with Identification Mechanism**|Zongxin Yang et.al.|[2203.11442](http://arxiv.org/abs/2203.11442)|**[link](https://github.com/yoxu515/aot-benchmark)**|\n", "2203.11191": "|**2022-07-20**|**Robust Visual Tracking by Segmentation**|Matthieu Paul et.al.|[2203.11191](http://arxiv.org/abs/2203.11191)|**[link](https://github.com/visionml/pytracking)**|\n", "2203.09773": "|**2024-01-19**|**Local-Global Context Aware Transformer for Language-Guided Video Segmentation**|Chen Liang et.al.|[2203.09773](http://arxiv.org/abs/2203.09773)|**[link](https://github.com/leonnnop/locater)**|\n", "2203.04251": "|**2022-07-01**|**End-to-End Semi-Supervised Learning for Video Action Detection**|Akash Kumar et.al.|[2203.04251](http://arxiv.org/abs/2203.04251)|**[link](https://github.com/AKASH2907/End-to-End-Semi-Supervised-Learning-for-Video-Action-Detection)**|\n", "2203.01784": "|**2022-06-07**|**Revisiting Click-based Interactive Video Object Segmentation**|Stephane Vujasinovic et.al.|[2203.01784](http://arxiv.org/abs/2203.01784)|**[link](https://github.com/Vujas-Eteph/CiVOS)**|\n", "2202.07025": "|**2022-02-16**|**Box Supervised Video Segmentation Proposal Network**|Tanveer Hannan et.al.|[2202.07025](http://arxiv.org/abs/2202.07025)|**[link](https://github.com/tanveer81/boxvos)**|\n", "2201.10162": "|**2022-05-09**|**Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks**|Xin Jin et.al.|[2201.10162](http://arxiv.org/abs/2201.10162)|null|\n", "2201.08379": "|**2022-04-04**|**Learning Pixel Trajectories with Multiscale Contrastive Random Walks**|Zhangxing Bian et.al.|[2201.08379](http://arxiv.org/abs/2201.08379)|null|\n", "2201.00487": "|**2022-03-13**|**Language as Queries for Referring Video Object Segmentation**|Jiannan Wu et.al.|[2201.00487](http://arxiv.org/abs/2201.00487)|**[link](https://github.com/wjn922/referformer)**|\n", "2112.13983": "|**2021-12-28**|**Siamese Network with Interactive Transformer for Video Object Segmentation**|Meng Lan et.al.|[2112.13983](http://arxiv.org/abs/2112.13983)|**[link](https://github.com/lanmng/sitvos)**|\n", "2112.12402": "|**2021-12-23**|**Iteratively Selecting an Easy Reference Frame Makes Unsupervised Video Object Segmentation Easier**|Youngjo Lee et.al.|[2112.12402](http://arxiv.org/abs/2112.12402)|null|\n", "2112.11846": "|**2021-12-27**|**A Discriminative Single-Shot Segmentation Network for Visual Object Tracking**|Alan Luke\u017ei\u010d et.al.|[2112.11846](http://arxiv.org/abs/2112.11846)|null|\n", "2112.09131": "|**2022-07-15**|**HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images**|Ali Athar et.al.|[2112.09131](http://arxiv.org/abs/2112.09131)|**[link](https://github.com/Ali2500/HODOR)**|\n", "2112.02853": "|**2021-12-06**|**Reliable Propagation-Correction Modulation for Video Object Segmentation**|Xiaohao Xu et.al.|[2112.02853](http://arxiv.org/abs/2112.02853)|**[link](https://github.com/jerryx1110/rpcmvos)**|\n", "2111.14821": "|**2022-04-03**|**End-to-End Referring Video Object Segmentation with Multimodal Transformers**|Adam Botach et.al.|[2111.14821](http://arxiv.org/abs/2111.14821)|**[link](https://github.com/mttr2021/MTTR)**|\n", "2111.14646": "|**2021-11-29**|**MUNet: Motion Uncertainty-aware Semi-supervised Video Object Segmentation**|Jiadai Sun et.al.|[2111.14646](http://arxiv.org/abs/2111.14646)|null|\n", "2111.14160": "|**2021-11-28**|**Learning To Segment Dominant Object Motion From Watching Videos**|Sahir Shrestha et.al.|[2111.14160](http://arxiv.org/abs/2111.14160)|null|\n", "2111.10621": "|**2021-11-20**|**FlowVOS: Weakly-Supervised Visual Warping for Detail-Preserving and Temporally Consistent Single-Shot Video Object Segmentation**|Julia Gong et.al.|[2111.10621](http://arxiv.org/abs/2111.10621)|null|\n", "2111.10531": "|**2021-11-20**|**FAMINet: Learning Real-time Semi-supervised Video Object Segmentation with Steepest Optimized Optical Flow**|Ziyang Liu et.al.|[2111.10531](http://arxiv.org/abs/2111.10531)|**[link](https://github.com/liuziyang123/faminet)**|\n", "2111.07774": "|**2021-11-15**|**D^2Conv3D: Dynamic Dilated Convolutions for Object Segmentation in Videos**|Christian Schmidt et.al.|[2111.07774](http://arxiv.org/abs/2111.07774)|**[link](https://github.com/schmiddo/d2conv3d)**|\n", "2111.06265": "|**2021-11-11**|**Dense Unsupervised Learning for Video Segmentation**|Nikita Araslanov et.al.|[2111.06265](http://arxiv.org/abs/2111.06265)|**[link](https://github.com/visinf/dense-ulearn-vos)**|\n", "2111.02368": "|**2021-11-03**|**Video Salient Object Detection via Contrastive Features and Attention Modules**|Yi-Wen Chen et.al.|[2111.02368](http://arxiv.org/abs/2111.02368)|null|\n", "2111.01323": "|**2022-07-25**|**Exploring the Semi-supervised Video Object Segmentation Problem from a Cyclic Perspective**|Yuxi Li et.al.|[2111.01323](http://arxiv.org/abs/2111.01323)|**[link](https://github.com/lyxok1/STM-Training)**|\n", "2110.14773": "|**2021-10-27**|**SiamPolar: Semi-supervised Realtime Video Object Segmentation with Polar Representation**|Yaochen Li et.al.|[2110.14773](http://arxiv.org/abs/2110.14773)|null|\n", "2110.11284": "|**2023-05-16**|**Multi-Object Tracking and Segmentation with a Space-Time Memory Network**|Mehdi Miah et.al.|[2110.11284](http://arxiv.org/abs/2110.11284)|null|\n", "2110.01644": "|**2021-11-12**|**Pixel-Level Bijective Matching for Video Object Segmentation**|Suhwan Cho et.al.|[2110.01644](http://arxiv.org/abs/2110.01644)|**[link](https://github.com/suhwan-cho/bmvos)**|\n", "2109.11404": "|**2021-09-23**|**Hierarchical Memory Matching Network for Video Object Segmentation**|Hongje Seong et.al.|[2109.11404](http://arxiv.org/abs/2109.11404)|**[link](https://github.com/hongje/hmmn)**|\n", "2109.06474": "|**2022-07-06**|**Space Time Recurrent Memory Network**|Hung Nguyen et.al.|[2109.06474](http://arxiv.org/abs/2109.06474)|null|\n", "2108.11575": "|**2021-10-29**|**Shifted Chunk Transformer for Spatio-Temporal Representational Learning**|Xuefan Zha et.al.|[2108.11575](http://arxiv.org/abs/2108.11575)|null|\n", "2108.08482": "|**2021-08-19**|**VIL-100: A New Dataset and A Baseline Model for Video Instance Lane Detection**|Yujun Zhang et.al.|[2108.08482](http://arxiv.org/abs/2108.08482)|**[link](https://github.com/yujun0-0/mma-net)**|\n", "2108.05076": "|**2021-08-11**|**Multi-Source Fusion and Automatic Predictor Selection for Zero-Shot Video Object Segmentation**|Xiaoqi Zhao et.al.|[2108.05076](http://arxiv.org/abs/2108.05076)|**[link](https://github.com/xiaoqi-zhao-dlut/multi-source-aps-zvos)**|\n", "2108.03679": "|**2021-08-08**|**Joint Inductive and Transductive Learning for Video Object Segmentation**|Yunyao Mao et.al.|[2108.03679](http://arxiv.org/abs/2108.03679)|**[link](https://github.com/maoyunyao/joint)**|\n", "2108.03151": "|**2021-09-03**|**Full-Duplex Strategy for Video Object Segmentation**|Ge-Peng Ji et.al.|[2108.03151](http://arxiv.org/abs/2108.03151)|**[link](https://github.com/GewelsJI/FSNet)**|\n", "2107.12569": "|**2021-10-28**|**Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation**|Bo Miao et.al.|[2107.12569](http://arxiv.org/abs/2107.12569)|**[link](https://github.com/bo-miao/MAMP)**|\n", "2107.12192": "|**2022-04-06**|**Accelerating Video Object Segmentation with Compressed Video**|Kai Xu et.al.|[2107.12192](http://arxiv.org/abs/2107.12192)|**[link](https://github.com/kai422/covos)**|\n", "2107.07067": "|**2021-07-15**|**MeNToS: Tracklets Association with a Space-Time Memory Network**|Mehdi Miah et.al.|[2107.07067](http://arxiv.org/abs/2107.07067)|null|\n", "2107.04279": "|**2021-07-09**|**Fast Pixel-Matching for Video Object Segmentation**|Siyue Yu et.al.|[2107.04279](http://arxiv.org/abs/2107.04279)|null|\n", "2106.10452": "|**2021-06-19**|**MSN: Efficient Online Mask Selection Network for Video Instance Segmentation**|Vidit Goel et.al.|[2106.10452](http://arxiv.org/abs/2106.10452)|**[link](https://github.com/SHI-Labs/Mask-Selection-Networks)**|\n", "2106.05210": "|**2021-10-08**|**Rethinking Space-Time Networks with Improved Memory Coverage for Efficient Video Object Segmentation**|Ho Kei Cheng et.al.|[2106.05210](http://arxiv.org/abs/2106.05210)|**[link](https://github.com/hkchengrex/STCN)**|\n", "2106.04403": "|**2021-06-09**|**SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation**|Ioannis Kazakos et.al.|[2106.04403](http://arxiv.org/abs/2106.04403)|**[link](https://github.com/imatge-upc/synthref)**|\n", "2106.03348": "|**2021-12-24**|**ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias**|Yufei Xu et.al.|[2106.03348](http://arxiv.org/abs/2106.03348)|**[link](https://github.com/Annbless/ViTAE)**|\n", "2106.02638": "|**2021-10-30**|**Associating Objects with Transformers for Video Object Segmentation**|Zongxin Yang et.al.|[2106.02638](http://arxiv.org/abs/2106.02638)|**[link](https://github.com/z-x-yang/AOT)**|\n", "2106.01061": "|**2024-01-19**|**Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation**|Chen Liang et.al.|[2106.01061](http://arxiv.org/abs/2106.01061)|null|\n", "2106.00588": "|**2021-09-18**|**TransVOS: Video Object Segmentation with Transformers**|Jianbiao Mei et.al.|[2106.00588](http://arxiv.org/abs/2106.00588)|**[link](https://github.com/sallymmx/TransVOS)**|\n", "2105.14584": "|**2021-05-30**|**Polygonal Point Set Tracking**|Gunhee Nam et.al.|[2105.14584](http://arxiv.org/abs/2105.14584)|null|\n", "2105.11427": "|**2021-07-29**|**Attention-guided Temporally Coherent Video Object Matting**|Yunke Zhang et.al.|[2105.11427](http://arxiv.org/abs/2105.11427)|**[link](https://github.com/yunkezhang/TCVOM)**|\n", "2105.10201": "|**2021-05-24**|**DAVOS: Semi-Supervised Video Object Segmentation via Adversarial Domain Adaptation**|Jinshuo Zhang et.al.|[2105.10201](http://arxiv.org/abs/2105.10201)|null|\n", "2105.05838": "|**2021-12-29**|**Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for Video Correspondence Learning**|Yansong Tang et.al.|[2105.05838](http://arxiv.org/abs/2105.05838)|**[link](https://github.com/steve-tod/stfc3)**|\n", "2104.10386": "|**2021-04-21**|**Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps**|Yuk Heo et.al.|[2104.10386](http://arxiv.org/abs/2104.10386)|**[link](https://github.com/yuk6heo/GIS-RAmap)**|\n", "2104.07658": "|**2021-08-11**|**Self-supervised Video Object Segmentation by Motion Grouping**|Charig Yang et.al.|[2104.07658](http://arxiv.org/abs/2104.07658)|null|\n", "2104.04782": "|**2021-04-10**|**Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation**|Tianfei Zhou et.al.|[2104.04782](http://arxiv.org/abs/2104.04782)|null|\n", "2104.04329": "|**2021-04-09**|**Learning Position and Target Consistency for Memory-based Video Object Segmentation**|Li Hu et.al.|[2104.04329](http://arxiv.org/abs/2104.04329)|null|\n", "2103.17263": "|**2021-10-14**|**Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective**|Jiarui Xu et.al.|[2103.17263](http://arxiv.org/abs/2103.17263)|**[link](https://github.com/xvjiarui/VFS)**|\n", "2103.12934": "|**2021-04-27**|**Efficient Regional Memory Network for Video Object Segmentation**|Haozhe Xie et.al.|[2103.12934](http://arxiv.org/abs/2103.12934)|**[link](https://github.com/hzxie/RMNet)**|\n", "2103.10391": "|**2021-06-17**|**Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild**|Zhaoyuan Yin et.al.|[2103.10391](http://arxiv.org/abs/2103.10391)|**[link](https://github.com/svip-lab/IVOS-W)**|\n", "2103.07941": "|**2021-03-21**|**Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion**|Ho Kei Cheng et.al.|[2103.07941](http://arxiv.org/abs/2103.07941)|**[link](https://github.com/hkchengrex/MiVOS)**|\n", "2103.06533": "|**2021-03-11**|**Triple-cooperative Video Shadow Detection**|Zhihao Chen et.al.|[2103.06533](http://arxiv.org/abs/2103.06533)|**[link](https://github.com/eraserNut/ViSha)**|\n", "2103.03821": "|**2021-04-21**|**Fast Interactive Video Object Segmentation with Graph Neural Networks**|Viktor Varga et.al.|[2103.03821](http://arxiv.org/abs/2103.03821)|**[link](https://github.com/vvarga90/gnn_annot)**|\n", "2102.04604": "|**2021-04-21**|**SwiftNet: Real-time Video Object Segmentation**|Haochen Wang et.al.|[2102.04604](http://arxiv.org/abs/2102.04604)|**[link](https://github.com/haochenheheda/SwiftNet)**|\n", "2102.02696": "|**2022-02-03**|**Active Boundary Loss for Semantic Segmentation**|Chi Wang et.al.|[2102.02696](http://arxiv.org/abs/2102.02696)|**[link](https://github.com/wangchi95/active-boundary-loss)**|\n", "2101.08833": "|**2021-03-29**|**SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation**|Brendan Duke et.al.|[2101.08833](http://arxiv.org/abs/2101.08833)|**[link](https://github.com/dukebw/SSTVOS)**|\n", "2101.06545": "|**2021-01-16**|**VideoClick: Video Object Segmentation with a Single Click**|Namdar Homayounfar et.al.|[2101.06545](http://arxiv.org/abs/2101.06545)|null|\n", "2101.02196": "|**2021-01-06**|**Generating Masks from Boxes by Mining Spatio-Temporal Consistencies in Videos**|Bin Zhao et.al.|[2101.02196](http://arxiv.org/abs/2101.02196)|**[link](https://github.com/visionml/pytracking)**|\n", "2012.11655": "|**2021-05-16**|**Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised Video Object Segmentation**|Hyojin Park et.al.|[2012.11655](http://arxiv.org/abs/2012.11655)|**[link](https://github.com/HYOJINPARK/Reuse_VOS)**|\n", "2012.05499": "|**2020-12-10**|**Spatiotemporal Graph Neural Network based Mask Reconstruction for Video Object Segmentation**|Daizong Liu et.al.|[2012.05499](http://arxiv.org/abs/2012.05499)|null|\n", "2012.05057": "|**2020-12-09**|**Contrastive Transformation for Self-supervised Correspondence Learning**|Ning Wang et.al.|[2012.05057](http://arxiv.org/abs/2012.05057)|**[link](https://github.com/594422814/ContrastCorr)**|\n", "2012.02534": "|**2020-12-04**|**F2Net: Learning to Focus on the Foreground for Unsupervised Video Object Segmentation**|Daizong Liu et.al.|[2012.02534](http://arxiv.org/abs/2012.02534)|null|\n", "2012.01866": "|**2020-12-03**|**Make One-Shot Video Object Segmentation Efficient Again**|Tim Meinhardt et.al.|[2012.01866](http://arxiv.org/abs/2012.01866)|**[link](https://github.com/dvl-tum/e-osvos)**|\n", "2011.04445": "|**2021-04-04**|**TTVOS: Lightweight Video Object Segmentation with Adaptive Template Attention Module and Temporal Consistency Loss**|Hyojin Park et.al.|[2011.04445](http://arxiv.org/abs/2011.04445)|**[link](https://github.com/HYOJINPARK/TTVOS)**|\n", "2011.01142": "|**2020-11-02**|**Reducing the Annotation Effort for Video Object Segmentation Datasets**|Paul Voigtlaender et.al.|[2011.01142](http://arxiv.org/abs/2011.01142)|null|\n", "2010.12176": "|**2020-10-23**|**Delving into the Cyclic Mechanism in Semi-supervised Video Object Segmentation**|Yuxi Li et.al.|[2010.12176](http://arxiv.org/abs/2010.12176)|**[link](https://github.com/lyxok1/STM-Training)**|\n", "2010.11649": "|**2020-10-22**|**Learning to Sort Image Sequences via Accumulated Temporal Differences**|Gagan Kanojia et.al.|[2010.11649](http://arxiv.org/abs/2010.11649)|null|\n", "2010.07958": "|**2020-10-15**|**Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement**|Yongqing Liang et.al.|[2010.07958](http://arxiv.org/abs/2010.07958)|**[link](https://github.com/xmlyqing00/AFB-URR)**|\n", "2010.06349": "|**2021-05-16**|**Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration**|Zongxin Yang et.al.|[2010.06349](http://arxiv.org/abs/2010.06349)|**[link](https://github.com/z-x-yang/CFBI)**|\n", "2010.05069": "|**2020-11-07**|**Hybrid-S2S: Video Object Segmentation with Recurrent Networks and Correspondence Matching**|Fatemeh Azimi et.al.|[2010.05069](http://arxiv.org/abs/2010.05069)|**[link](https://github.com/fatemehazimi990/HS2S)**|\n", "2010.00263": "|**2020-10-01**|**RefVOS: A Closer Look at Referring Expressions for Video Object Segmentation**|Miriam Bellver et.al.|[2010.00263](http://arxiv.org/abs/2010.00263)|**[link](https://github.com/miriambellver/refvos)**|\n", "2009.09669": "|**2021-04-06**|**Learning Spatio-Appearance Memory Network for High-Performance Visual Tracking**|Fei Xie et.al.|[2009.09669](http://arxiv.org/abs/2009.09669)|**[link](https://github.com/phiphiphi31/DMB)**|\n", "2009.08855": "|**2020-09-18**|**PMVOS: Pixel-Level Matching-Based Video Object Segmentation**|Suhwan Cho et.al.|[2009.08855](http://arxiv.org/abs/2009.08855)|null|\n", "2009.00771": "|**2020-09-02**|**LSMVOS: Long-Short-Term Similarity Matching for Video Object**|Zhang Xuerui et.al.|[2009.00771](http://arxiv.org/abs/2009.00771)|null|\n", "2008.09721": "|**2020-08-22**|**ScribbleBox: Interactive Annotation Framework for Video Object Segmentation**|Bowen Chen et.al.|[2008.09721](http://arxiv.org/abs/2008.09721)|null|\n", "2008.07012": "|**2021-04-03**|**DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping**|Yanchao Yang et.al.|[2008.07012](http://arxiv.org/abs/2008.07012)|null|\n", "2008.06698": "|**2020-08-15**|**Curriculum Learning for Recurrent Video Object Segmentation**|Maria Gonzalez-i-Calabuig et.al.|[2008.06698](http://arxiv.org/abs/2008.06698)|**[link](https://github.com/imatge-upc/rvos-mots)**|\n", "2008.01270": "|**2020-08-04**|**Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation**|Mingmin Zhen et.al.|[2008.01270](http://arxiv.org/abs/2008.01270)|null|\n", "2008.00637": "|**2020-08-03**|**Self-supervised Object Tracking with Cycle-consistent Siamese Networks**|Weihao Yuan et.al.|[2008.00637](http://arxiv.org/abs/2008.00637)|**[link](https://github.com/weihaosky/CycleSiam)**|\n", "2007.08270": "|**2020-07-16**|**Kernelized Memory Network for Video Object Segmentation**|Hongje Seong et.al.|[2007.08270](http://arxiv.org/abs/2007.08270)|null|\n", "2007.08139": "|**2020-07-16**|**Interactive Video Object Segmentation Using Global and Local Transfer Modules**|Yuk Heo et.al.|[2007.08139](http://arxiv.org/abs/2007.08139)|**[link](https://github.com/yuk6heo/IVOS-ATNet)**|\n", "2007.07020": "|**2020-12-09**|**Video Object Segmentation with Episodic Graph Memory Networks**|Xiankai Lu et.al.|[2007.07020](http://arxiv.org/abs/2007.07020)|**[link](https://github.com/carrierlxk/GraphMemVOS)**|\n", "2007.05687": "|**2020-07-11**|**Fast Video Object Segmentation With Temporal Aggregation Network and Dynamic Template Matching**|Xuhua Huang et.al.|[2007.05687](http://arxiv.org/abs/2007.05687)|null|\n", "2007.05676": "|**2020-12-18**|**Learning Object Depth from Camera Motion and Video Object Segmentation**|Brent A. Griffin et.al.|[2007.05676](http://arxiv.org/abs/2007.05676)|**[link](https://github.com/griffbr/ODMS)**|\n", "2007.01120": "|**2020-07-01**|**Motion Prediction in Visual Object Tracking**|Jianren Wang et.al.|[2007.01120](http://arxiv.org/abs/2007.01120)|null|\n", "2006.12480": "|**2020-06-22**|**Self-supervised Video Object Segmentation**|Fangrui Zhu et.al.|[2006.12480](http://arxiv.org/abs/2006.12480)|null|\n", "2005.13039": "|**2020-08-14**|**ALBA : Reinforcement Learning for Video Object Segmentation**|Shreyank N Gowda et.al.|[2005.13039](http://arxiv.org/abs/2005.13039)|null|\n", "2004.12170": "|**2020-04-25**|**Revisiting Sequence-to-Sequence Video Object Segmentation with Multi-Task Loss and Skip-Memory**|Fatemeh Azimi et.al.|[2004.12170](http://arxiv.org/abs/2004.12170)|**[link](https://github.com/fatemehazimi990/RS2S)**|\n", "2004.07538": "|**2020-04-16**|**Fast Template Matching and Update for Video Object Tracking and Segmentation**|Mingjie Sun et.al.|[2004.07538](http://arxiv.org/abs/2004.07538)|**[link](https://github.com/insomnia94/FTMU)**|\n", "2004.07193": "|**2020-04-16**|**A Transductive Approach for Video Object Segmentation**|Yizhuo Zhang et.al.|[2004.07193](http://arxiv.org/abs/2004.07193)|**[link](https://github.com/microsoft/transductive-vos.pytorch)**|\n", "2003.13246": "|**2020-03-30**|**Memory Aggregation Networks for Efficient Interactive Video Object Segmentation**|Jiaxu Miao et.al.|[2003.13246](http://arxiv.org/abs/2003.13246)|null|\n", "2003.13141": "|**2020-03-29**|**Learning a Weakly-Supervised Video Actor-Action Segmentation Model with a Wise Selection**|Jie Chen et.al.|[2003.13141](http://arxiv.org/abs/2003.13141)|null|\n", "2003.11540": "|**2020-05-01**|**Learning What to Learn for Video Object Segmentation**|Goutam Bhat et.al.|[2003.11540](http://arxiv.org/abs/2003.11540)|**[link](https://github.com/visionml/pytracking)**|\n", "2003.08333": "|**2020-07-23**|**Collaborative Video Object Segmentation by Foreground-Background Integration**|Zongxin Yang et.al.|[2003.08333](http://arxiv.org/abs/2003.08333)|**[link](https://github.com/z-x-yang/CFBI)**|\n", "2003.06125": "|**2020-03-13**|**Dual Temporal Memory Network for Efficient Video Object Segmentation**|Kaihua Zhang et.al.|[2003.06125](http://arxiv.org/abs/2003.06125)|null|\n", "2003.05020": "|**2020-03-10**|**Learning Video Object Segmentation from Unlabeled Videos**|Xiankai Lu et.al.|[2003.05020](http://arxiv.org/abs/2003.05020)|**[link](https://github.com/carrierlxk/MuG)**|\n", "2003.04253": "|**2020-07-09**|**Motion-Attentive Transition for Zero-Shot Video Object Segmentation**|Tianfei Zhou et.al.|[2003.04253](http://arxiv.org/abs/2003.04253)|**[link](https://github.com/tfzhou/MATNet)**|\n", "2003.00482": "|**2020-03-01**|**State-Aware Tracker for Real-Time Video Object Segmentation**|Xi Chen et.al.|[2003.00482](http://arxiv.org/abs/2003.00482)|**[link](https://github.com/MegviiDetection/video_analyst)**|\n", "2003.00908": "|**2020-03-31**|**Learning Fast and Robust Target Models for Video Object Segmentation**|Andreas Robinson et.al.|[2003.00908](http://arxiv.org/abs/2003.00908)|**[link](https://github.com/andr345/frtm-vos)**|\n", "2002.07793": "|**2020-02-26**|**MAST: A Memory-Augmented Self-supervised Tracker**|Zihang Lai et.al.|[2002.07793](http://arxiv.org/abs/2002.07793)|**[link](https://github.com/zlai0/MAST)**|\n", "2002.06736": "|**2020-02-17**|**Directional Deep Embedding and Appearance Learning for Fast Video Object Segmentation**|Yingjie Yin et.al.|[2002.06736](http://arxiv.org/abs/2002.06736)|**[link](https://github.com/YingjieYin/Directional-Deep-Embedding-and-Appearance-Learning-for-Fast-Video-Object-Segmentation)**|\n", "2002.03651": "|**2020-06-02**|**CRVOS: Clue Refining Network for Video Object Segmentation**|Suhwan Cho et.al.|[2002.03651](http://arxiv.org/abs/2002.03651)|null|\n", "2001.11243": "|**2020-07-18**|**Fast Video Object Segmentation using the Global Context Module**|Yu Li et.al.|[2001.11243](http://arxiv.org/abs/2001.11243)|**[link](https://github.com/cmsflash/global-context-module)**|\n", "2001.06810": "|**2020-01-19**|**See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks**|Xiankai Lu et.al.|[2001.06810](http://arxiv.org/abs/2001.06810)|**[link](https://github.com/carrierlxk/COSNet)**|\n", "2001.06807": "|**2020-01-19**|**Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks**|Wenguan Wang et.al.|[2001.06807](http://arxiv.org/abs/2001.06807)|**[link](https://github.com/carrierlxk/AGNN)**|\n", "2001.05425": "|**2020-01-15**|**UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking**|Jonathon Luiten et.al.|[2001.05425](http://arxiv.org/abs/2001.05425)|null|\n", "1912.08936": "|**2019-12-18**|**One-Shot Weakly Supervised Video Object Segmentation**|Mennatullah Siam et.al.|[1912.08936](http://arxiv.org/abs/1912.08936)|null|\n", "1912.01373": "|**2019-12-03**|**Automatic Video Object Segmentation via Motion-Appearance-Stream Fusion and Instance-aware Segmentation**|Sungkwon Choo et.al.|[1912.01373](http://arxiv.org/abs/1912.01373)|null|\n", "1911.08862": "|**2020-04-14**|**D3S -- A Discriminative Single Shot Segmentation Tracker**|Alan Luke\u017ei\u010d et.al.|[1911.08862](http://arxiv.org/abs/1911.08862)|**[link](https://github.com/alanlukezic/d3s)**|\n", "1910.11844": "|**2019-10-25**|**Learning to Track Any Object**|Achal Dave et.al.|[1910.11844](http://arxiv.org/abs/1910.11844)|null|\n", "1910.10895": "|**2019-10-24**|**Anchor Diffusion for Unsupervised Video Object Segmentation**|Zhao Yang et.al.|[1910.10895](http://arxiv.org/abs/1910.10895)|**[link](https://github.com/yz93/anchor-diff-VOS)**|\n", "1910.02258": "|**2020-08-14**|**Object Segmentation Tracking from Generic Video Cues**|Amirhossein Kardoost et.al.|[1910.02258](http://arxiv.org/abs/1910.02258)|null|\n", "1910.00132": "|**2019-09-30**|**CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing**|Kevin Duarte et.al.|[1910.00132](http://arxiv.org/abs/1910.00132)|**[link](https://github.com/KevinDuarte/CapsuleVOS)**|\n", "1910.00032": "|**2019-09-30**|**LIP: Learning Instance Propagation for Video Object Segmentation**|Ye Lyu et.al.|[1910.00032](http://arxiv.org/abs/1910.00032)|null|\n", "1909.13583": "|**2019-09-30**|**Towards Good Practices for Video Object Segmentation**|Dongdong Yu et.al.|[1909.13583](http://arxiv.org/abs/1909.13583)|null|\n", "1909.13247": "|**2019-10-10**|**RPM-Net: Robust Pixel-Level Matching Networks for Self-Supervised Video Object Segmentation**|Youngeun Kim et.al.|[1909.13247](http://arxiv.org/abs/1909.13247)|null|\n", "1909.13046": "|**2019-09-28**|**Meta Learning with Differentiable Closed-form Solver for Fast Video Object Segmentation**|Yu Liu et.al.|[1909.13046](http://arxiv.org/abs/1909.13046)|null|\n", "1909.12482": "|**2019-09-27**|**Adaptive ROI Generation for Video Object Segmentation Using Reinforcement Learning**|Mingjie Sun et.al.|[1909.12482](http://arxiv.org/abs/1909.12482)|null|\n", "1909.12471": "|**2019-09-27**|**DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation**|Xiaohui Zeng et.al.|[1909.12471](http://arxiv.org/abs/1909.12471)|**[link](https://github.com/ZENGXH/DMM_Net)**|\n", "1908.10717": "|**2019-08-28**|**Fast Video Object Segmentation via Mask Transfer Network**|Tao Zhuo et.al.|[1908.10717](http://arxiv.org/abs/1908.10717)|null|\n", "1908.06692": "|**2019-08-20**|**In defense of OSVOS**|Yu Liu et.al.|[1908.06692](http://arxiv.org/abs/1908.06692)|null|\n", "1908.06647": "|**2019-09-08**|**RANet: Ranking Attention Network for Fast Video Object Segmentation**|Ziqin Wang et.al.|[1908.06647](http://arxiv.org/abs/1908.06647)|**[link](https://github.com/Storife/RANet)**|\n", "1907.12769": "|**2019-07-30**|**An Empirical Study of Propagation-based Methods for Video Object Segmentation**|Hengkai Guo et.al.|[1907.12769](http://arxiv.org/abs/1907.12769)|null|\n", "1907.08895": "|**2019-07-21**|**An Efficient 3D CNN for Action/Object Segmentation in Video**|Rui Hou et.al.|[1907.08895](http://arxiv.org/abs/1907.08895)|null|\n", "1907.04409": "|**2020-02-22**|**Global Optimality Guarantees for Nonconvex Unsupervised Video Segmentation**|Brendon G. Anderson et.al.|[1907.04409](http://arxiv.org/abs/1907.04409)|null|\n", "1907.03326": "|**2019-08-03**|**Spacetime Graph Optimization for Video Object Segmentation**|Emanuela Haller et.al.|[1907.03326](http://arxiv.org/abs/1907.03326)|null|\n", "1907.01203": "|**2019-07-04**|**Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video Object Segmentation**|Qiang Zhou et.al.|[1907.01203](http://arxiv.org/abs/1907.01203)|**[link](https://github.com/sydney0zq/PTSNet)**|\n", "1906.07851": "|**2019-07-26**|**Key Instance Selection for Unsupervised Video Object Segmentation**|Donghyeon Cho et.al.|[1906.07851](http://arxiv.org/abs/1906.07851)|null|\n", "1905.10064": "|**2019-07-02**|**OVSNet : Towards One-Pass Real-Time Video Object Segmentation**|Peng Sun et.al.|[1905.10064](http://arxiv.org/abs/1905.10064)|null|\n", "1905.07826": "|**2019-05-19**|**U-Net Based Multi-instance Video Object Segmentation**|Heguang Liu et.al.|[1905.07826](http://arxiv.org/abs/1905.07826)|null|\n", "1905.00737": "|**2019-05-02**|**The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation**|Sergi Caelles et.al.|[1905.00737](http://arxiv.org/abs/1905.00737)|null|\n", "1904.11256": "|**2019-04-25**|**On guiding video object segmentation**|Diego Ortego et.al.|[1904.11256](http://arxiv.org/abs/1904.11256)|null|\n", "1904.09791": "|**2019-05-02**|**Fast User-Guided Video Object Segmentation by Interaction-and-Propagation Networks**|Seoung Wug Oh et.al.|[1904.09791](http://arxiv.org/abs/1904.09791)|null|\n", "1904.09172": "|**2019-04-26**|**Video Object Segmentation and Tracking: A Survey**|Rui Yao et.al.|[1904.09172](http://arxiv.org/abs/1904.09172)|null|\n", "1904.08630": "|**2019-04-18**|**Discriminative Online Learning for Fast Video Object Segmentation**|Andreas Robinson et.al.|[1904.08630](http://arxiv.org/abs/1904.08630)|null|\n", "1904.08141": "|**2019-04-17**|**MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation**|Shuangjie Xu et.al.|[1904.08141](http://arxiv.org/abs/1904.08141)|**[link](https://github.com/shuangjiexu/MHP-VOS)**|\n", "1904.04552": "|**2019-12-29**|**BoLTVOS: Box-Level Tracking for Video Object Segmentation**|Paul Voigtlaender et.al.|[1904.04552](http://arxiv.org/abs/1904.04552)|null|\n", "1904.02363": "|**2019-04-04**|**Spatiotemporal CNN for Video Object Segmentation**|Kai Xu et.al.|[1904.02363](http://arxiv.org/abs/1904.02363)|**[link](https://github.com/longyin880815/STCNN)**|\n", "1904.01784": "|**2019-08-20**|**Patchwork: A Patch-wise Attention Network for Efficient Object Detection and Segmentation in Video Streams**|Yuning Chai et.al.|[1904.01784](http://arxiv.org/abs/1904.01784)|null|\n", "1904.01693": "|**2019-04-02**|**Multigrid Predictive Filter Flow for Unsupervised Learning on Videos**|Shu Kong et.al.|[1904.01693](http://arxiv.org/abs/1904.01693)|null|\n", "1904.00607": "|**2019-08-12**|**Video Object Segmentation using Space-Time Memory Networks**|Seoung Wug Oh et.al.|[1904.00607](http://arxiv.org/abs/1904.00607)|null|\n", "1903.12161": "|**2019-03-28**|**Fast video object segmentation with Spatio-Temporal GANs**|Sergi Caelles et.al.|[1903.12161](http://arxiv.org/abs/1903.12161)|null|\n", "1903.11779": "|**2020-11-24**|**BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames**|Brent A. Griffin et.al.|[1903.11779](http://arxiv.org/abs/1903.11779)|**[link](https://github.com/griffbr/BubbleNets)**|\n", "1904.00758": "|**2019-03-21**|**Value of Temporal Dynamics Information in Driving Scene Segmentation**|Li Ding et.al.|[1904.00758](http://arxiv.org/abs/1904.00758)|null|\n", "1903.08336": "|**2020-01-10**|**Video Object Segmentation-based Visual Servo Control and Object Depth Estimation on a Mobile Robot**|Brent A. Griffin et.al.|[1903.08336](http://arxiv.org/abs/1903.08336)|**[link](https://github.com/griffbr/VOSVS)**|\n", "1903.07593": "|**2019-04-02**|**Learning Correspondence from the Cycle-Consistency of Time**|Xiaolong Wang et.al.|[1903.07593](http://arxiv.org/abs/1903.07593)|null|\n", "1903.05612": "|**2019-05-21**|**RVOS: End-to-End Recurrent Network for Video Object Segmentation**|Carles Ventura et.al.|[1903.05612](http://arxiv.org/abs/1903.05612)|**[link](https://github.com/imatge-upc/rvos)**|\n", "1902.09513": "|**2019-04-08**|**FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation**|Paul Voigtlaender et.al.|[1902.09513](http://arxiv.org/abs/1902.09513)|**[link](https://github.com/tensorflow/models)**|\n", "1812.07712": "|**2018-12-19**|**Unsupervised Video Object Segmentation with Distractor-Aware Online Adaptation**|Ye Wang et.al.|[1812.07712](http://arxiv.org/abs/1812.07712)|null|\n", "1812.05206": "|**2018-12-13**|**Design Pseudo Ground Truth with Motion Cue for Unsupervised Video Object Segmentation**|Ye Wang et.al.|[1812.05206](http://arxiv.org/abs/1812.05206)|null|\n", "1812.05050": "|**2019-05-05**|**Fast Online Object Tracking and Segmentation: A Unifying Approach**|Qiang Wang et.al.|[1812.05050](http://arxiv.org/abs/1812.05050)|null|\n", "1812.02699": "|**2020-01-27**|**Online Model Distillation for Efficient Video Inference**|Ravi Teja Mullapudi et.al.|[1812.02699](http://arxiv.org/abs/1812.02699)|null|\n", "1812.01397": "|**2020-08-17**|**Meta Learning Deep Visual Words for Fast Video Object Segmentation**|Harkirat Singh Behl et.al.|[1812.01397](http://arxiv.org/abs/1812.01397)|**[link](https://github.com/harkiratbehl/MetaVOS)**|\n", "1811.11611": "|**2018-12-07**|**A Generative Appearance Model for End-to-end Video Object Segmentation**|Joakim Johnander et.al.|[1811.11611](http://arxiv.org/abs/1811.11611)|null|\n", "1811.07958": "|**2018-11-30**|**Tukey-Inspired Video Object Segmentation**|Brent A. Griffin et.al.|[1811.07958](http://arxiv.org/abs/1811.07958)|**[link](https://github.com/griffbr/TIS)**|\n", "1811.01526": "|**2018-11-05**|**Unsupervised RGBD Video Object Segmentation Using GANs**|Maryam Sultana et.al.|[1811.01526](http://arxiv.org/abs/1811.01526)|null|\n", "1810.10289": "|**2018-10-24**|**Mask Propagation Network for Video Object Segmentation**|Jia Sun et.al.|[1810.10289](http://arxiv.org/abs/1810.10289)|null|\n", "1810.07733": "|**2019-03-12**|**Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting**|Mennatullah Siam et.al.|[1810.07733](http://arxiv.org/abs/1810.07733)|**[link](https://github.com/MSiam/motion_adaptation)**|\n", "1810.03783": "|**2019-08-06**|**Unsupervised Online Video Object Segmentation with Motion Property Understanding**|Tao Zhuo et.al.|[1810.03783](http://arxiv.org/abs/1810.03783)|**[link](https://github.com/VisionTao/UOVOS)**|\n", "1809.10260": "|**2018-09-26**|**A Coarse-To-Fine Framework For Video Object Segmentation**|Chi Zhang et.al.|[1809.10260](http://arxiv.org/abs/1809.10260)|null|\n", "1809.03327": "|**2018-09-06**|**YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark**|Ning Xu et.al.|[1809.03327](http://arxiv.org/abs/1809.03327)|null|\n", "1809.01125": "|**2018-09-04**|**Unsupervised Video Object Segmentation using Motion Saliency-Guided Spatio-Temporal Propagation**|Yuan-Ting Hu et.al.|[1809.01125](http://arxiv.org/abs/1809.01125)|null|\n", "1809.01123": "|**2018-09-04**|**VideoMatch: Matching based Video Object Segmentation**|Yuan-Ting Hu et.al.|[1809.01123](http://arxiv.org/abs/1809.01123)|null|\n", "1809.00461": "|**2018-09-03**|**YouTube-VOS: Sequence-to-Sequence Video Object Segmentation**|Ning Xu et.al.|[1809.00461](http://arxiv.org/abs/1809.00461)|null|\n", "1808.04551": "|**2018-08-14**|**Moving Object Segmentation in Jittery Videos by Stabilizing Trajectories Modeled in Kendall's Shape Space**|Geethu Miriam Jacob et.al.|[1808.04551](http://arxiv.org/abs/1808.04551)|null|\n", "1808.00661": "|**2018-08-10**|**Adaptive Temporal Encoding Network for Video Instance-level Human Parsing**|Qixian Zhou et.al.|[1808.00661](http://arxiv.org/abs/1808.00661)|**[link](https://github.com/HCPLab-SYSU/ATEN)**|\n", "1807.09190": "|**2018-11-03**|**PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation**|Jonathon Luiten et.al.|[1807.09190](http://arxiv.org/abs/1807.09190)|null|\n", "1806.05510": "|**2018-06-18**|**ReConvNet: Video Object Segmentation with Spatio-Temporal Features Modulation**|Francesco Lattari et.al.|[1806.05510](http://arxiv.org/abs/1806.05510)|null|\n", "1806.02323": "|**2018-06-06**|**Fast and Accurate Online Video Object Segmentation via Tracking Parts**|Jingchun Cheng et.al.|[1806.02323](http://arxiv.org/abs/1806.02323)|**[link](https://github.com/JingchunCheng/FAVOS)**|\n", "1805.07780": "|**2018-05-20**|**Unsupervised Video Object Segmentation for Deep Reinforcement Learning**|Vik Goel et.al.|[1805.07780](http://arxiv.org/abs/1805.07780)|null|\n", "1804.03131": "|**2018-04-09**|**Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning**|Yuhua Chen et.al.|[1804.03131](http://arxiv.org/abs/1804.03131)|null|\n", "1803.11187": "|**2018-03-29**|**MaskRNN: Instance Level Video Object Segmentation**|Yuan-Ting Hu et.al.|[1803.11187](http://arxiv.org/abs/1803.11187)|null|\n", "1803.09453": "|**2018-03-26**|**CNN in MRF: Video Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRF**|Linchao Bao et.al.|[1803.09453](http://arxiv.org/abs/1803.09453)|null|\n", "1803.09092": "|**2019-09-17**|**Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos**|C. Spampinato et.al.|[1803.09092](http://arxiv.org/abs/1803.09092)|null|\n", "1803.08006": "|**2019-02-05**|**Video Object Segmentation with Language Referring Expressions**|Anna Khoreva et.al.|[1803.08006](http://arxiv.org/abs/1803.08006)|null|\n", "1803.04242": "|**2018-03-14**|**Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation**|Xiaoxiao Li et.al.|[1803.04242](http://arxiv.org/abs/1803.04242)|null|\n", "1803.00557": "|**2018-03-27**|**The 2018 DAVIS Challenge on Video Object Segmentation**|Sergi Caelles et.al.|[1803.00557](http://arxiv.org/abs/1803.00557)|null|\n", "1802.01218": "|**2018-02-04**|**Efficient Video Object Segmentation via Network Modulation**|Linjie Yang et.al.|[1802.01218](http://arxiv.org/abs/1802.01218)|**[link](https://github.com/linjieyangsc/video_seg)**|\n", "1801.00908": "|**2018-02-27**|**Instance Embedding Transfer to Unsupervised Video Object Segmentation**|Siyang Li et.al.|[1801.00908](http://arxiv.org/abs/1801.00908)|null|\n", "1801.00269": "|**2017-12-31**|**Interactive Video Object Segmentation in the Wild**|Arnaud Benard et.al.|[1801.00269](http://arxiv.org/abs/1801.00269)|null|\n", "1712.01111": "|**2017-11-30**|**An End-to-end 3D Convolutional Neural Network for Action Detection and Segmentation in Videos**|Rui Hou et.al.|[1712.01111](http://arxiv.org/abs/1712.01111)|null|\n", "1711.09081": "|**2018-03-27**|**Deep Extreme Cut: From Extreme Points to Object Segmentation**|Kevis-Kokitsi Maninis et.al.|[1711.09081](http://arxiv.org/abs/1711.09081)|**[link](https://github.com/scaelles/DEXTR-PyTorch)**|\n", "1709.06750": "|**2017-09-20**|**SegFlow: Joint Learning for Video Object Segmentation and Optical Flow**|Jingchun Cheng et.al.|[1709.06750](http://arxiv.org/abs/1709.06750)|**[link](https://github.com/JingchunCheng/SegFlow)**|\n", "1709.06031": "|**2018-05-16**|**Video Object Segmentation Without Temporal Information**|Kevis-Kokitsi Maninis et.al.|[1709.06031](http://arxiv.org/abs/1709.06031)|null|\n", "1708.05137": "|**2017-08-17**|**Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks**|Jae Shin Yoon et.al.|[1708.05137](http://arxiv.org/abs/1708.05137)|null|\n", "1708.01447": "|**2018-06-18**|**Video Salient Object Detection Using Spatiotemporal Deep Features**|Trung-Nghia Le et.al.|[1708.01447](http://arxiv.org/abs/1708.01447)|null|\n", "1708.00197": "|**2017-08-01**|**Video Object Segmentation with Re-identification**|Xiaoxiao Li et.al.|[1708.00197](http://arxiv.org/abs/1708.00197)|null|\n", "1707.06545": "|**2017-07-20**|**Video Object Segmentation using Tracked Object Proposals**|Gilad Sharir et.al.|[1707.06545](http://arxiv.org/abs/1707.06545)|null|\n", "1706.09544": "|**2017-06-29**|**Flow-free Video Object Segmentation**|Aditya Vora et.al.|[1706.09544](http://arxiv.org/abs/1706.09544)|null|\n", "1706.09364": "|**2017-08-01**|**Online Adaptation of Convolutional Neural Networks for Video Object Segmentation**|Paul Voigtlaender et.al.|[1706.09364](http://arxiv.org/abs/1706.09364)|null|\n", "1705.05020": "|**2018-04-28**|**Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs**|Emanuel Laude et.al.|[1705.05020](http://arxiv.org/abs/1705.05020)|null|\n", "1704.05737": "|**2017-07-12**|**Learning Video Object Segmentation with Visual Memory**|Pavel Tokmakov et.al.|[1704.05737](http://arxiv.org/abs/1704.05737)|null|\n", "1704.05165": "|**2017-04-18**|**Video Object Segmentation using Supervoxel-Based Gerrymandering**|Brent A. Griffin et.al.|[1704.05165](http://arxiv.org/abs/1704.05165)|**[link](https://github.com/griffbr/supervoxel-gerrymandering)**|\n", "1704.01926": "|**2018-07-17**|**Semantically-Guided Video Object Segmentation**|Sergi Caelles et.al.|[1704.01926](http://arxiv.org/abs/1704.01926)|null|\n", "1704.00675": "|**2018-03-01**|**The 2017 DAVIS Challenge on Video Object Segmentation**|Jordi Pont-Tuset et.al.|[1704.00675](http://arxiv.org/abs/1704.00675)|null|\n", "1703.09554": "|**2019-03-13**|**Lucid Data Dreaming for Video Object Segmentation**|Anna Khoreva et.al.|[1703.09554](http://arxiv.org/abs/1703.09554)|null|\n", "1612.08169": "|**2016-12-24**|**Unsupervised Video Segmentation via Spatio-Temporally Nonlocal Appearance Learning**|Kaihua Zhang et.al.|[1612.08169](http://arxiv.org/abs/1612.08169)|null|\n", "1612.05478": "|**2017-04-11**|**Video Propagation Networks**|Varun Jampani et.al.|[1612.05478](http://arxiv.org/abs/1612.05478)|null|\n", "1612.02646": "|**2016-12-08**|**Learning Video Object Segmentation from Static Images**|Anna Khoreva et.al.|[1612.02646](http://arxiv.org/abs/1612.02646)|null|\n", "1611.05198": "|**2017-04-13**|**One-Shot Video Object Segmentation**|Sergi Caelles et.al.|[1611.05198](http://arxiv.org/abs/1611.05198)|null|\n", "1607.01115": "|**2016-07-05**|**Click Carving: Segmenting Objects in Video with Point Clicks**|Suyog Dutt Jain et.al.|[1607.01115](http://arxiv.org/abs/1607.01115)|null|\n", "1606.02280": "|**2016-06-07**|**Semi-Supervised Domain Adaptation for Weakly Labeled Semantic Video Object Segmentation**|Huiling Wang et.al.|[1606.02280](http://arxiv.org/abs/1606.02280)|null|\n", "1601.00825": "|**2016-01-05**|**Gamifying Video Object Segmentation**|Simone Palazzo et.al.|[1601.00825](http://arxiv.org/abs/1601.00825)|null|\n", "1308.0315": "|**2013-08-01**|**MAS for video objects segmentation and tracking based on active contours and SURF descriptor**|Mohamed Chakroun et.al.|[1308.0315](http://arxiv.org/abs/1308.0315)|null|\n", "1305.6918": "|**2013-05-29**|**Video Human Segmentation using Fuzzy Object Models and its Application to Body Pose Estimation of Toddlers for Behavior Studies**|Thiago V. Spina et.al.|[1305.6918](http://arxiv.org/abs/1305.6918)|null|\n", "1301.5356": "|**2014-10-27**|**Efficient MRF Energy Propagation for Video Segmentation via Bilateral Filters**|Ozan Sener et.al.|[1301.5356](http://arxiv.org/abs/1301.5356)|null|\n", "2412.13803": "|**2024-12-19**|**M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation**|Zixuan Chen et.al.|[2412.13803](http://arxiv.org/abs/2412.13803)|null|\n"}}