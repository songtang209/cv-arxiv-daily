---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.16
> Usage instructions: [here](./docs/README.md#usage)

## RES

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-03**|**RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2412.02402](http://arxiv.org/abs/2412.02402)|**[link](https://github.com/sosppxo/rg-san)**|
|**2024-11-28**|**MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation**|Minhyun Lee et.al.|[2411.19067](http://arxiv.org/abs/2411.19067)|**[link](https://github.com/naver-ai/maskris)**|
|**2024-11-22**|**Instance-Aware Generalized Referring Expression Segmentation**|E-Ro Nguyen et.al.|[2411.15087](http://arxiv.org/abs/2411.15087)|null|
|**2024-11-03**|**Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation**|Seongsu Ha et.al.|[2411.01494](http://arxiv.org/abs/2411.01494)|null|
|**2024-10-31**|**SegLLM: Multi-round Reasoning Segmentation**|XuDong Wang et.al.|[2410.18923](http://arxiv.org/abs/2410.18923)|null|
|**2024-10-13**|**Text4Seg: Reimagining Image Segmentation as Text Generation**|Mengcheng Lan et.al.|[2410.09855](http://arxiv.org/abs/2410.09855)|**[link](https://github.com/mc-lan/text4seg)**|
|**2024-12-04**|**Boosting Weakly-Supervised Referring Image Segmentation via Progressive Comprehension**|Zaiquan Yang et.al.|[2410.01544](http://arxiv.org/abs/2410.01544)|null|
|**2024-09-29**|**Fully Aligned Network for Referring Image Segmentation**|Yong Liu et.al.|[2409.19569](http://arxiv.org/abs/2409.19569)|null|
|**2024-09-28**|**A Parameter-Efficient Tuning Framework for Language-guided Object Grounding and Robot Grasping**|Houjian Yu et.al.|[2409.19457](http://arxiv.org/abs/2409.19457)|null|
|**2024-09-25**|**PTQ4RIS: Post-Training Quantization for Referring Image Segmentation**|Xiaoyan Jiang et.al.|[2409.17020](http://arxiv.org/abs/2409.17020)|**[link](https://github.com/gugu511yy/ptq4ris)**|
|**2024-09-17**|**Robot Manipulation in Salient Vision through Referring Image Segmentation and Geometric Constraints**|Chen Jiang et.al.|[2409.11518](http://arxiv.org/abs/2409.11518)|null|
|**2024-12-09**|**SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation**|Yi-Chia Chen et.al.|[2409.10542](http://arxiv.org/abs/2409.10542)|null|
|**2024-09-19**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|Seonghoon Yu et.al.|[2408.15521](http://arxiv.org/abs/2408.15521)|null|
|**2024-08-14**|**Cross-aware Early Fusion with Stage-divided Vision and Language Transformer Encoders for Referring Image Segmentation**|Yubin Cho et.al.|[2408.07539](http://arxiv.org/abs/2408.07539)|null|
|**2024-08-07**|**How Well Can Vision Language Models See Image Details?**|Chenhui Gou et.al.|[2408.03940](http://arxiv.org/abs/2408.03940)|null|
|**2024-07-31**|**3D-GRES: Generalized 3D Referring Expression Segmentation**|Changli Wu et.al.|[2407.20664](http://arxiv.org/abs/2407.20664)|**[link](https://github.com/sosppxo/MDIN)**|
|**2024-07-25**|**RefMask3D: Language-Guided Transformer for 3D Referring Segmentation**|Shuting He et.al.|[2407.18244](http://arxiv.org/abs/2407.18244)|**[link](https://github.com/heshuting555/refmask3d)**|
|**2024-07-17**|**Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation**|Seonghoon Yu et.al.|[2407.07412](http://arxiv.org/abs/2407.07412)|**[link](https://github.com/seonghoon-yu/pseudo-ris)**|
|**2024-07-02**|**SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation**|Sayan Nag et.al.|[2407.02389](http://arxiv.org/abs/2407.02389)|null|
|**2024-10-15**|**EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model**|Yuxuan Zhang et.al.|[2406.20076](http://arxiv.org/abs/2406.20076)|**[link](https://github.com/hustvl/evf-sam)**|
|**2024-06-09**|**F-LMM: Grounding Frozen Large Multimodal Models**|Size Wu et.al.|[2406.05821](http://arxiv.org/abs/2406.05821)|**[link](https://github.com/wusize/f-lmm)**|
|**2024-06-03**|**SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation**|Danni Yang et.al.|[2406.01451](http://arxiv.org/abs/2406.01451)|**[link](https://github.com/nini0919/semires)**|
|**2024-07-27**|**GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary Semantic-space Hyperplane**|Yansong Qu et.al.|[2405.17596](http://arxiv.org/abs/2405.17596)|null|
|**2024-11-25**|**CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation**|Zhuoyan Luo et.al.|[2405.15658](http://arxiv.org/abs/2405.15658)|**[link](https://github.com/robertluo1/cohd)**|
|**2024-05-24**|**Bring Adaptive Binding Prototypes to Generalized Referring Expression Segmentation**|Weize Li et.al.|[2405.15169](http://arxiv.org/abs/2405.15169)|**[link](https://github.com/buptlwz/mabp)**|
|**2024-05-18**|**Fuse & Calibrate: A bi-directional Vision-Language Guided Framework for Referring Image Segmentation**|Yichen Yan et.al.|[2405.11205](http://arxiv.org/abs/2405.11205)|null|
|**2024-05-21**|**HARIS: Human-Like Attention for Reference Image Segmentation**|Mengxi Zhang et.al.|[2405.10707](http://arxiv.org/abs/2405.10707)|null|
|**2024-05-15**|**Spatial Semantic Recurrent Mining for Referring Image Segmentation**|Jiaxing Yang et.al.|[2405.09006](http://arxiv.org/abs/2405.09006)|null|
|**2024-04-18**|**Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation**|Qiyuan Dai et.al.|[2404.11998](http://arxiv.org/abs/2404.11998)|null|
|**2024-11-04**|**Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding**|Hai Nguyen-Truong et.al.|[2404.08590](http://arxiv.org/abs/2404.08590)|null|
|**2024-04-12**|**Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation**|Yichen Yan et.al.|[2404.08281](http://arxiv.org/abs/2404.08281)|null|
|**2024-04-27**|**Deep Instruction Tuning for Segment Anything Model**|Xiaorui Huang et.al.|[2404.00650](http://arxiv.org/abs/2404.00650)|**[link](https://github.com/wysnzzzz/dit)**|
|**2024-07-25**|**ReMamber: Referring Image Segmentation with Mamba Twister**|Yuhuan Yang et.al.|[2403.17839](http://arxiv.org/abs/2403.17839)|**[link](https://github.com/yyh-rain-song/ReMamber)**|
|**2024-03-21**|**PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model**|Zheng Zhang et.al.|[2403.14598](http://arxiv.org/abs/2403.14598)|**[link](https://github.com/zamling/psalm)**|
|**2024-06-27**|**Towards Alleviating Text-to-Image Retrieval Hallucination for CLIP in Zero-shot Learning**|Hanyao Wang et.al.|[2402.18400](http://arxiv.org/abs/2402.18400)|null|
|**2024-02-11**|**RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner**|Ying Zang et.al.|[2402.05589](http://arxiv.org/abs/2402.05589)|null|
|**2024-02-04**|**Generalizable Entity Grounding via Assistance of Large Language Model**|Lu Qi et.al.|[2402.02555](http://arxiv.org/abs/2402.02555)|null|
|**2024-01-22**|**Collaborative Position Reasoning Network for Referring Image Segmentation**|Jianjian Cao et.al.|[2401.11775](http://arxiv.org/abs/2401.11775)|null|
|**2023-12-25**|**UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces**|Jiannan Wu et.al.|[2312.15715](http://arxiv.org/abs/2312.15715)|**[link](https://github.com/foundationvision/uniref)**|
|**2024-04-02**|**Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation**|Sihan Liu et.al.|[2312.12470](http://arxiv.org/abs/2312.12470)|**[link](https://github.com/lsan2401/rmsin)**|
|**2024-03-25**|**Mask Grounding for Referring Image Segmentation**|Yong Xien Chng et.al.|[2312.12198](http://arxiv.org/abs/2312.12198)|**[link](https://github.com/yxchng/mask-grounding)**|
|**2024-03-21**|**GSVA: Generalized Segmentation via Multimodal Large Language Models**|Zhuofan Xia et.al.|[2312.10103](http://arxiv.org/abs/2312.10103)|**[link](https://github.com/leaplabthu/gsva)**|
|**2024-03-21**|**Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring Expression Segmentation**|Wenxuan Wang et.al.|[2312.08007](http://arxiv.org/abs/2312.08007)|**[link](https://github.com/rubics-xuan/mres)**|
|**2023-12-01**|**Towards Generalizable Referring Image Segmentation via Target Prompt and Visual Coherence**|Yajie Liu et.al.|[2312.00452](http://arxiv.org/abs/2312.00452)|null|
|**2023-11-30**|**InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation**|Rongyao Fang et.al.|[2311.18835](http://arxiv.org/abs/2311.18835)|**[link](https://github.com/rongyaofang/instructseq)**|
|**2023-11-29**|**Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation**|Minhyeok Lee et.al.|[2311.17952](http://arxiv.org/abs/2311.17952)|null|
|**2024-05-21**|**RISAM: Referring Image Segmentation via Mutual-Aware Attention Features**|Mengxi Zhang et.al.|[2311.15727](http://arxiv.org/abs/2311.15727)|null|
|**2023-11-22**|**Visual In-Context Prompting**|Feng Li et.al.|[2311.13601](http://arxiv.org/abs/2311.13601)|**[link](https://github.com/ux-decoder/dinov)**|
|**2024-04-26**|**Enhancing Visual Grounding and Generalization: A Multi-Task Cycle Training Approach for Vision-Language Models**|Xiaoyu Yang et.al.|[2311.12327](http://arxiv.org/abs/2311.12327)|**[link](https://github.com/anonymgiant/vilam)**|
|**2023-12-18**|**NExT-Chat: An LMM for Chat, Detection and Segmentation**|Ao Zhang et.al.|[2311.04498](http://arxiv.org/abs/2311.04498)|**[link](https://github.com/next-chatv/next-chat)**|
|**2024-06-02**|**GLaMM: Pixel Grounding Large Multimodal Model**|Hanoona Rasheed et.al.|[2311.03356](http://arxiv.org/abs/2311.03356)|**[link](https://github.com/mbzuai-oryx/groundingLMM)**|
|**2023-11-27**|**Towards Omni-supervised Referring Expression Segmentation**|Minglang Huang et.al.|[2311.00397](http://arxiv.org/abs/2311.00397)|**[link](https://github.com/nineblu/omni-res)**|
|**2023-10-27**|**Text Augmented Spatial-aware Zero-shot Referring Image Segmentation**|Yucheng Suo et.al.|[2310.18049](http://arxiv.org/abs/2310.18049)|null|
|**2024-08-20**|**Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation**|Francisco Eiras et.al.|[2310.13479](http://arxiv.org/abs/2310.13479)|**[link](https://github.com/fgirbal/segment-select-correct)**|
|**2023-09-29**|**Towards Complex-query Referring Image Segmentation: A Novel Benchmark**|Wei Ji et.al.|[2309.17205](http://arxiv.org/abs/2309.17205)|null|
|**2023-09-17**|**CLIPUNetr: Assisting Human-robot Interface for Uncalibrated Visual Servoing Control with CLIP-driven Referring Expression Segmentation**|Chen Jiang et.al.|[2309.09183](http://arxiv.org/abs/2309.09183)|null|
|**2024-10-01**|**From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models**|Changming Xiao et.al.|[2309.04109](http://arxiv.org/abs/2309.04109)|**[link](https://github.com/Big-Brother-Pikachu/Text2Mask)**|
|**2023-09-02**|**Contrastive Grouping with Transformer for Referring Image Segmentation**|Jiajin Tang et.al.|[2309.01017](http://arxiv.org/abs/2309.01017)|**[link](https://github.com/toneyaya/cgformer)**|
|**2023-09-01**|**Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models**|Minheng Ni et.al.|[2308.16777](http://arxiv.org/abs/2308.16777)|null|
|**2023-08-31**|**3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2308.16632](http://arxiv.org/abs/2308.16632)|**[link](https://github.com/sosppxo/3d-stmn)**|
|**2023-10-24**|**Shatter and Gather: Learning Referring Image Segmentation with Text Supervision**|Dongwon Kim et.al.|[2308.15512](http://arxiv.org/abs/2308.15512)|**[link](https://github.com/kdwonn/SaG)**|
|**2023-08-28**|**Referring Image Segmentation Using Text Supervision**|Fang Liu et.al.|[2308.14575](http://arxiv.org/abs/2308.14575)|**[link](https://github.com/fawnliu/tris)**|
|**2023-08-26**|**Beyond One-to-One: Rethinking the Referring Image Segmentation**|Yutao Hu et.al.|[2308.13853](http://arxiv.org/abs/2308.13853)|**[link](https://github.com/toggle1995/ris-dmmi)**|
|**2024-10-12**|**EAVL: Explicitly Align Vision and Language for Referring Image Segmentation**|Yichen Yan et.al.|[2308.09779](http://arxiv.org/abs/2308.09779)|null|
|**2023-07-21**|**Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation**|Zunnan Xu et.al.|[2307.11545](http://arxiv.org/abs/2307.11545)|**[link](https://github.com/kkakkkka/etris)**|
|**2024-09-03**|**RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation**|Yonglin Li et.al.|[2307.00997](http://arxiv.org/abs/2307.00997)|**[link](https://github.com/lancasterli/refsam)**|
|**2023-06-19**|**WiCo: Win-win Cooperation of Bottom-up and Top-down Referring Image Segmentation**|Zesen Cheng et.al.|[2306.10750](http://arxiv.org/abs/2306.10750)|null|
|**2024-03-01**|**RRSIS: Referring Remote Sensing Image Segmentation**|Zhenghang Yuan et.al.|[2306.08625](http://arxiv.org/abs/2306.08625)|null|
|**2024-04-07**|**Extending CLIP's Image-Text Alignment to Referring Image Segmentation**|Seoyeon Kim et.al.|[2306.08498](http://arxiv.org/abs/2306.08498)|null|
|**2023-06-01**|**GRES: Generalized Referring Expression Segmentation**|Chang Liu et.al.|[2306.00968](http://arxiv.org/abs/2306.00968)|**[link](https://github.com/henghuiding/ReLA)**|
|**2024-08-12**|**Contextual Object Detection with Multimodal Large Language Models**|Yuhang Zang et.al.|[2305.18279](http://arxiv.org/abs/2305.18279)|**[link](https://github.com/yuhangzang/contextdet)**|
|**2023-05-24**|**Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation**|Chang Liu et.al.|[2305.15302](http://arxiv.org/abs/2305.15302)|null|
|**2023-05-24**|**MMNet: Multi-Mask Network for Referring Image Segmentation**|Yichen Yan et.al.|[2305.14969](http://arxiv.org/abs/2305.14969)|null|
|**2023-05-21**|**Advancing Referring Expression Segmentation Beyond Single Image**|Yixuan Wu et.al.|[2305.12452](http://arxiv.org/abs/2305.12452)|null|
|**2024-02-14**|**CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation**|Wenxuan Wang et.al.|[2305.11481](http://arxiv.org/abs/2305.11481)|null|
|**2023-04-12**|**Meta Compositional Referring Expression Segmentation**|Li Xu et.al.|[2304.04415](http://arxiv.org/abs/2304.04415)|null|
|**2023-04-03**|**Zero-shot Referring Image Segmentation with Global-Local Context Features**|Seonghoon Yu et.al.|[2303.17811](http://arxiv.org/abs/2303.17811)|**[link](https://github.com/seonghoon-yu/zero-shot-ris)**|
|**2023-03-11**|**Semantics-Aware Dynamic Localization and Refinement for Referring Image Segmentation**|Zhao Yang et.al.|[2303.06345](http://arxiv.org/abs/2303.06345)|null|
|**2023-03-03**|**Unleashing Text-to-Image Diffusion Models for Visual Perception**|Wenliang Zhao et.al.|[2303.02153](http://arxiv.org/abs/2303.02153)|**[link](https://github.com/wl-zhao/VPD)**|
|**2023-03-27**|**PolyFormer: Referring Image Segmentation as Sequential Polygon Generation**|Jiang Liu et.al.|[2302.07387](http://arxiv.org/abs/2302.07387)|**[link](https://github.com/amazon-science/polygon-transformer)**|
|**2023-03-22**|**Linguistic Query-Guided Mask Generation for Referring Image Segmentation**|Zhichao Wei et.al.|[2301.06429](http://arxiv.org/abs/2301.06429)|null|
|**2022-12-27**|**Position-Aware Contrastive Alignment for Referring Image Segmentation**|Bo Chen et.al.|[2212.13419](http://arxiv.org/abs/2212.13419)|null|
|**2022-12-17**|**Fully and Weakly Supervised Referring Expression Segmentation with End-to-End Learning**|Hui Li et.al.|[2212.10278](http://arxiv.org/abs/2212.10278)|null|
|**2022-12-04**|**CoupAlign: Coupling Word-Pixel with Sentence-Mask Alignments for Referring Image Segmentation**|Zicheng Zhang et.al.|[2212.01769](http://arxiv.org/abs/2212.01769)|null|
|**2022-11-15**|**A Unified Mutual Supervision Framework for Referring Expression Segmentation and Generation**|Shijia Huang et.al.|[2211.07919](http://arxiv.org/abs/2211.07919)|null|
|**2022-09-21**|**Exploring Modulated Detection Transformer as a Tool for Action Recognition in Videos**|Tomás Crisol et.al.|[2209.10126](http://arxiv.org/abs/2209.10126)|**[link](https://github.com/bhi-research/ava_mdetr)**|
|**2023-07-23**|**Towards Robust Referring Image Segmentation**|Jianzong Wu et.al.|[2209.09554](http://arxiv.org/abs/2209.09554)|**[link](https://github.com/jianzongwu/robust-ref-seg)**|
|**2022-05-12**|**Weakly-supervised segmentation of referring expressions**|Robin Strudel et.al.|[2205.04725](http://arxiv.org/abs/2205.04725)|null|
|**2022-03-31**|**ReSTR: Convolution-free Referring Image Segmentation Using Transformers**|Namyup Kim et.al.|[2203.16768](http://arxiv.org/abs/2203.16768)|null|
|**2021-12-24**|**Grounding Linguistic Commands to Navigable Regions**|Nivedita Rufus et.al.|[2112.13031](http://arxiv.org/abs/2112.13031)|**[link](https://github.com/kanji95/Talk2car-Refseg)**|
|**2022-03-30**|**Image Segmentation Using Text and Image Prompts**|Timo Lüddecke et.al.|[2112.10003](http://arxiv.org/abs/2112.10003)|**[link](https://github.com/timojl/clipseg)**|
|**2022-04-05**|**LAVT: Language-Aware Vision Transformer for Referring Image Segmentation**|Zhao Yang et.al.|[2112.02244](http://arxiv.org/abs/2112.02244)|**[link](https://github.com/yz93/lavt-ris)**|
|**2022-03-14**|**CRIS: CLIP-Driven Referring Image Segmentation**|Zhaoqing Wang et.al.|[2111.15174](http://arxiv.org/abs/2111.15174)|**[link](https://github.com/DerrickWang005/CRIS.pytorch)**|
|**2021-11-25**|**MaIL: A Unified Mask-Image-Language Trimodal Network for Referring Image Segmentation**|Zizhang Li et.al.|[2111.10747](http://arxiv.org/abs/2111.10747)|null|
|**2021-10-09**|**Two-stage Visual Cues Enhancement Network for Referring Image Segmentation**|Yang Jiao et.al.|[2110.04435](http://arxiv.org/abs/2110.04435)|**[link](https://github.com/sxjyjay/tv-net)**|
|**2021-06-16**|**CMF: Cascaded Multi-model Fusion for Referring Image Segmentation**|Jianhua Yang et.al.|[2106.08617](http://arxiv.org/abs/2106.08617)|**[link](https://github.com/jianhua2022/CMF-Refseg)**|
|**2021-05-15**|**Cross-Modal Progressive Comprehension for Referring Segmentation**|Si Liu et.al.|[2105.07175](http://arxiv.org/abs/2105.07175)|**[link](https://github.com/spyflying/CMPC-Refseg)**|
|**2021-05-05**|**Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation**|Guang Feng et.al.|[2105.01839](http://arxiv.org/abs/2105.01839)|null|
|**2022-08-14**|**Comprehensive Multi-Modal Interactions for Referring Image Segmentation**|Kanishk Jain et.al.|[2104.10412](http://arxiv.org/abs/2104.10412)|**[link](https://github.com/kanji95/SHNET)**|
|**2021-03-30**|**Locate then Segment: A Strong Pipeline for Referring Image Segmentation**|Ya Jing et.al.|[2103.16284](http://arxiv.org/abs/2103.16284)|null|
|**2021-04-14**|**OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene Grounding**|Ke-Jyun Wang et.al.|[2103.07679](http://arxiv.org/abs/2103.07679)|**[link](https://github.com/lluma/OCID-Ref)**|
|**2020-10-05**|**Linguistic Structure Guided Context Modeling for Referring Image Segmentation**|Tianrui Hui et.al.|[2010.00515](http://arxiv.org/abs/2010.00515)|**[link](https://github.com/spyflying/LSCM-Refseg)**|
|**2020-10-01**|**Referring Image Segmentation via Cross-Modal Progressive Comprehension**|Shaofei Huang et.al.|[2010.00514](http://arxiv.org/abs/2010.00514)|**[link](https://github.com/spyflying/CMPC-Refseg)**|
|**2022-06-23**|**Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters**|İlker Kesen et.al.|[2003.12739](http://arxiv.org/abs/2003.12739)|**[link](https://github.com/ilkerkesen/bvpr)**|
|**2020-01-30**|**Dual Convolutional LSTM Network for Referring Image Segmentation**|Linwei Ye et.al.|[2001.11561](http://arxiv.org/abs/2001.11561)|null|
|**2019-04-09**|**Cross-Modal Self-Attention Network for Referring Image Segmentation**|Linwei Ye et.al.|[1904.04745](http://arxiv.org/abs/1904.04745)|null|
|**2019-04-06**|**CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions**|Runtao Liu et.al.|[1901.00850](http://arxiv.org/abs/1901.00850)|null|
|**2017-08-04**|**Recurrent Multimodal Interaction for Referring Image Segmentation**|Chenxi Liu et.al.|[1703.07939](http://arxiv.org/abs/1703.07939)|**[link](https://github.com/chenxi116/TF-phrasecut-public)**|

## REC

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-11**|**Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models**|Quang-Hung Le et.al.|[2412.08125](http://arxiv.org/abs/2412.08125)|null|
|**2024-12-11**|**Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic Losses**|Jiayun Luo et.al.|[2412.08110](http://arxiv.org/abs/2412.08110)|null|
|**2024-12-09**|**3D Spatial Understanding in MLLMs: Disambiguation and Evaluation**|Chun-Peng Chang et.al.|[2412.06613](http://arxiv.org/abs/2412.06613)|null|
|**2024-12-10**|**TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action**|Zixian Ma et.al.|[2412.05479](http://arxiv.org/abs/2412.05479)|null|
|**2024-12-06**|**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**|Zhe Chen et.al.|[2412.05271](http://arxiv.org/abs/2412.05271)|**[link](https://github.com/opengvlab/internvl)**|
|**2024-12-05**|**SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding**|Rong Li et.al.|[2412.04383](http://arxiv.org/abs/2412.04383)|null|
|**2024-12-05**|**M $^{3}$ D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction**|Jiang Liu et.al.|[2412.04026](http://arxiv.org/abs/2412.04026)|**[link](https://github.com/solkx/m3d)**|
|**2024-12-01**|**Paint Outside the Box: Synthesizing and Selecting Training Data for Visual Grounding**|Zilin Du et.al.|[2412.00684](http://arxiv.org/abs/2412.00684)|null|
|**2024-11-27**|**3D Scene Graph Guided Vision-Language Pre-training**|Hao Liu et.al.|[2411.18666](http://arxiv.org/abs/2411.18666)|null|
|**2024-11-25**|**Interpreting Object-level Foundation Models via Visual Precision Search**|Ruoyu Chen et.al.|[2411.16198](http://arxiv.org/abs/2411.16198)|null|
|**2024-11-27**|**BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence**|Xuewu Lin et.al.|[2411.14869](http://arxiv.org/abs/2411.14869)|**[link](https://github.com/HorizonRobotics/BIP3D)**|
|**2024-11-22**|**Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension**|Luca Parolari et.al.|[2411.14807](http://arxiv.org/abs/2411.14807)|null|
|**2024-11-21**|**Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction Problems**|Qihao Yuan et.al.|[2411.14594](http://arxiv.org/abs/2411.14594)|**[link](https://github.com/sunsleaf/csvg)**|
|**2024-11-21**|**Visual Contexts Clarify Ambiguous Expressions: A Benchmark Dataset**|Heejeong Nam et.al.|[2411.14137](http://arxiv.org/abs/2411.14137)|**[link](https://github.com/hazel-heejeong-nam/vague)**|
|**2024-11-16**|**GeoGround: A Unified Large Vision-Language Model. for Remote Sensing Visual Grounding**|Yue Zhou et.al.|[2411.11904](http://arxiv.org/abs/2411.11904)|**[link](https://github.com/zytx121/geoground)**|
|**2024-11-15**|**Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level**|Andong Deng et.al.|[2411.09921](http://arxiv.org/abs/2411.09921)|null|
|**2024-11-07**|**VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos**|Shehan Munasinghe et.al.|[2411.04923](http://arxiv.org/abs/2411.04923)|null|
|**2024-11-07**|**LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with Transformers**|Yeong-Seung Baek et.al.|[2411.04351](http://arxiv.org/abs/2411.04351)|null|
|**2024-11-05**|**Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding**|Sombit Dey et.al.|[2411.03405](http://arxiv.org/abs/2411.03405)|null|
|**2024-10-31**|**Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding**|Jinlong He et.al.|[2410.23822](http://arxiv.org/abs/2410.23822)|null|
|**2024-10-31**|**Phrase Decoupling Cross-Modal Hierarchical Matching and Progressive Position Correction for Visual Grounding**|Minghong Xie et.al.|[2410.23570](http://arxiv.org/abs/2410.23570)|**[link](https://github.com/X7J92/VGNet)**|
|**2024-10-21**|**Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models**|Yufei Zhan et.al.|[2410.16163](http://arxiv.org/abs/2410.16163)|**[link](https://github.com/jefferyzhan/griffon)**|
|**2024-10-21**|**Joint Top-Down and Bottom-Up Frameworks for 3D Visual Grounding**|Yang Liu et.al.|[2410.15615](http://arxiv.org/abs/2410.15615)|null|
|**2024-10-17**|**VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding**|Runsen Xu et.al.|[2410.13860](http://arxiv.org/abs/2410.13860)|**[link](https://github.com/openrobotlab/vlm-grounder)**|
|**2024-10-17**|**Trust but Verify: Programmatic VLM Evaluation in the Wild**|Viraj Prabhu et.al.|[2410.13121](http://arxiv.org/abs/2410.13121)|null|
|**2024-11-28**|**WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines**|Genta Indra Winata et.al.|[2410.12705](http://arxiv.org/abs/2410.12705)|**[link](https://github.com/worldcuisines/worldcuisines)**|
|**2024-10-16**|**VividMed: Vision Language Model with Versatile Visual Grounding for Medicine**|Lingxiao Luo et.al.|[2410.12694](http://arxiv.org/abs/2410.12694)|**[link](https://github.com/function2-llx/mmmm)**|
|**2024-10-16**|**Context-Infused Visual Grounding for Art**|Selina Khan et.al.|[2410.12369](http://arxiv.org/abs/2410.12369)|**[link](https://github.com/selinakhan/CIGAr)**|
|**2024-10-16**|**MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs**|Yunqiu Xu et.al.|[2410.12332](http://arxiv.org/abs/2410.12332)|null|
|**2024-10-15**|**VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI**|Sijie Cheng et.al.|[2410.11623](http://arxiv.org/abs/2410.11623)|null|
|**2024-10-14**|**Learning to Ground VLMs without Forgetting**|Aritra Bhowmik et.al.|[2410.10491](http://arxiv.org/abs/2410.10491)|null|
|**2024-10-10**|**Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics**|Junyi Cao et.al.|[2410.08257](http://arxiv.org/abs/2410.08257)|null|
|**2024-10-10**|**Grounding Robot Policies with Visuomotor Language Guidance**|Arthur Bucker et.al.|[2410.06473](http://arxiv.org/abs/2410.06473)|null|
|**2024-10-10**|**Context-Aware Command Understanding for Tabletop Scenarios**|Paul Gajewski et.al.|[2410.06355](http://arxiv.org/abs/2410.06355)|null|
|**2024-10-07**|**Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents**|Boyu Gou et.al.|[2410.05243](http://arxiv.org/abs/2410.05243)|**[link](https://github.com/OSU-NLP-Group/UGround)**|
|**2024-10-11**|**VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks**|Ziyan Jiang et.al.|[2410.05160](http://arxiv.org/abs/2410.05160)|null|
|**2024-10-04**|**Adaptive Masking Enhances Visual Grounding**|Sen Jia et.al.|[2410.03161](http://arxiv.org/abs/2410.03161)|null|
|**2024-09-30**|**World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering**|Jiacong Wang et.al.|[2409.20424](http://arxiv.org/abs/2409.20424)|**[link](https://github.com/foundation-multimodal-models/world2code)**|
|**2024-09-27**|**Individuation in Neural Models with and without Visual Grounding**|Alexey Tikhonov et.al.|[2409.18868](http://arxiv.org/abs/2409.18868)|null|
|**2024-10-29**|**ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue**|Zhangpu Li et.al.|[2409.17610](http://arxiv.org/abs/2409.17610)|null|
|**2024-10-28**|**SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion**|Ming Dai et.al.|[2409.17531](http://arxiv.org/abs/2409.17531)|**[link](https://github.com/dmmm1997/simvg)**|
|**2024-11-01**|**Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE**|Xun Zhu et.al.|[2409.17508](http://arxiv.org/abs/2409.17508)|**[link](https://github.com/tsinghua-msiip/uni-med)**|
|**2024-09-05**|**Visual Prompting in Multimodal Large Language Models: A Survey**|Junda Wu et.al.|[2409.15310](http://arxiv.org/abs/2409.15310)|null|
|**2024-09-23**|**FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension**|Junzhuo Liu et.al.|[2409.14750](http://arxiv.org/abs/2409.14750)|**[link](https://github.com/liujunzhuo/FineCops-Ref)**|
|**2024-10-06**|**MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension**|Ting Liu et.al.|[2409.13609](http://arxiv.org/abs/2409.13609)|**[link](https://github.com/liuting20/mapper)**|
|**2024-10-15**|**LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension**|Amaia Cardiel et.al.|[2409.11919](http://arxiv.org/abs/2409.11919)|null|
|**2024-12-09**|**Improving the Efficiency of Visually Augmented Language Models**|Paula Ontalvilla et.al.|[2409.11148](http://arxiv.org/abs/2409.11148)|**[link](https://github.com/paulaonta/blind-valm)**|
|**2024-09-16**|**HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models**|Vineet Bhat et.al.|[2409.10419](http://arxiv.org/abs/2409.10419)|null|
|**2024-09-12**|**Bayesian Self-Training for Semi-Supervised 3D Segmentation**|Ozan Unal et.al.|[2409.08102](http://arxiv.org/abs/2409.08102)|null|
|**2024-09-09**|**Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings**|Leanne Nortje et.al.|[2409.06013](http://arxiv.org/abs/2409.06013)|**[link](https://github.com/LeanneNortje/low-resource_VPKL)**|
|**2024-09-09**|**Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding**|Bram Willemsen et.al.|[2409.05721](http://arxiv.org/abs/2409.05721)|**[link](https://github.com/willemsenbram/reg-with-guiding)**|
|**2024-10-01**|**Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling**|Georgios Pantazopoulos et.al.|[2409.05395](http://arxiv.org/abs/2409.05395)|**[link](https://github.com/gpantaz/vl_mamba)**|
|**2024-09-08**|**Visual Grounding with Multi-modal Conditional Adaptation**|Ruilin Yao et.al.|[2409.04999](http://arxiv.org/abs/2409.04999)|**[link](https://github.com/mr-bigworth/mmca)**|
|**2024-11-23**|**Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding**|Yunze Man et.al.|[2409.03757](http://arxiv.org/abs/2409.03757)|null|
|**2024-09-05**|**Make Graph-based Referring Expression Comprehension Great Again through Expression-guided Dynamic Gating and Regression**|Jingcheng Ke et.al.|[2409.03385](http://arxiv.org/abs/2409.03385)|null|
|**2024-09-03**|**Visually Grounded Speech Models for Low-resource Languages and Cognitive Modelling**|Leanne Nortje et.al.|[2409.02865](http://arxiv.org/abs/2409.02865)|null|
|**2024-11-12**|**ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation**|Wenlong Huang et.al.|[2409.01652](http://arxiv.org/abs/2409.01652)|null|
|**2024-09-02**|**CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding**|Ivana Beňová et.al.|[2409.01389](http://arxiv.org/abs/2409.01389)|null|
|**2024-08-30**|**NanoMVG: USV-Centric Low-Power Multi-Task Visual Grounding based on Prompt-Guided Camera and 4D mmWave Radar**|Runwei Guan et.al.|[2408.17207](http://arxiv.org/abs/2408.17207)|null|
|**2024-08-29**|**ResVG: Enhancing Relation and Semantic Understanding in Multiple Instances for Visual Grounding**|Minghang Zheng et.al.|[2408.16314](http://arxiv.org/abs/2408.16314)|**[link](https://github.com/minghangz/resvg)**|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213](http://arxiv.org/abs/2408.16213)|null|
|**2024-08-23**|**IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities**|Bin Wang et.al.|[2408.12902](http://arxiv.org/abs/2408.12902)|**[link](https://github.com/360cvgroup/inner-adaptor-architecture)**|
|**2024-10-22**|**A Lightweight Modular Framework for Low-Cost Open-Vocabulary Object Detection Training**|Bilal Faye et.al.|[2408.10787](http://arxiv.org/abs/2408.10787)|**[link](https://github.com/b-faye/lightmdetr)**|
|**2024-08-15**|**Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models**|Tianyu Wang et.al.|[2408.07975](http://arxiv.org/abs/2408.07975)|null|
|**2024-08-09**|**Revisiting Multi-Modal LLM Evaluation**|Jian Lu et.al.|[2408.05334](http://arxiv.org/abs/2408.05334)|null|
|**2024-08-09**|**In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation**|Dahyun Kang et.al.|[2408.04961](http://arxiv.org/abs/2408.04961)|**[link](https://github.com/dahyun-kang/lazygrounding)**|
|**2024-08-07**|**Task-oriented Sequential Grounding in 3D Scenes**|Zhuofan Zhang et.al.|[2408.04034](http://arxiv.org/abs/2408.04034)|null|
|**2024-08-04**|**Visual Grounding for Object-Level Generalization in Reinforcement Learning**|Haobin Jiang et.al.|[2408.01942](http://arxiv.org/abs/2408.01942)|**[link](https://github.com/pku-rl/copl)**|
|**2024-11-28**|**VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance**|Divyansh Srivastava et.al.|[2408.01432](http://arxiv.org/abs/2408.01432)|**[link](https://github.com/trustworthy-ml-lab/vlg-cbm)**|
|**2024-08-02**|**An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding**|Wei Chen et.al.|[2408.01120](http://arxiv.org/abs/2408.01120)|**[link](https://github.com/chenwei746/eevg)**|
|**2024-07-29**|**MaskInversion: Localized Embeddings via Optimization of Explainability Maps**|Walid Bousselham et.al.|[2407.20034](http://arxiv.org/abs/2407.20034)|null|
|**2024-07-25**|**UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models**|Xinyu Pi et.al.|[2407.18391](http://arxiv.org/abs/2407.18391)|null|
|**2024-07-25**|**RefMask3D: Language-Guided Transformer for 3D Referring Segmentation**|Shuting He et.al.|[2407.18244](http://arxiv.org/abs/2407.18244)|**[link](https://github.com/heshuting555/refmask3d)**|
|**2024-07-23**|**Unveiling and Mitigating Bias in Audio Visual Segmentation**|Peiwen Sun et.al.|[2407.16638](http://arxiv.org/abs/2407.16638)|null|
|**2024-07-18**|**Learning Visual Grounding from Generative Vision and Language Model**|Shijie Wang et.al.|[2407.14563](http://arxiv.org/abs/2407.14563)|null|
|**2024-09-02**|**PD-APE: A Parallel Decoding Framework with Adaptive Position Encoding for 3D Visual Grounding**|Chenshu Hou et.al.|[2407.14491](http://arxiv.org/abs/2407.14491)|null|
|**2024-07-18**|**Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models**|Xiaoyu Zhu et.al.|[2407.13642](http://arxiv.org/abs/2407.13642)|null|
|**2024-07-17**|**CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**|Kalliopi Basioti et.al.|[2407.11393](http://arxiv.org/abs/2407.11393)|**[link](https://github.com/SamsungLabs/CIC-BART-SSA)**|
|**2024-07-12**|**Robotic Control via Embodied Chain-of-Thought Reasoning**|Michał Zawalski et.al.|[2407.08693](http://arxiv.org/abs/2407.08693)|null|
|**2024-07-08**|**VIMI: Grounding Video Generation through Multi-modal Instruction**|Yuwei Fang et.al.|[2407.06304](http://arxiv.org/abs/2407.06304)|null|
|**2024-07-08**|**3D Vision and Language Pretraining with Large-Scale Synthetic Data**|Dejie Yang et.al.|[2407.06084](http://arxiv.org/abs/2407.06084)|**[link](https://github.com/idejie/3DSyn)**|
|**2024-08-21**|**FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance**|Jiedong Zhuang et.al.|[2407.05578](http://arxiv.org/abs/2407.05578)|null|
|**2024-07-10**|**Multi-branch Collaborative Learning Network for 3D Visual Grounding**|Zhipeng Qian et.al.|[2407.05363](http://arxiv.org/abs/2407.05363)|**[link](https://github.com/qzp2018/MCLN)**|
|**2024-07-07**|**Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model**|Danni Yang et.al.|[2407.05352](http://arxiv.org/abs/2407.05352)|**[link](https://github.com/nini0919/diffpng)**|
|**2024-07-06**|**The Solution for the 5th GCAIAC Zero-shot Referring Expression Comprehension Challenge**|Longfei Huang et.al.|[2407.04998](http://arxiv.org/abs/2407.04998)|null|
|**2024-10-25**|**Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition**|Aditya K Surikuchi et.al.|[2407.04559](http://arxiv.org/abs/2407.04559)|**[link](https://github.com/akskuchi/dhm-visual-storytelling)**|
|**2024-07-05**|**Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge**|Xiangyu Wu et.al.|[2407.04255](http://arxiv.org/abs/2407.04255)|null|
|**2024-07-05**|**Smart Vision-Language Reasoners**|Denisa Roberts et.al.|[2407.04212](http://arxiv.org/abs/2407.04212)|**[link](https://github.com/smarter-vlm/smarter)**|
|**2024-07-06**|**ACTRESS: Active Retraining for Semi-supervised Visual Grounding**|Weitai Kang et.al.|[2407.03251](http://arxiv.org/abs/2407.03251)|null|
|**2024-07-06**|**Visual Grounding with Attention-Driven Constraint Balancing**|Weitai Kang et.al.|[2407.03243](http://arxiv.org/abs/2407.03243)|null|
|**2024-07-06**|**SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding**|Weitai Kang et.al.|[2407.03200](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|
|**2024-08-04**|**ViG-Bias: Visually Grounded Bias Discovery and Mitigation**|Badr-Eddine Marani et.al.|[2407.01996](http://arxiv.org/abs/2407.01996)|null|
|**2024-07-02**|**The Solution for the ICCV 2023 Perception Test Challenge 2023 -- Task 6 -- Grounded videoQA**|Hailiang Zhang et.al.|[2407.01907](http://arxiv.org/abs/2407.01907)|null|
|**2024-09-17**|**Visual grounding for desktop graphical user interfaces**|Tassnim Dardouri et.al.|[2407.01558](http://arxiv.org/abs/2407.01558)|null|
|**2024-07-17**|**ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities**|Chenming Zhu et.al.|[2407.01525](http://arxiv.org/abs/2407.01525)|null|
|**2024-10-29**|**M $^2$ IST: Multi-Modal Interactive Side-Tuning for Efficient Referring Expression Comprehension**|Xuyang Liu et.al.|[2407.01131](http://arxiv.org/abs/2407.01131)|null|
|**2024-07-01**|**CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation**|Yuxuan Wang et.al.|[2407.01081](http://arxiv.org/abs/2407.01081)|**[link](https://github.com/WangYuxuan93/CVLUE)**|
|**2024-06-28**|**From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models**|Mehar Bhatia et.al.|[2407.00263](http://arxiv.org/abs/2407.00263)|null|
|**2024-06-28**|**FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts**|Shubhankar Singh et.al.|[2406.19237](http://arxiv.org/abs/2406.19237)|null|
|**2024-06-30**|**Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO**|Fuseini Mumuni et.al.|[2406.19057](http://arxiv.org/abs/2406.19057)|null|
|**2024-10-13**|**Towards Open-World Grasping with Large Vision-Language Models**|Georgios Tziafas et.al.|[2406.18722](http://arxiv.org/abs/2406.18722)|null|
|**2024-06-26**|**On the Role of Visual Grounding in VQA**|Daniel Reich et.al.|[2406.18253](http://arxiv.org/abs/2406.18253)|null|
|**2024-06-26**|**ScanFormer: Referring Expression Comprehension by Iteratively Scanning**|Wei Su et.al.|[2406.18048](http://arxiv.org/abs/2406.18048)|null|
|**2024-06-24**|**Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models**|Jierun Chen et.al.|[2406.16866](http://arxiv.org/abs/2406.16866)|**[link](https://github.com/jierunchen/ref-l4)**|
|**2024-12-04**|**Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs**|Shengbang Tong et.al.|[2406.16860](http://arxiv.org/abs/2406.16860)|**[link](https://github.com/cambrian-mllm/cambrian)**|
|**2024-06-21**|**AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**|Wenbin An et.al.|[2406.12718](http://arxiv.org/abs/2406.12718)|**[link](https://github.com/lackel/agla)**|
|**2024-11-11**|**VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding**|Xiang Li et.al.|[2406.12384](http://arxiv.org/abs/2406.12384)|**[link](https://github.com/lx709/vrsbench)**|
|**2024-06-17**|**Reframing linguistic bootstrapping as joint inference using visually-grounded grammar induction models**|Eva Portelance et.al.|[2406.11977](http://arxiv.org/abs/2406.11977)|null|
|**2024-09-11**|**DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models**|Renqiu Xia et.al.|[2406.11633](http://arxiv.org/abs/2406.11633)|**[link](https://github.com/UniModal4Reasoning/DocGenome)**|
|**2024-06-14**|**ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation**|Chufan Shi et.al.|[2406.09961](http://arxiv.org/abs/2406.09961)|**[link](https://github.com/chartmimic/chartmimic)**|
|**2024-10-21**|**Learning Language Structures through Grounding**|Freda Shi et.al.|[2406.09662](http://arxiv.org/abs/2406.09662)|null|
|**2024-06-13**|**MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations**|Ruiyuan Lyu et.al.|[2406.09401](http://arxiv.org/abs/2406.09401)|**[link](https://github.com/openrobotlab/embodiedscan)**|
|**2024-06-13**|**Towards Vision-Language Geo-Foundation Model: A Survey**|Yue Zhou et.al.|[2406.09385](http://arxiv.org/abs/2406.09385)|**[link](https://github.com/zytx121/awesome-vlgfm)**|
|**2024-06-13**|**Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding**|Yue Xu et.al.|[2406.08907](http://arxiv.org/abs/2406.08907)|null|
|**2024-06-11**|**Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation**|Jinyuan Li et.al.|[2406.07268](http://arxiv.org/abs/2406.07268)|**[link](https://github.com/JinYuanLi0012/RiVEG)**|
|**2024-06-11**|**Translating speech with just images**|Dan Oneata et.al.|[2406.07133](http://arxiv.org/abs/2406.07133)|**[link](https://github.com/danoneata/strim)**|
|**2024-06-09**|**F-LMM: Grounding Frozen Large Multimodal Models**|Size Wu et.al.|[2406.05821](http://arxiv.org/abs/2406.05821)|**[link](https://github.com/wusize/f-lmm)**|
|**2024-07-22**|**A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances, and Future Directions**|Daizong Liu et.al.|[2406.05785](http://arxiv.org/abs/2406.05785)|**[link](https://github.com/liudaizong/awesome-3d-visual-grounding)**|
|**2024-06-09**|**Separating the "Chirp" from the "Chat": Self-supervised Visual Grounding of Sound and Language**|Mark Hamilton et.al.|[2406.05629](http://arxiv.org/abs/2406.05629)|**[link](https://github.com/mhamilton723/DenseAV)**|
|**2024-06-03**|**Selectively Answering Visual Questions**|Julian Martin Eisenschlos et.al.|[2406.00980](http://arxiv.org/abs/2406.00980)|null|
|**2024-11-01**|**HENASY: Learning to Assemble Scene-Entities for Egocentric Video-Language Model**|Khoa Vo et.al.|[2406.00307](http://arxiv.org/abs/2406.00307)|null|
|**2024-10-16**|**Instruction-Guided Visual Masking**|Jinliang Zheng et.al.|[2405.19783](http://arxiv.org/abs/2405.19783)|**[link](https://github.com/2toinf/ivm)**|
|**2024-11-03**|**Why are Visually-Grounded Language Models Bad at Image Classification?**|Yuhui Zhang et.al.|[2405.18415](http://arxiv.org/abs/2405.18415)|**[link](https://github.com/yuhui-zh15/vlmclassifier)**|
|**2024-07-06**|**Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention**|Weitai Kang et.al.|[2405.18295](http://arxiv.org/abs/2405.18295)|null|
|**2024-05-28**|**LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding**|Haoyu Zhao et.al.|[2405.17104](http://arxiv.org/abs/2405.17104)|null|
|**2024-05-28**|**VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models**|Zejun Li et.al.|[2405.16919](http://arxiv.org/abs/2405.16919)|**[link](https://github.com/rupertluo/vocot)**|
|**2024-05-24**|**Talk to Parallel LiDARs: A Human-LiDAR Interaction Method Based on 3D Visual Grounding**|Yuhang Liu et.al.|[2405.15274](http://arxiv.org/abs/2405.15274)|null|
|**2024-05-23**|**Multi-modality Regional Alignment Network for Covid X-Ray Survival Prediction and Report Generation**|Zhusi Zhong et.al.|[2405.14113](http://arxiv.org/abs/2405.14113)|**[link](https://github.com/zzs95/mranet)**|
|**2024-07-19**|**Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension**|Runwei Guan et.al.|[2405.12821](http://arxiv.org/abs/2405.12821)|**[link](https://github.com/guanrunwei/talk2radar)**|
|**2024-03-22**|**Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery**|Guankun Wang et.al.|[2405.10948](http://arxiv.org/abs/2405.10948)|null|
|**2024-05-16**|**Adversarial Robustness for Visual Grounding of Multimodal Large Language Models**|Kuofeng Gao et.al.|[2405.09981](http://arxiv.org/abs/2405.09981)|**[link](https://github.com/KuofengGao/MLLM-Grounding-Robustness)**|
|**2024-05-11**|**LogoMotion: Visually Grounded Code Generation for Content-Aware Animation**|Vivian Liu et.al.|[2405.07065](http://arxiv.org/abs/2405.07065)|null|
|**2024-06-08**|**DARA: Domain- and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding**|Ting Liu et.al.|[2405.06217](http://arxiv.org/abs/2405.06217)|**[link](https://github.com/liuting20/dara)**|
|**2024-04-30**|**Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners**|Chun Feng et.al.|[2404.19696](http://arxiv.org/abs/2404.19696)|null|
|**2024-04-29**|**Q-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM**|Navid Rajabi et.al.|[2404.19128](http://arxiv.org/abs/2404.19128)|null|
|**2024-08-02**|**BlenderAlchemy: Editing 3D Graphics with Vision-Language Models**|Ian Huang et.al.|[2404.17672](http://arxiv.org/abs/2404.17672)|null|
|**2024-04-25**|**List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs**|An Yan et.al.|[2404.16375](http://arxiv.org/abs/2404.16375)|**[link](https://github.com/zzxslp/som-llava)**|
|**2024-07-15**|**ChEX: Interactive Localization and Region Description in Chest X-rays**|Philip Müller et.al.|[2404.15770](http://arxiv.org/abs/2404.15770)|**[link](https://github.com/philip-mueller/chex)**|
|**2024-04-21**|**Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following**|Suyeon Shin et.al.|[2404.15190](http://arxiv.org/abs/2404.15190)|null|
|**2024-09-05**|**HiVG: Hierarchical Multimodal Fine-grained Modulation for Visual Grounding**|Linhui Xiao et.al.|[2404.13400](http://arxiv.org/abs/2404.13400)|**[link](https://github.com/linhuixiao/hivg)**|
|**2024-04-19**|**Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models**|Chuofan Ma et.al.|[2404.13013](http://arxiv.org/abs/2404.13013)|**[link](https://github.com/FoundationVision/Groma)**|
|**2024-09-19**|**Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework through Prompt-based Localization**|Yongdong Luo et.al.|[2404.11064](http://arxiv.org/abs/2404.11064)|null|
|**2024-04-10**|**MedRG: Medical Report Grounding with Multi-modal Large Language Model**|Ke Zou et.al.|[2404.06798](http://arxiv.org/abs/2404.06798)|null|
|**2024-04-03**|**Text-driven Affordance Learning from Egocentric Vision**|Tomoya Yoshida et.al.|[2404.02523](http://arxiv.org/abs/2404.02523)|null|
|**2024-11-06**|**VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis**|Chao Pang et.al.|[2403.20213](http://arxiv.org/abs/2403.20213)|**[link](https://github.com/opendatalab/vhm)**|
|**2024-07-22**|**PropTest: Automatic Property Testing for Improved Visual Programming**|Jaywon Koo et.al.|[2403.16921](http://arxiv.org/abs/2403.16921)|null|
|**2024-12-04**|**Data-Efficient 3D Visual Grounding via Order-Aware Referring**|Tung-Yu Wu et.al.|[2403.16539](http://arxiv.org/abs/2403.16539)|null|
|**2024-03-29**|**MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis**|Mai A. Shaaban et.al.|[2403.15585](http://arxiv.org/abs/2403.15585)|**[link](https://github.com/biomedia-mbzuai/medpromptx)**|
|**2024-10-06**|**Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping**|Pengwei Xie et.al.|[2403.15054](http://arxiv.org/abs/2403.15054)|null|
|**2024-03-21**|**VidLA: Video-Language Alignment at Scale**|Mamshad Nayeem Rizve et.al.|[2403.14870](http://arxiv.org/abs/2403.14870)|null|
|**2024-03-21**|**Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling**|Chengxu Zhuang et.al.|[2403.14551](http://arxiv.org/abs/2403.14551)|null|
|**2024-03-20**|**Visually Grounded Speech Models have a Mutual Exclusivity Bias**|Leanne Nortje et.al.|[2403.13922](http://arxiv.org/abs/2403.13922)|null|
|**2024-03-20**|**Learning from Models and Data for Visual Grounding**|Ruozhen He et.al.|[2403.13804](http://arxiv.org/abs/2403.13804)|null|
|**2024-04-05**|**WaterVG: Waterway Visual Grounding based on Text-Guided Vision and mmWave Radar**|Runwei Guan et.al.|[2403.12686](http://arxiv.org/abs/2403.12686)|null|
|**2024-07-23**|**DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM**|Yixuan Wu et.al.|[2403.12488](http://arxiv.org/abs/2403.12488)|**[link](https://github.com/yixuan730/DetToolChain)**|
|**2024-10-27**|**Veagle: Advancements in Multimodal Representation Learning**|Rajat Chawla et.al.|[2403.08773](http://arxiv.org/abs/2403.08773)|**[link](https://github.com/superagi/veagle)**|
|**2024-03-13**|**SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention**|Feng Xiao et.al.|[2403.08182](http://arxiv.org/abs/2403.08182)|null|
|**2024-12-01**|**MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding**|Chun-Peng Chang et.al.|[2403.03077](http://arxiv.org/abs/2403.03077)|**[link](https://github.com/dfki-av/mikasa-3dvg)**|
|**2024-03-05**|**Detecting Concrete Visual Tokens for Multimodal Machine Translation**|Braeden Bowen et.al.|[2403.03075](http://arxiv.org/abs/2403.03075)|null|
|**2024-03-04**|**RegionGPT: Towards Region Understanding Vision Language Model**|Qiushan Guo et.al.|[2403.02330](http://arxiv.org/abs/2403.02330)|null|
|**2024-03-04**|**Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training**|David Wan et.al.|[2403.02325](http://arxiv.org/abs/2403.02325)|null|
|**2024-03-02**|**Adversarial Testing for Visual Grounding via Image-Aware Property Reduction**|Zhiyuan Chang et.al.|[2403.01118](http://arxiv.org/abs/2403.01118)|null|
|**2024-06-27**|**Towards Alleviating Text-to-Image Retrieval Hallucination for CLIP in Zero-shot Learning**|Hanyao Wang et.al.|[2402.18400](http://arxiv.org/abs/2402.18400)|null|
|**2024-07-12**|**ShapeLLM: Universal 3D Object Understanding for Embodied Interaction**|Zekun Qi et.al.|[2402.17766](http://arxiv.org/abs/2402.17766)|**[link](https://github.com/qizekun/ShapeLLM)**|
|**2024-07-21**|**OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web**|Raghav Kapoor et.al.|[2402.17553](http://arxiv.org/abs/2402.17553)|null|
|**2024-04-23**|**Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding**|Ailin Deng et.al.|[2402.15300](http://arxiv.org/abs/2402.15300)|**[link](https://github.com/d-ailin/clip-guided-decoding)**|
|**2024-06-06**|**The Revolution of Multimodal Large Language Models: A Survey**|Davide Caffagni et.al.|[2402.12451](http://arxiv.org/abs/2402.12451)|null|
|**2024-02-20**|**SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction**|Jie Xu et.al.|[2402.11792](http://arxiv.org/abs/2402.11792)|null|
|**2024-05-24**|**Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions**|Wenxuan Wang et.al.|[2402.11265](http://arxiv.org/abs/2402.11265)|**[link](https://github.com/rubics-xuan/ivg)**|
|**2024-05-29**|**LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition**|Jinyuan Li et.al.|[2402.09989](http://arxiv.org/abs/2402.09989)|**[link](https://github.com/jinyuanli0012/pgim)**|
|**2024-02-13**|**Pixel Sentence Representation Learning**|Chenghao Xiao et.al.|[2402.08183](http://arxiv.org/abs/2402.08183)|**[link](https://github.com/gowitheflow-1998/pixel-linguist)**|
|**2024-02-10**|**SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data**|Hsuan-Fu Wang et.al.|[2402.06959](http://arxiv.org/abs/2402.06959)|**[link](https://github.com/ShampooWang/SpeechCLIP_plus)**|
|**2024-10-13**|**ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling**|Siming Yan et.al.|[2402.06118](http://arxiv.org/abs/2402.06118)|**[link](https://github.com/amazon-science/vigor)**|
|**2024-02-08**|**Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model**|Hung-Chieh Fang et.al.|[2402.05819](http://arxiv.org/abs/2402.05819)|null|
|**2024-02-01**|**SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling**|Eileen Wang et.al.|[2402.00319](http://arxiv.org/abs/2402.00319)|null|
|**2024-02-18**|**Towards Unified Interactive Visual Grounding in The Wild**|Jie Xu et.al.|[2401.16699](http://arxiv.org/abs/2401.16699)|null|
|**2024-03-23**|**LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answering**|Yuhan Chen et.al.|[2401.15842](http://arxiv.org/abs/2401.15842)|null|
|**2024-01-24**|**ChatterBox: Multi-round Multimodal Referring and Grounding**|Yunjie Tian et.al.|[2401.13307](http://arxiv.org/abs/2401.13307)|**[link](https://github.com/sunsmarterjie/chatterbox)**|
|**2024-01-20**|**Unifying Visual and Vision-Language Tracking via Contrastive Learning**|Yinchao Ma et.al.|[2401.11228](http://arxiv.org/abs/2401.11228)|**[link](https://github.com/openspaceai/uvltrack)**|
|**2024-01-18**|**SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**|Yang Zhan et.al.|[2401.09712](http://arxiv.org/abs/2401.09712)|**[link](https://github.com/zhanyang-nwpu/skyeyegpt)**|
|**2024-09-24**|**SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding**|Baoxiong Jia et.al.|[2401.09340](http://arxiv.org/abs/2401.09340)|null|
|**2024-02-15**|**Uncovering the Full Potential of Visual Grounding Methods in VQA**|Daniel Reich et.al.|[2401.07803](http://arxiv.org/abs/2401.07803)|**[link](https://github.com/dreichcsl/truevg)**|
|**2024-04-25**|**Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs**|Shengbang Tong et.al.|[2401.06209](http://arxiv.org/abs/2401.06209)|**[link](https://github.com/tsb0601/MMVP)**|
|**2024-01-05**|**An Open and Comprehensive Pipeline for Unified Object Grounding and Detection**|Xiangyu Zhao et.al.|[2401.02361](http://arxiv.org/abs/2401.02361)|**[link](https://github.com/open-mmlab/mmdetection)**|
|**2024-05-14**|**Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers**|Aleksandar Stanić et.al.|[2401.01974](http://arxiv.org/abs/2401.01974)|null|
|**2024-07-06**|**Bridging Modality Gap for Visual Grounding with Effecitve Cross-modal Distillation**|Jiaxi Wang et.al.|[2312.17648](http://arxiv.org/abs/2312.17648)|null|
|**2024-07-11**|**One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts**|Ziheng Zhao et.al.|[2312.17183](http://arxiv.org/abs/2312.17183)|**[link](https://github.com/zhaoziheng/sat-ds)**|
|**2023-12-23**|**Cycle-Consistency Learning for Captioning and Grounding**|Ning Wang et.al.|[2312.15162](http://arxiv.org/abs/2312.15162)|null|
|**2023-12-22**|**GroundVLP: Harnessing Zero-shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection**|Haozhan Shen et.al.|[2312.15043](http://arxiv.org/abs/2312.15043)|**[link](https://github.com/om-ai-lab/groundvlp)**|
|**2023-12-21**|**Compositional Zero-Shot Learning for Attribute-Based Object Reference in Human-Robot Interaction**|Peng Gao et.al.|[2312.13655](http://arxiv.org/abs/2312.13655)|null|
|**2024-03-25**|**Mask Grounding for Referring Image Segmentation**|Yong Xien Chng et.al.|[2312.12198](http://arxiv.org/abs/2312.12198)|**[link](https://github.com/yxchng/mask-grounding)**|
|**2023-12-19**|**Context Disentangling and Prototype Inheriting for Robust Visual Grounding**|Wei Tang et.al.|[2312.11967](http://arxiv.org/abs/2312.11967)|**[link](https://github.com/waynetomas/transcp)**|
|**2024-08-30**|**Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment**|Xiaoxu Xu et.al.|[2312.09625](http://arxiv.org/abs/2312.09625)|null|
|**2023-12-13**|**Mono3DVG: 3D Visual Grounding in Monocular Images**|Yang Zhan et.al.|[2312.08022](http://arxiv.org/abs/2312.08022)|**[link](https://github.com/zhanyang-nwpu/mono3dvg)**|
|**2024-03-21**|**Unveiling Parts Beyond Objects:Towards Finer-Granularity Referring Expression Segmentation**|Wenxuan Wang et.al.|[2312.08007](http://arxiv.org/abs/2312.08007)|**[link](https://github.com/rubics-xuan/mres)**|
|**2023-12-08**|**Visual Grounding of Whole Radiology Reports for 3D CT Images**|Akimichi Ichinose et.al.|[2312.04794](http://arxiv.org/abs/2312.04794)|null|
|**2023-12-07**|**Improved Visual Grounding through Self-Consistent Explanations**|Ruozhen He et.al.|[2312.04554](http://arxiv.org/abs/2312.04554)|null|
|**2024-07-17**|**Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment**|Brian Gordon et.al.|[2312.03766](http://arxiv.org/abs/2312.03766)|**[link](https://github.com/mismatchquest/mismatchquest)**|
|**2023-12-06**|**GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models**|Haicheng Liao et.al.|[2312.03543](http://arxiv.org/abs/2312.03543)|**[link](https://github.com/petrichor625/talk2car_cavg)**|
|**2023-12-05**|**Uni3DL: Unified Model for 3D and Language Understanding**|Xiang Li et.al.|[2312.03026](http://arxiv.org/abs/2312.03026)|null|
|**2023-12-05**|**Visually Grounded Language Learning: a review of language games, datasets, tasks, and models**|Alessandro Suglia et.al.|[2312.02431](http://arxiv.org/abs/2312.02431)|null|
|**2024-06-12**|**Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models**|Andrés Villa et.al.|[2312.02219](http://arxiv.org/abs/2312.02219)|**[link](https://github.com/ojedaf/merlim)**|
|**2023-12-04**|**Aligning and Prompting Everything All at Once for Universal Visual Perception**|Yunhang Shen et.al.|[2312.02153](http://arxiv.org/abs/2312.02153)|**[link](https://github.com/shenyunhang/ape)**|
|**2023-12-04**|**Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object Detection**|Sunghun Kang et.al.|[2312.02103](http://arxiv.org/abs/2312.02103)|null|
|**2024-01-09**|**Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment**|Cong-Duy Nguyen et.al.|[2312.01592](http://arxiv.org/abs/2312.01592)|null|
|**2024-10-24**|**G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training**|Che Liu et.al.|[2312.01522](http://arxiv.org/abs/2312.01522)|**[link](https://github.com/cheliu-computation/g2d-neurips24)**|
|**2024-04-09**|**Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions**|Zeyu Han et.al.|[2311.17048](http://arxiv.org/abs/2311.17048)|**[link](https://github.com/show-han/zeroshot_rec)**|
|**2024-08-11**|**Context-Aware Indoor Point Cloud Object Generation through User Instructions**|Yiyang Luo et.al.|[2311.16501](http://arxiv.org/abs/2311.16501)|null|
|**2023-11-24**|**GeoChat: Grounded Large Vision-Language Model for Remote Sensing**|Kartik Kuckreja et.al.|[2311.15826](http://arxiv.org/abs/2311.15826)|**[link](https://github.com/mbzuai-oryx/geochat)**|
|**2024-03-23**|**Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding**|Zhihao Yuan et.al.|[2311.15383](http://arxiv.org/abs/2311.15383)|**[link](https://github.com/CurryYuan/ZSVG3D)**|
|**2023-11-25**|**Continual Referring Expression Comprehension via Dual Modular Memorization**|Heng Tao Shen et.al.|[2311.14909](http://arxiv.org/abs/2311.14909)|**[link](https://github.com/zackschen/DMM)**|
|**2024-04-26**|**Enhancing Visual Grounding and Generalization: A Multi-Task Cycle Training Approach for Vision-Language Models**|Xiaoyu Yang et.al.|[2311.12327](http://arxiv.org/abs/2311.12327)|**[link](https://github.com/anonymgiant/vilam)**|
|**2023-12-06**|**InfMLLM: A Unified Framework for Visual-Language Tasks**|Qiang Zhou et.al.|[2311.06791](http://arxiv.org/abs/2311.06791)|**[link](https://github.com/mightyzau/infmllm)**|
|**2023-11-09**|**Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter**|Georgios Tziafas et.al.|[2311.05779](http://arxiv.org/abs/2311.05779)|**[link](https://github.com/gtziafas/ocid-vlg)**|
|**2023-11-08**|**GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs**|Zhenfang Chen et.al.|[2311.04901](http://arxiv.org/abs/2311.04901)|null|
|**2023-12-18**|**NExT-Chat: An LMM for Chat, Detection and Segmentation**|Ao Zhang et.al.|[2311.04498](http://arxiv.org/abs/2311.04498)|**[link](https://github.com/next-chatv/next-chat)**|
|**2024-06-02**|**GLaMM: Pixel Grounding Large Multimodal Model**|Hanoona Rasheed et.al.|[2311.03356](http://arxiv.org/abs/2311.03356)|**[link](https://github.com/mbzuai-oryx/groundingLMM)**|
|**2023-11-06**|**CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding**|Junyan Li et.al.|[2311.03354](http://arxiv.org/abs/2311.03354)|null|
|**2024-04-16**|**GPT-4V-AD: Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection**|Jiangning Zhang et.al.|[2311.02612](http://arxiv.org/abs/2311.02612)|**[link](https://github.com/zhangzjn/gpt-4v-ad)**|
|**2023-11-02**|**MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition**|Guangyue Xu et.al.|[2311.01580](http://arxiv.org/abs/2311.01580)|null|
|**2024-01-30**|**A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis**|Yingshu Li et.al.|[2310.20381](http://arxiv.org/abs/2310.20381)|null|
|**2023-10-30**|**Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction**|Zexu Pan et.al.|[2310.19644](http://arxiv.org/abs/2310.19644)|null|
|**2023-10-28**|**CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data**|Taiki Miyanishi et.al.|[2310.18773](http://arxiv.org/abs/2310.18773)|**[link](https://github.com/atr-dbi/cityrefer)**|
|**2023-10-26**|**GROOViST: A Metric for Grounding Objects in Visual Storytelling**|Aditya K Surikuchi et.al.|[2310.17770](http://arxiv.org/abs/2310.17770)|**[link](https://github.com/akskuchi/groovist)**|
|**2023-10-25**|**Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network**|Yiming Lin et.al.|[2310.16616](http://arxiv.org/abs/2310.16616)|null|
|**2023-10-25**|**Video Referring Expression Comprehension via Transformer with Content-conditioned Query**|Ji Jiang et.al.|[2310.16402](http://arxiv.org/abs/2310.16402)|null|
|**2023-10-24**|**Visually Grounded Continual Language Learning with Selective Specialization**|Kyra Ahrens et.al.|[2310.15571](http://arxiv.org/abs/2310.15571)|**[link](https://github.com/ky-ah/selective-lilac)**|
|**2023-10-22**|**OV-VG: A Benchmark for Open-Vocabulary Visual Grounding**|Chunlei Wang et.al.|[2310.14374](http://arxiv.org/abs/2310.14374)|**[link](https://github.com/cv516buaa/ov-vg)**|
|**2023-10-21**|**On the Transferability of Visually Grounded PCFGs**|Yanpeng Zhao et.al.|[2310.14107](http://arxiv.org/abs/2310.14107)|**[link](https://github.com/zhaoyanpeng/cpcfg)**|
|**2024-03-25**|**Visual Grounding Helps Learn Word Meanings in Low-Data Regimes**|Chengxu Zhuang et.al.|[2310.13257](http://arxiv.org/abs/2310.13257)|**[link](https://github.com/EvLab-MIT/LexiContrastiveGrd)**|
|**2023-10-18**|**InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot Interactions**|Hanbo Zhang et.al.|[2310.12147](http://arxiv.org/abs/2310.12147)|null|
|**2023-11-06**|**Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V**|Jianwei Yang et.al.|[2310.11441](http://arxiv.org/abs/2310.11441)|**[link](https://github.com/microsoft/SoM)**|
|**2023-10-23**|**NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning**|Haowei Wang et.al.|[2310.10975](http://arxiv.org/abs/2310.10975)|**[link](https://github.com/mr-neko/nice)**|
|**2023-11-11**|**Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms**|Seungju Han et.al.|[2310.10418](http://arxiv.org/abs/2310.10418)|**[link](https://github.com/wade3han/normlens)**|
|**2023-11-07**|**MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning**|Jun Chen et.al.|[2310.09478](http://arxiv.org/abs/2310.09478)|null|
|**2024-03-08**|**From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models**|Dongsheng Jiang et.al.|[2310.08825](http://arxiv.org/abs/2310.08825)|**[link](https://github.com/yuchenliu98/comm)**|
|**2023-10-11**|**Audio-Visual Neural Syntax Acquisition**|Cheng-I Jeff Lai et.al.|[2310.07654](http://arxiv.org/abs/2310.07654)|null|
|**2024-10-05**|**CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding**|Eslam Abdelrahman et.al.|[2310.06214](http://arxiv.org/abs/2310.06214)|**[link](https://github.com/eslambakr/CoT3D_VG)**|
|**2024-04-02**|**Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models**|Archiki Prasad et.al.|[2310.05861](http://arxiv.org/abs/2310.05861)|**[link](https://github.com/archiki/repare)**|
|**2023-10-08**|**Lightweight In-Context Tuning for Multimodal Unified Models**|Yixin Chen et.al.|[2310.05109](http://arxiv.org/abs/2310.05109)|null|
|**2023-10-17**|**ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks**|Zejun Li et.al.|[2310.02569](http://arxiv.org/abs/2310.02569)|**[link](https://github.com/fudandisc/reform-eval)**|
|**2023-10-28**|**Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering**|Weizhe Lin et.al.|[2309.17133](http://arxiv.org/abs/2309.17133)|**[link](https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering)**|
|**2023-09-23**|**Resolving References in Visually-Grounded Dialogue via Text Generation**|Bram Willemsen et.al.|[2309.13430](http://arxiv.org/abs/2309.13430)|**[link](https://github.com/willemsenbram/reference-resolution-via-text-generation)**|
|**2023-09-21**|**LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent**|Jianing Yang et.al.|[2309.12311](http://arxiv.org/abs/2309.12311)|**[link](https://github.com/sled-group/chat-with-nerf)**|
|**2023-09-21**|**Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill**|Wenzhe Cai et.al.|[2309.10309](http://arxiv.org/abs/2309.10309)|**[link](https://github.com/wzcai99/pixel-navigator)**|
|**2023-09-18**|**Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection**|Chenming Zhu et.al.|[2309.09456](http://arxiv.org/abs/2309.09456)|null|
|**2024-04-05**|**PROGrasp: Pragmatic Human-Robot Communication for Object Grasping**|Gi-Cheon Kang et.al.|[2309.07759](http://arxiv.org/abs/2309.07759)|**[link](https://github.com/gicheonkang/prograsp)**|
|**2023-09-14**|**VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue**|Yunshui Li et.al.|[2309.07387](http://arxiv.org/abs/2309.07387)|null|
|**2023-09-11**|**Multi3DRefer: Grounding Text Description to Multiple 3D Objects**|Yiming Zhang et.al.|[2309.05251](http://arxiv.org/abs/2309.05251)|null|
|**2023-09-10**|**Collecting Visually-Grounded Dialogue with A Game Of Sorts**|Bram Willemsen et.al.|[2309.05162](http://arxiv.org/abs/2309.05162)|**[link](https://github.com/willemsenbram/a-game-of-sorts)**|
|**2023-09-08**|**Leveraging Pretrained Image-text Models for Improving Audio-Visual Learning**|Saurabhchand Bhati et.al.|[2309.04628](http://arxiv.org/abs/2309.04628)|null|
|**2024-07-16**|**Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding**|Ozan Unal et.al.|[2309.04561](http://arxiv.org/abs/2309.04561)|null|
|**2024-05-03**|**Compositional Learning of Visually-Grounded Concepts Using Reinforcement**|Zijun Lin et.al.|[2309.04504](http://arxiv.org/abs/2309.04504)|**[link](https://github.com/haidiazaman/rl-concept-learning-project)**|
|**2023-09-07**|**Interpretable Visual Question Answering via Reasoning Supervision**|Maria Parelli et.al.|[2309.03726](http://arxiv.org/abs/2309.03726)|null|
|**2023-09-07**|**DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners**|Clarence Lee et.al.|[2309.03483](http://arxiv.org/abs/2309.03483)|**[link](https://github.com/clarence-lee-sheng/determinet)**|
|**2024-03-30**|**Can I Trust Your Answer? Visually Grounded Video Question Answering**|Junbin Xiao et.al.|[2309.01327](http://arxiv.org/abs/2309.01327)|**[link](https://github.com/doc-doc/next-gqa)**|
|**2024-01-23**|**VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders**|Xuyang Liu et.al.|[2309.01141](http://arxiv.org/abs/2309.01141)|**[link](https://github.com/xuyang-liu16/vgdiffzero)**|
|**2023-08-31**|**FACET: Fairness in Computer Vision Evaluation Benchmark**|Laura Gustafson et.al.|[2309.00035](http://arxiv.org/abs/2309.00035)|null|
|**2024-08-27**|**Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations**|Kilichbek Haydarov et.al.|[2308.16349](http://arxiv.org/abs/2308.16349)|null|
|**2023-12-24**|**GREC: Generalized Referring Expression Comprehension**|Shuting He et.al.|[2308.16182](http://arxiv.org/abs/2308.16182)|**[link](https://github.com/henghuiding/grefcoco)**|
|**2023-08-31**|**WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model**|Tianyu Wang et.al.|[2308.15962](http://arxiv.org/abs/2308.15962)|null|
|**2023-10-13**|**Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond**|Jinze Bai et.al.|[2308.12966](http://arxiv.org/abs/2308.12966)|**[link](https://github.com/qwenlm/qwen-vl)**|
|**2023-08-24**|**HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks**|Zichao Dong et.al.|[2308.12537](http://arxiv.org/abs/2308.12537)|**[link](https://github.com/dzcgaara/HuBo-VLM)**|
|**2023-10-30**|**RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D**|Shuhei Kurita et.al.|[2308.12035](http://arxiv.org/abs/2308.12035)|**[link](https://github.com/shuheikurita/refego)**|
|**2023-11-20**|**A Unified Framework for 3D Point Cloud Visual Grounding**|Haojia Lin et.al.|[2308.11887](http://arxiv.org/abs/2308.11887)|**[link](https://github.com/leon1207/3dreftr)**|
|**2023-08-24**|**VQA Therapy: Exploring Answer Differences by Visually Grounding Answers**|Chongyan Chen et.al.|[2308.11662](http://arxiv.org/abs/2308.11662)|**[link](https://github.com/ccychongyanchen/vqatherapycrowdsourcing)**|
|**2023-12-14**|**Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation**|Yifei Su et.al.|[2308.11561](http://arxiv.org/abs/2308.11561)|**[link](https://github.com/yifeisu/tg-gat)**|
|**2023-08-19**|**Whether you can locate or not? Interactive Referring Expression Generation**|Fulong Ye et.al.|[2308.09977](http://arxiv.org/abs/2308.09977)|**[link](https://github.com/superhero-7/ireg)**|
|**2023-08-18**|**Language-Guided Diffusion Model for Visual Grounding**|Sijia Chen et.al.|[2308.09599](http://arxiv.org/abs/2308.09599)|**[link](https://github.com/iqua/vggbase)**|
|**2024-04-22**|**Learning the meanings of function words from grounded language using a visual question answering model**|Eva Portelance et.al.|[2308.08628](http://arxiv.org/abs/2308.08628)|**[link](https://github.com/evaportelance/vqa-function-word-learning)**|
|**2024-02-11**|**Detecting and Preventing Hallucinations in Large Vision Language Models**|Anisha Gunjal et.al.|[2308.06394](http://arxiv.org/abs/2308.06394)|**[link](https://github.com/hendryx-scale/mhal-detect)**|
|**2023-08-08**|**3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment**|Ziyu Zhu et.al.|[2308.04352](http://arxiv.org/abs/2308.04352)|null|
|**2023-08-02**|**ADS-Cap: A Framework for Accurate and Diverse Stylized Captioning with Unpaired Stylistic Corpora**|Kanzhi Cheng et.al.|[2308.01143](http://arxiv.org/abs/2308.01143)|**[link](https://github.com/njucckevin/ads-cap)**|
|**2023-08-01**|**VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes**|Yuhao Lu et.al.|[2308.00640](http://arxiv.org/abs/2308.00640)|**[link](https://github.com/luyh20/vl-grasp)**|
|**2023-07-28**|**'What are you referring to?' Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges**|Javier Chiyah-Garcia et.al.|[2307.15554](http://arxiv.org/abs/2307.15554)|**[link](https://github.com/jchiyah/what-are-you-referring-to)**|
|**2023-07-25**|**3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding**|Zehan Wang et.al.|[2307.13363](http://arxiv.org/abs/2307.13363)|null|
|**2023-10-11**|**Described Object Detection: Liberating Object Detection with Flexible Expressions**|Chi Xie et.al.|[2307.12813](http://arxiv.org/abs/2307.12813)|**[link](https://github.com/shikras/d-cube)**|
|**2023-07-23**|**Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision**|Menghao Li et.al.|[2307.12392](http://arxiv.org/abs/2307.12392)|**[link](https://github.com/cv516buaa/ir-vg)**|
|**2023-07-21**|**Advancing Visual Grounding with Scene Knowledge: Benchmark and Method**|Zhihong Chen et.al.|[2307.11558](http://arxiv.org/abs/2307.11558)|**[link](https://github.com/zhjohnchan/sk-vg)**|
|**2023-07-18**|**Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding**|Zehan Wang et.al.|[2307.09267](http://arxiv.org/abs/2307.09267)|**[link](https://github.com/ZzZZCHS/WS-3DVG)**|
|**2023-07-17**|**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**|Yang Zhao et.al.|[2307.08581](http://arxiv.org/abs/2307.08581)|null|
|**2023-07-14**|**Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks**|Ryosuke Korekata et.al.|[2307.07166](http://arxiv.org/abs/2307.07166)|null|
|**2023-07-12**|**GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation**|Junghyun Kim et.al.|[2307.05963](http://arxiv.org/abs/2307.05963)|**[link](https://github.com/JHKim-snu/GVCCI)**|
|**2023-07-12**|**OG: Equip vision occupancy with instance segmentation and visual grounding**|Zichao Dong et.al.|[2307.05873](http://arxiv.org/abs/2307.05873)|null|
|**2024-04-28**|**Origami Single-end Capacitive Sensing for Continuous Shape Estimation of Morphing Structures**|Lala Shakti Swarup Ray et.al.|[2307.05370](http://arxiv.org/abs/2307.05370)|null|
|**2024-01-31**|**What Do Self-Supervised Speech Models Know About Words?**|Ankita Pasad et.al.|[2307.00162](http://arxiv.org/abs/2307.00162)|**[link](https://github.com/ankitapasad/layerwise-analysis)**|
|**2023-07-13**|**Kosmos-2: Grounding Multimodal Large Language Models to the World**|Zhiliang Peng et.al.|[2306.14824](http://arxiv.org/abs/2306.14824)|**[link](https://github.com/microsoft/unilm/tree/master/kosmos-2)**|
|**2023-06-26**|**Learning with Difference Attention for Visually Grounded Self-supervised Representations**|Aishwarya Agarwal et.al.|[2306.14603](http://arxiv.org/abs/2306.14603)|null|
|**2023-06-25**|**Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input**|Qingpei Guo et.al.|[2306.14182](http://arxiv.org/abs/2306.14182)|null|
|**2024-04-18**|**Visually grounded few-shot word learning in low-resource settings**|Leanne Nortje et.al.|[2306.11371](http://arxiv.org/abs/2306.11371)|null|
|**2023-06-19**|**A neuro-symbolic approach for multimodal reference expression comprehension**|Aman Jain et.al.|[2306.10717](http://arxiv.org/abs/2306.10717)|null|
|**2023-06-14**|**World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models**|Ziqiao Ma et.al.|[2306.08685](http://arxiv.org/abs/2306.08685)|**[link](https://github.com/sled-group/world-to-words)**|
|**2023-06-10**|**Referring to Screen Texts with Voice Assistants**|Shruti Bhargava et.al.|[2306.07298](http://arxiv.org/abs/2306.07298)|null|
|**2023-06-06**|**Language Adaptive Weight Generation for Multi-task Visual Grounding**|Wei Su et.al.|[2306.04652](http://arxiv.org/abs/2306.04652)|**[link](https://github.com/dcdcvgroup/vglaw-mindspore)**|
|**2023-10-16**|**Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards**|Alexandre Ramé et.al.|[2306.04488](http://arxiv.org/abs/2306.04488)|**[link](https://github.com/alexrame/rewardedsoups)**|
|**2023-06-06**|**Referring Expression Comprehension Using Language Adaptive Inference**|Wei Su et.al.|[2306.04451](http://arxiv.org/abs/2306.04451)|null|
|**2023-06-05**|**Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System**|Khazar Khorrami et.al.|[2306.02972](http://arxiv.org/abs/2306.02972)|null|
|**2023-06-04**|**Leverage Points in Modality Shifts: Comparing Language-only and Multimodal Word Representations**|Aleksey Tikhonov et.al.|[2306.02348](http://arxiv.org/abs/2306.02348)|**[link](https://github.com/altsoph/modality_shifts)**|
|**2023-05-31**|**Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind**|Ece Takmaz et.al.|[2305.19933](http://arxiv.org/abs/2305.19933)|**[link](https://github.com/nicofirst1/speaker-adaptation)**|
|**2023-05-30**|**Wave to Syntax: Probing spoken language models for syntax**|Gaofei Shen et.al.|[2305.18957](http://arxiv.org/abs/2305.18957)|**[link](https://github.com/techsword/wave-to-syntax)**|
|**2023-05-30**|**Graph Neural Networks for Contextual ASR with the Tree-Constrained Pointer Generator**|Guangzhi Sun et.al.|[2305.18824](http://arxiv.org/abs/2305.18824)|**[link](https://github.com/briansidp/espnet)**|
|**2023-05-27**|**Benchmarking Diverse-Modal Entity Linking with Generative Models**|Sijia Wang et.al.|[2305.17337](http://arxiv.org/abs/2305.17337)|null|
|**2023-05-15**|**Semantic Composition in Visually Grounded Language Models**|Rohan Pandey et.al.|[2305.16328](http://arxiv.org/abs/2305.16328)|null|
|**2023-05-25**|**Visually grounded few-shot word acquisition with fewer shots**|Leanne Nortje et.al.|[2305.15937](http://arxiv.org/abs/2305.15937)|null|
|**2023-05-25**|**Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving**|Wenhao Cheng et.al.|[2305.15765](http://arxiv.org/abs/2305.15765)|null|
|**2023-10-14**|**Measuring Faithful and Plausible Visual Grounding in VQA**|Daniel Reich et.al.|[2305.15015](http://arxiv.org/abs/2305.15015)|**[link](https://github.com/dreichcsl/fpvg)**|
|**2024-02-06**|**An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics**|Saba Ahmadi et.al.|[2305.14998](http://arxiv.org/abs/2305.14998)|**[link](https://github.com/saba96/img-cap-metrics-robustness)**|
|**2024-02-07**|**Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans**|Taiki Miyanishi et.al.|[2305.13876](http://arxiv.org/abs/2305.13876)|**[link](https://github.com/atr-dbi/cross3dvg)**|
|**2023-05-19**|**TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding**|Chenchi Zhang et.al.|[2305.11497](http://arxiv.org/abs/2305.11497)|null|
|**2023-07-23**|**Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model**|Puyuan Peng et.al.|[2305.11435](http://arxiv.org/abs/2305.11435)|**[link](https://github.com/jasonppy/syllable-discovery)**|
|**2023-05-18**|**ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities**|Peng Wang et.al.|[2305.11172](http://arxiv.org/abs/2305.11172)|**[link](https://github.com/OFA-Sys/ONE-PEACE)**|
|**2023-05-18**|**Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding**|Taolin Zhang et.al.|[2305.10714](http://arxiv.org/abs/2305.10714)|null|
|**2024-11-19**|**CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding**|Linhui Xiao et.al.|[2305.08685](http://arxiv.org/abs/2305.08685)|**[link](https://github.com/linhuixiao/clip-vg)**|
|**2023-08-12**|**Sample-Specific Debiasing for Better Image-Text Models**|Peiqi Wang et.al.|[2304.13181](http://arxiv.org/abs/2304.13181)|null|
|**2023-04-20**|**Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining**|Qin Chao et.al.|[2304.10311](http://arxiv.org/abs/2304.10311)|null|
|**2023-06-19**|**Grounding Classical Task Planners via Vision-Language Models**|Xiaohan Zhang et.al.|[2304.08587](http://arxiv.org/abs/2304.08587)|null|
|**2023-08-18**|**What does CLIP know about a red circle? Visual prompt engineering for VLMs**|Aleksandar Shtedritski et.al.|[2304.06712](http://arxiv.org/abs/2304.06712)|null|
|**2024-07-15**|**WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language**|Zhenxiang Lin et.al.|[2304.05645](http://arxiv.org/abs/2304.05645)|**[link](https://github.com/4dvlab/wildrefer)**|
|**2024-03-29**|**VicTR: Video-conditioned Text Representations for Activity Recognition**|Kumara Kahatapitiya et.al.|[2304.02560](http://arxiv.org/abs/2304.02560)|null|
|**2023-03-30**|**Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples**|Hyeonggon Ryu et.al.|[2303.17517](http://arxiv.org/abs/2303.17517)|null|
|**2023-12-05**|**ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance**|Zoey Guo et.al.|[2303.16894](http://arxiv.org/abs/2303.16894)|**[link](https://github.com/ivan-tang-3d/viewrefer3d)**|
|**2023-03-23**|**NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations**|Joy Hsu et.al.|[2303.13483](http://arxiv.org/abs/2303.13483)|null|
|**2023-03-23**|**ScanERU: Interactive 3D Visual Grounding based on Embodied Reference Understanding**|Ziyang Lu et.al.|[2303.13186](http://arxiv.org/abs/2303.13186)|**[link](https://github.com/mrlearnedtoad/scaneru)**|
|**2023-03-21**|**Joint Visual Grounding and Tracking with Natural Language Specification**|Li Zhou et.al.|[2303.12027](http://arxiv.org/abs/2303.12027)|**[link](https://github.com/lizhou-cs/jointnlt)**|
|**2023-03-19**|**FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering**|Weizhe Lin et.al.|[2303.10699](http://arxiv.org/abs/2303.10699)|null|
|**2023-03-14**|**Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment**|Zhihao Chen et.al.|[2303.07618](http://arxiv.org/abs/2303.07618)|null|
|**2023-03-14**|**Parallel Vertex Diffusion for Unified Visual Grounding**|Zesen Cheng et.al.|[2303.07216](http://arxiv.org/abs/2303.07216)|null|
|**2023-08-17**|**Universal Instance Perception as Object Discovery and Retrieval**|Bin Yan et.al.|[2303.06674](http://arxiv.org/abs/2303.06674)|**[link](https://github.com/MasterBin-IIAU/UNINEXT)**|
|**2023-05-17**|**Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos**|Teng Wang et.al.|[2303.06378](http://arxiv.org/abs/2303.06378)|**[link](https://github.com/zjr2000/gvl)**|
|**2024-07-19**|**Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection**|Shilong Liu et.al.|[2303.05499](http://arxiv.org/abs/2303.05499)|**[link](https://github.com/idea-research/groundingdino)**|
|**2023-02-24**|**Language-Driven Representation Learning for Robotics**|Siddharth Karamcheti et.al.|[2302.12766](http://arxiv.org/abs/2302.12766)|**[link](https://github.com/siddk/voltron-evaluation)**|
|**2024-10-31**|**A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter**|Kechun Xu et.al.|[2302.12610](http://arxiv.org/abs/2302.12610)|**[link](https://github.com/xukechun/Vision-Language-Grasping)**|
|**2023-09-25**|**HL Dataset: Visually-grounded Description of Scenes, Actions and Rationales**|Michele Cafagna et.al.|[2302.12189](http://arxiv.org/abs/2302.12189)|**[link](https://github.com/michelecafagna26/hl-dataset)**|
|**2023-02-22**|**Focusing On Targets For Improving Weakly Supervised Visual Grounding**|Viet-Quoc Pham et.al.|[2302.11252](http://arxiv.org/abs/2302.11252)|null|
|**2023-02-17**|**CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension**|Zhi Zhang et.al.|[2302.09027](http://arxiv.org/abs/2302.09027)|**[link](https://github.com/fightingfighting/ck-transformer)**|
|**2023-02-01**|**Visually Grounded Keyword Detection and Localisation for Low-Resource Languages**|Kayode Kolawole Olaleye et.al.|[2302.00765](http://arxiv.org/abs/2302.00765)|null|
|**2023-06-13**|**Grounding Language Models to Images for Multimodal Inputs and Outputs**|Jing Yu Koh et.al.|[2301.13823](http://arxiv.org/abs/2301.13823)|**[link](https://github.com/kohjingyu/fromage)**|
|**2023-02-20**|**Champion Solution for the WSDM2023 Toloka VQA Challenge**|Shengyi Gao et.al.|[2301.09045](http://arxiv.org/abs/2301.09045)|**[link](https://github.com/czczup/vit-adapter)**|
|**2023-01-20**|**Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences**|Xudong Hong et.al.|[2301.08571](http://arxiv.org/abs/2301.08571)|null|
|**2023-06-07**|**Position-guided Text Prompt for Vision-Language Pre-training**|Alex Jinpeng Wang et.al.|[2212.09737](http://arxiv.org/abs/2212.09737)|**[link](https://github.com/sail-sg/ptp)**|
|**2023-03-09**|**Using Multiple Instance Learning to Build Multimodal Representations**|Peiqi Wang et.al.|[2212.05561](http://arxiv.org/abs/2212.05561)|null|
|**2022-09-15**|**Can Offline Reinforcement Learning Help Natural Language Understanding?**|Ziqi Zhang et.al.|[2212.03864](http://arxiv.org/abs/2212.03864)|null|
|**2022-12-01**|**Focus! Relevant and Sufficient Context Selection for News Image Captioning**|Mingyang Zhou et.al.|[2212.00843](http://arxiv.org/abs/2212.00843)|null|
|**2022-12-01**|**UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2212.00836](http://arxiv.org/abs/2212.00836)|null|
|**2022-11-30**|**DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding**|Shilong Liu et.al.|[2211.15516](http://arxiv.org/abs/2211.15516)|**[link](https://github.com/idea-research/dq-detr)**|
|**2022-11-27**|**MNER-QG: An End-to-End MRC framework for Multimodal Named Entity Recognition with Query Grounding**|Meihuizi Jia et.al.|[2211.14739](http://arxiv.org/abs/2211.14739)|null|
|**2022-11-25**|**Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding**|Eslam Mohamed Bakr et.al.|[2211.14241](http://arxiv.org/abs/2211.14241)|**[link](https://github.com/eslambakr/LAR-Look-Around-and-Refer)**|
|**2023-03-25**|**Visually Grounded Commonsense Knowledge Acquisition**|Yuan Yao et.al.|[2211.12054](http://arxiv.org/abs/2211.12054)|**[link](https://github.com/thunlp/clever)**|
|**2022-11-15**|**Visually Grounded VQA by Lattice-based Retrieval**|Daniel Reich et.al.|[2211.08086](http://arxiv.org/abs/2211.08086)|**[link](https://github.com/dreichCSL/GQA_generalization_splits)**|
|**2022-11-15**|**YORO -- Lightweight End to End Visual Grounding**|Chih-Hui Ho et.al.|[2211.07912](http://arxiv.org/abs/2211.07912)|**[link](https://github.com/chihhuiho/yoro)**|
|**2023-03-25**|**Instruction-Following Agents with Multimodal Transformer**|Hao Liu et.al.|[2210.13431](http://arxiv.org/abs/2210.13431)|**[link](https://github.com/lhao499/instructrl)**|
|**2022-10-24**|**Are Current Decoding Strategies Capable of Facing the Challenges of Visual Dialogue?**|Amit Kumar Chaudhary et.al.|[2210.12997](http://arxiv.org/abs/2210.12997)|null|
|**2022-10-23**|**Towards Pragmatic Production Strategies for Natural Language Generation Tasks**|Mario Giulianelli et.al.|[2210.12828](http://arxiv.org/abs/2210.12828)|null|
|**2022-10-23**|**RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing Data**|Yang Zhan et.al.|[2210.12634](http://arxiv.org/abs/2210.12634)|**[link](https://github.com/zhanyang-nwpu/rsvg-pytorch)**|
|**2022-10-22**|**A Visual Tour Of Current Challenges In Multimodal Language Models**|Shashank Sonkar et.al.|[2210.12565](http://arxiv.org/abs/2210.12565)|null|
|**2023-06-09**|**Learning Point-Language Hierarchical Alignment for 3D Visual Grounding**|Jiaming Chen et.al.|[2210.12513](http://arxiv.org/abs/2210.12513)|**[link](https://github.com/ppjmchen/ham)**|
|**2022-10-19**|**TOIST: Task Oriented Instance Segmentation Transformer with Noun-Pronoun Distillation**|Pengfei Li et.al.|[2210.10775](http://arxiv.org/abs/2210.10775)|**[link](https://github.com/air-discover/toist)**|
|**2022-10-17**|**Vision-Language Pre-training: Basics, Recent Advances, and Future Trends**|Zhe Gan et.al.|[2210.09263](http://arxiv.org/abs/2210.09263)|**[link](https://github.com/computer-vision-in-the-wild/cvinw_readings)**|
|**2023-02-13**|**Like a bilingual baby: The advantage of visually grounding a bilingual language model**|Khai-Nguyen Nguyen et.al.|[2210.05487](http://arxiv.org/abs/2210.05487)|null|
|**2022-10-12**|**YFACC: A Yorùbá speech-image dataset for cross-lingual keyword localisation through visual grounding**|Kayode Olaleye et.al.|[2210.04600](http://arxiv.org/abs/2210.04600)|null|
|**2023-06-14**|**MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning**|Zijia Zhao et.al.|[2210.04183](http://arxiv.org/abs/2210.04183)|null|
|**2023-10-30**|**VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment**|Shraman Pramanick et.al.|[2210.04135](http://arxiv.org/abs/2210.04135)|**[link](https://github.com/ShramanPramanick/VoLTA)**|
|**2022-10-07**|**Learning a Visually Grounded Memory Assistant**|Meera Hahn et.al.|[2210.03787](http://arxiv.org/abs/2210.03787)|null|
|**2022-10-07**|**Detailed Annotations of Chest X-Rays via CT Projection for Report Understanding**|Constantin Seibold et.al.|[2210.03416](http://arxiv.org/abs/2210.03416)|null|
|**2023-04-17**|**A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning**|Aishwarya Kamath et.al.|[2210.03112](http://arxiv.org/abs/2210.03112)|null|
|**2022-10-06**|**Video Referring Expression Comprehension via Transformer with Content-aware Query**|Ji Jiang et.al.|[2210.02953](http://arxiv.org/abs/2210.02953)|null|
|**2022-10-05**|**GLAD: Grounded Layered Autonomous Driving for Complex Service Tasks**|Yan Ding et.al.|[2210.02302](http://arxiv.org/abs/2210.02302)|null|
|**2023-05-07**|**Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach**|Georgios Tziafas et.al.|[2210.00858](http://arxiv.org/abs/2210.00858)|**[link](https://github.com/gtziafas/hots)**|
|**2023-03-13**|**Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement**|Zirui Zhao et.al.|[2210.00215](http://arxiv.org/abs/2210.00215)|null|
|**2023-04-24**|**EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**|Yanmin Wu et.al.|[2209.14941](http://arxiv.org/abs/2209.14941)|**[link](https://github.com/yanmin-wu/eda)**|
|**2023-10-26**|**Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding**|Fengyuan Shi et.al.|[2209.13959](http://arxiv.org/abs/2209.13959)|null|
|**2022-09-24**|**Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline**|Lichen Zhao et.al.|[2209.12028](http://arxiv.org/abs/2209.12028)|**[link](https://github.com/zlccccc/3dgqa)**|
|**2022-09-21**|**Exploring Modulated Detection Transformer as a Tool for Action Recognition in Videos**|Tomás Crisol et.al.|[2209.10126](http://arxiv.org/abs/2209.10126)|**[link](https://github.com/bhi-research/ava_mdetr)**|
|**2022-09-17**|**Introspective Learning : A Two-Stage Approach for Inference in Neural Networks**|Mohit Prabhushankar et.al.|[2209.08425](http://arxiv.org/abs/2209.08425)|**[link](https://github.com/olivesgatech/introspective-learning)**|
|**2022-09-19**|**Learning to Evaluate Performance of Multi-modal Semantic Localization**|Zhiqiang Yuan et.al.|[2209.06515](http://arxiv.org/abs/2209.06515)|**[link](https://github.com/xiaoyuan1996/semanticlocalizationmetrics)**|
|**2022-11-21**|**Visual Grounding of Inter-lingual Word-Embeddings**|Wafaa Mohammed et.al.|[2209.03714](http://arxiv.org/abs/2209.03714)|null|
|**2022-10-05**|**Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment**|Mustafa Shukor et.al.|[2208.13628](http://arxiv.org/abs/2208.13628)|**[link](https://github.com/mshukor/vicha)**|
|**2022-08-22**|**Neuro-Symbolic Visual Dialog**|Adnen Abdessaied et.al.|[2208.10353](http://arxiv.org/abs/2208.10353)|**[link](https://github.com/adnenabdessaied/NSVD)**|
|**2022-08-19**|**VLMAE: Vision-Language Masked Autoencoder**|Sunan He et.al.|[2208.09374](http://arxiv.org/abs/2208.09374)|null|
|**2022-09-19**|**Fine-Grained Semantically Aligned Vision-Language Pre-Training**|Juncheng Li et.al.|[2208.02515](http://arxiv.org/abs/2208.02515)|**[link](https://github.com/yyjmjc/loupe)**|
|**2022-10-27**|**One for All: One-stage Referring Expression Comprehension with Dynamic Reasoning**|Zhipeng Zhang et.al.|[2208.00361](http://arxiv.org/abs/2208.00361)|null|
|**2022-07-27**|**SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding**|Mengxue Qu et.al.|[2207.13325](http://arxiv.org/abs/2207.13325)|**[link](https://github.com/qumengxue/siri-vg)**|
|**2022-08-17**|**Correspondence Matters for Video Referring Expression Comprehension**|Meng Cao et.al.|[2207.10400](http://arxiv.org/abs/2207.10400)|**[link](https://github.com/mengcaopku/dcnet)**|
|**2022-06-21**|**Tell Me the Evidence? Dual Visual-Linguistic Interaction for Answer Grounding**|Junwen Pan et.al.|[2207.05703](http://arxiv.org/abs/2207.05703)|null|
|**2022-11-29**|**Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection**|Hanoona Rasheed et.al.|[2207.03482](http://arxiv.org/abs/2207.03482)|null|
|**2022-07-06**|**Adversarial Robustness of Visual Dialog**|Lu Yu et.al.|[2207.02639](http://arxiv.org/abs/2207.02639)|null|
|**2023-05-27**|**Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases**|Zhihao Yuan et.al.|[2207.01821](http://arxiv.org/abs/2207.01821)|null|
|**2022-07-02**|**Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition**|Guangzhi Sun et.al.|[2207.00857](http://arxiv.org/abs/2207.00857)|null|
|**2024-01-07**|**Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations**|Ziyan Yang et.al.|[2206.15462](http://arxiv.org/abs/2206.15462)|**[link](https://github.com/uvavision/amc-grounding)**|
|**2023-10-31**|**How direct is the link between words and images?**|Hassan Shahmohammadi et.al.|[2206.15381](http://arxiv.org/abs/2206.15381)|null|
|**2022-06-22**|**Doubly Reparameterized Importance Weighted Structure Learning for Scene Graph Generation**|Daqi Liu et.al.|[2206.11352](http://arxiv.org/abs/2206.11352)|null|
|**2022-06-22**|**Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution**|Chonghan Chen et.al.|[2206.09114](http://arxiv.org/abs/2206.09114)|null|
|**2022-06-17**|**VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix**|Teng Wang et.al.|[2206.08919](http://arxiv.org/abs/2206.08919)|null|
|**2023-10-31**|**Language with Vision: a Study on Grounded Word and Sentence Embeddings**|Hassan Shahmohammadi et.al.|[2206.08823](http://arxiv.org/abs/2206.08823)|**[link](https://github.com/hazel1994/visually_grounded_word_embeddings_2)**|
|**2023-01-09**|**MixGen: A New Multi-Modal Data Augmentation**|Xiaoshuai Hao et.al.|[2206.08358](http://arxiv.org/abs/2206.08358)|**[link](https://github.com/amazon-research/mix-generation)**|
|**2022-06-16**|**RefCrowd: Grounding the Target in Crowd with Referring Expressions**|Heqian Qiu et.al.|[2206.08172](http://arxiv.org/abs/2206.08172)|null|
|**2022-11-18**|**Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone**|Zi-Yi Dou et.al.|[2206.07643](http://arxiv.org/abs/2206.07643)|**[link](https://github.com/microsoft/fiber)**|
|**2022-06-14**|**Comprehending and Ordering Semantics for Image Captioning**|Yehao Li et.al.|[2206.06930](http://arxiv.org/abs/2206.06930)|**[link](https://github.com/yehli/xmodaler)**|
|**2022-06-14**|**TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer**|Jiajun Deng et.al.|[2206.06619](http://arxiv.org/abs/2206.06619)|**[link](https://github.com/djiajunustc/TransVG)**|
|**2022-06-07**|**Intra-agent speech permits zero-shot task acquisition**|Chen Yan et.al.|[2206.03139](http://arxiv.org/abs/2206.03139)|null|
|**2022-05-25**|**Guiding Visual Question Answering with Attention Priors**|Thao Minh Le et.al.|[2205.12616](http://arxiv.org/abs/2205.12616)|null|
|**2023-03-02**|**The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training**|Gi-Cheon Kang et.al.|[2205.12502](http://arxiv.org/abs/2205.12502)|**[link](https://github.com/gicheonkang/gst-visdial)**|
|**2022-07-10**|**Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution**|Georgios Tziafas et.al.|[2205.12089](http://arxiv.org/abs/2205.12089)|null|
|**2022-05-25**|**mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections**|Chenliang Li et.al.|[2205.12005](http://arxiv.org/abs/2205.12005)|**[link](https://github.com/alibaba/AliceMind/tree/main/mPLUG)**|
|**2022-11-22**|**PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models**|Yuan Yao et.al.|[2205.11169](http://arxiv.org/abs/2205.11169)|**[link](https://github.com/thunlp/pevl)**|
|**2022-05-14**|**Importance Weighted Structure Learning for Scene Graph Generation**|Daqi Liu et.al.|[2205.07017](http://arxiv.org/abs/2205.07017)|null|
|**2022-05-12**|**Weakly-supervised segmentation of referring expressions**|Robin Strudel et.al.|[2205.04725](http://arxiv.org/abs/2205.04725)|null|
|**2022-05-08**|**RoViST:Learning Robust Metrics for Visual Storytelling**|Eileen Wang et.al.|[2205.03774](http://arxiv.org/abs/2205.03774)|**[link](https://github.com/usydnlp/rovist)**|
|**2022-05-30**|**Language Models Can See: Plugging Visual Controls in Text Generation**|Yixuan Su et.al.|[2205.02655](http://arxiv.org/abs/2205.02655)|**[link](https://github.com/yxuansu/magic)**|
|**2022-06-08**|**Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning**|Li Yang et.al.|[2205.00272](http://arxiv.org/abs/2205.00272)|**[link](https://github.com/yangli18/vltvg)**|
|**2022-04-22**|**Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering**|Yu-Jung Heo et.al.|[2204.10448](http://arxiv.org/abs/2204.10448)|**[link](https://github.com/yujungheo/kbvqa-public)**|
|**2024-03-12**|**Self-paced Multi-grained Cross-modal Interaction Modeling for Referring Expression Comprehension**|Peihan Miao et.al.|[2204.09957](http://arxiv.org/abs/2204.09957)|null|
|**2023-09-14**|**A Survivor in the Era of Large-Scale Pretraining: An Empirical Study of One-Stage Referring Expression Comprehension**|Gen Luo et.al.|[2204.07913](http://arxiv.org/abs/2204.07913)|**[link](https://github.com/luogen1996/simrec)**|
|**2022-05-03**|**XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding**|Chan-Jan Hsu et.al.|[2204.07316](http://arxiv.org/abs/2204.07316)|null|
|**2022-04-15**|**Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning**|Feilong Chen et.al.|[2204.07302](http://arxiv.org/abs/2204.07302)|null|
|**2022-04-13**|**3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection**|Junyu Luo et.al.|[2204.06272](http://arxiv.org/abs/2204.06272)|**[link](https://github.com/fjhzhixi/3d-sps)**|
|**2022-05-02**|**ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension**|Sanjay Subramanian et.al.|[2204.05991](http://arxiv.org/abs/2204.05991)|**[link](https://github.com/allenai/reclip)**|
|**2022-04-05**|**Multi-View Transformer for 3D Visual Grounding**|Shijia Huang et.al.|[2204.02174](http://arxiv.org/abs/2204.02174)|**[link](https://github.com/sega-hsj/mvt-3dvg)**|
|**2022-08-09**|**FindIt: Generalized Localization with Natural Language Queries**|Weicheng Kuo et.al.|[2203.17273](http://arxiv.org/abs/2203.17273)|null|
|**2022-03-30**|**To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo**|Yiran Luo et.al.|[2203.16682](http://arxiv.org/abs/2203.16682)|**[link](https://github.com/fpsluozi/tofindwaldo)**|
|**2022-07-24**|**SeqTR: A Simple yet Universal Network for Visual Grounding**|Chaoyang Zhu et.al.|[2203.16265](http://arxiv.org/abs/2203.16265)|**[link](https://github.com/sean-zhuh/seqtr)**|
|**2022-03-29**|**Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding**|Jiabo Ye et.al.|[2203.15442](http://arxiv.org/abs/2203.15442)|**[link](https://github.com/lukeforeveryoung/qrnet)**|
|**2023-06-20**|**Word Discovery in Visually Grounded, Self-Supervised Speech Models**|Puyuan Peng et.al.|[2203.15081](http://arxiv.org/abs/2203.15081)|**[link](https://github.com/kamperh/vqwordseg)**|
|**2022-03-25**|**Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas**|Raphael Schumann et.al.|[2203.13838](http://arxiv.org/abs/2203.13838)|**[link](https://github.com/raphael-sch/map2seq_vln)**|
|**2023-05-26**|**VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning**|Wenjia Xu et.al.|[2203.10444](http://arxiv.org/abs/2203.10444)|**[link](https://github.com/wenjiaxu/vgse)**|
|**2024-01-19**|**Local-Global Context Aware Transformer for Language-Guided Video Segmentation**|Chen Liang et.al.|[2203.09773](http://arxiv.org/abs/2203.09773)|**[link](https://github.com/leonnnop/locater)**|
|**2022-03-22**|**Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding**|Haojun Jiang et.al.|[2203.08481](http://arxiv.org/abs/2203.08481)|**[link](https://github.com/leaplabthu/pseudo-q)**|
|**2022-03-14**|**Modelling word learning and recognition using visually grounded speech**|Danny Merkx et.al.|[2203.06937](http://arxiv.org/abs/2203.06937)|**[link](https://github.com/DannyMerkx/speech2image)**|
|**2023-06-02**|**Differentiated Relevances Embedding for Group-based Referring Expression Comprehension**|Fuhai Chen et.al.|[2203.06382](http://arxiv.org/abs/2203.06382)|null|
|**2022-03-11**|**REX: Reasoning-aware and Grounded Explanation**|Shi Chen et.al.|[2203.06107](http://arxiv.org/abs/2203.06107)|**[link](https://github.com/szzexpoi/rex)**|
|**2023-08-21**|**Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding**|Yang Jiao et.al.|[2203.05186](http://arxiv.org/abs/2203.05186)|null|
|**2022-02-24**|**Visually Grounded Task and Motion Planning for Mobile Manipulation**|Xiaohan Zhang et.al.|[2202.10667](http://arxiv.org/abs/2202.10667)|null|
|**2022-02-21**|**Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge**|Danny Merkx et.al.|[2202.10292](http://arxiv.org/abs/2202.10292)|**[link](https://github.com/DannyMerkx/speech2image)**|
|**2022-02-17**|**Visual Ground Truth Construction as Faceted Classification**|Fausto Giunchiglia et.al.|[2202.08512](http://arxiv.org/abs/2202.08512)|null|
|**2022-03-02**|**Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling**|Puyuan Peng et.al.|[2202.03543](http://arxiv.org/abs/2202.03543)|**[link](https://github.com/jasonppy/fast-vgs-family)**|
|**2022-06-01**|**OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework**|Peng Wang et.al.|[2202.03052](http://arxiv.org/abs/2202.03052)|**[link](https://github.com/ofa-sys/ofa)**|
|**2022-04-08**|**Grounding Answers for Visual Questions Asked by Visually Impaired People**|Chongyan Chen et.al.|[2202.01993](http://arxiv.org/abs/2202.01993)|**[link](https://github.com/ccychongyanchen/vizwizvqagroundingcrowdsourcing)**|
|**2022-02-02**|**Keyword localisation in untranscribed speech using visually grounded speech models**|Kayode Olaleye et.al.|[2202.01107](http://arxiv.org/abs/2202.01107)|**[link](https://github.com/kayodeolaleye/keyword_localisation_speech)**|
|**2022-01-27**|**Constrained Structure Learning for Scene Graph Generation**|Daqi Liu et.al.|[2201.11697](http://arxiv.org/abs/2201.11697)|null|
|**2021-12-31**|**Deconfounded Visual Grounding**|Jianqiang Huang et.al.|[2112.15324](http://arxiv.org/abs/2112.15324)|null|
|**2021-12-23**|**Align and Prompt: Video-and-Language Pre-training with Entity Prompts**|Dongxu Li et.al.|[2112.09583](http://arxiv.org/abs/2112.09583)|**[link](https://github.com/salesforce/alpro)**|
|**2023-05-11**|**CLIP-Lite: Information Efficient Visual Representation Learning with Language Supervision**|Aman Shrivastava et.al.|[2112.07133](http://arxiv.org/abs/2112.07133)|**[link](https://github.com/4m4n5/CLIP-Lite)**|
|**2022-07-22**|**D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2112.01551](http://arxiv.org/abs/2112.01551)|null|
|**2021-11-25**|**Scene Graph Generation with Geometric Context**|Vishal Kumar et.al.|[2111.13131](http://arxiv.org/abs/2111.13131)|null|
|**2022-04-04**|**Less is More: Generating Grounded Navigation Instructions from Landmarks**|Su Wang et.al.|[2111.12872](http://arxiv.org/abs/2111.12872)|null|
|**2022-07-27**|**UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling**|Zhengyuan Yang et.al.|[2111.12085](http://arxiv.org/abs/2111.12085)|**[link](https://github.com/microsoft/UniTAB)**|
|**2021-11-13**|**Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning**|Yizhen Zhang et.al.|[2111.07180](http://arxiv.org/abs/2111.07180)|null|
|**2022-07-18**|**Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation**|Chuang Lin et.al.|[2111.05759](http://arxiv.org/abs/2111.05759)|**[link](https://github.com/clin1223/mtvm)**|
|**2022-11-30**|**Evaluating and Improving Interactions with Hazy Oracles**|Stephan J. Lemmer et.al.|[2110.10206](http://arxiv.org/abs/2110.10206)|null|
|**2021-10-19**|**Open-domain clarification question generation without question examples**|Julia White et.al.|[2110.09779](http://arxiv.org/abs/2110.09779)|null|
|**2023-09-14**|**Towards Language-guided Visual Recognition via Dynamic Convolutions**|Gen Luo et.al.|[2110.08797](http://arxiv.org/abs/2110.08797)|**[link](https://github.com/luogen1996/laconvnet)**|
|**2021-10-14**|**Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset**|Ian Palmer et.al.|[2110.07575](http://arxiv.org/abs/2110.07575)|**[link](https://github.com/iapalm/Spoken-ObjectNet)**|
|**2021-10-06**|**Efficient Multi-Modal Embeddings from Structured Data**|Anita L. Verő et.al.|[2110.02577](http://arxiv.org/abs/2110.02577)|null|
|**2021-09-29**|**Visually Grounded Concept Composition**|Bowen Zhang et.al.|[2109.14115](http://arxiv.org/abs/2109.14115)|null|
|**2021-10-21**|**Visually Grounded Reasoning across Languages and Cultures**|Fangyu Liu et.al.|[2109.13238](http://arxiv.org/abs/2109.13238)|**[link](https://github.com/marvl-challenge/marvl-code)**|
|**2022-05-20**|**CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models**|Yuan Yao et.al.|[2109.11797](http://arxiv.org/abs/2109.11797)|**[link](https://github.com/thunlp/cpt)**|
|**2021-09-22**|**COVR: A test-bed for Visually Grounded Compositional Generalization with real images**|Ben Bogin et.al.|[2109.10613](http://arxiv.org/abs/2109.10613)|**[link](https://github.com/benbogin/covr-dataset)**|
|**2021-09-22**|**Audio-Visual Grounding Referring Expression for Robotic Manipulation**|Yefei Wang et.al.|[2109.10571](http://arxiv.org/abs/2109.10571)|null|
|**2021-09-20**|**Dependency Induction Through the Lens of Visual Perception**|Ruisi Su et.al.|[2109.09790](http://arxiv.org/abs/2109.09790)|**[link](https://github.com/ruisi-su/concrete_dep)**|
|**2021-09-17**|**Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation**|Feilong Chen et.al.|[2109.08478](http://arxiv.org/abs/2109.08478)|**[link](https://github.com/zyang-ur/onestage_grounding)**|
|**2022-03-02**|**Fast-Slow Transformer for Visually Grounding Speech**|Puyuan Peng et.al.|[2109.08186](http://arxiv.org/abs/2109.08186)|**[link](https://github.com/jasonppy/fast-vgs-family)**|
|**2022-11-08**|**Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering**|Jihyung Kil et.al.|[2109.06122](http://arxiv.org/abs/2109.06122)|**[link](https://github.com/heendung/simpleaug)**|
|**2021-09-10**|**Panoptic Narrative Grounding**|C. González et.al.|[2109.04988](http://arxiv.org/abs/2109.04988)|**[link](https://github.com/bcv-uniandes/png)**|
|**2022-03-02**|**PlaTe: Visually-Grounded Planning with Transformers in Procedural Tasks**|Jiankai Sun et.al.|[2109.04869](http://arxiv.org/abs/2109.04869)|null|
|**2021-09-10**|**EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation**|Yanjun Gao et.al.|[2109.04600](http://arxiv.org/abs/2109.04600)|null|
|**2022-03-25**|**Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models**|Steven Y. Feng et.al.|[2109.03892](http://arxiv.org/abs/2109.03892)|**[link](https://github.com/styfeng/visctg)**|
|**2021-09-07**|**Learning grounded word meaning representations on similarity graphs**|Mariella Dimiccoli et.al.|[2109.03084](http://arxiv.org/abs/2109.03084)|**[link](https://github.com/mdimiccoli/hm-sge)**|
|**2024-01-08**|**INVIGORATE: Interactive Visual Grounding and Grasping in Clutter**|Hanbo Zhang et.al.|[2108.11092](http://arxiv.org/abs/2108.11092)|null|
|**2021-08-17**|**Who's Waldo? Linking People Across Text and Images**|Claire Yuqing Cui et.al.|[2108.07253](http://arxiv.org/abs/2108.07253)|**[link](https://github.com/clairecyq/whos-waldo)**|
|**2022-02-02**|**A Better Loss for Visual-Textual Grounding**|Davide Rigoni et.al.|[2108.05308](http://arxiv.org/abs/2108.05308)|**[link](https://github.com/drigoni/Loss_VT_Grounding)**|
|**2021-07-05**|**Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models**|Khazar Khorrami et.al.|[2108.02562](http://arxiv.org/abs/2108.02562)|null|
|**2021-08-11**|**TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding**|Dailan He et.al.|[2108.02388](http://arxiv.org/abs/2108.02388)|null|
|**2021-07-31**|**Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding**|Heng Zhao et.al.|[2108.00205](http://arxiv.org/abs/2108.00205)|null|
|**2021-07-14**|**ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition**|Afra Alishahi et.al.|[2107.06546](http://arxiv.org/abs/2107.06546)|**[link](https://github.com/bhigy/zr-2021vg_baseline)**|
|**2021-07-09**|**Using Depth for Improving Referring Expression Comprehension in Real-World Environments**|Fethiye Irmak Dogan et.al.|[2107.04658](http://arxiv.org/abs/2107.04658)|null|
|**2021-11-04**|**LanguageRefer: Spatial-Language Model for 3D Visual Grounding**|Junha Roh et.al.|[2107.03438](http://arxiv.org/abs/2107.03438)|null|
|**2021-10-19**|**VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer**|Zineng Tang et.al.|[2107.02681](http://arxiv.org/abs/2107.02681)|**[link](https://github.com/zinengtang/VidLanKD)**|
|**2021-06-28**|**Adventurer's Treasure Hunt: A Transparent System for Visually Grounded Compositional Visual Question Answering based on Scene Graphs**|Daniel Reich et.al.|[2106.14476](http://arxiv.org/abs/2106.14476)|null|
|**2022-09-30**|**Draw Me a Flower: Processing and Grounding Abstraction in Natural Language**|Royi Lachmy et.al.|[2106.14321](http://arxiv.org/abs/2106.14321)|null|
|**2021-06-23**|**Attention-Based Keyword Localisation in Speech using Visual Grounding**|Kayode Olaleye et.al.|[2106.08859](http://arxiv.org/abs/2106.08859)|null|
|**2021-06-16**|**Semantic sentence similarity: size does not always matter**|Danny Merkx et.al.|[2106.08648](http://arxiv.org/abs/2106.08648)|**[link](https://github.com/DannyMerkx/speech2image)**|
|**2021-06-09**|**SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation**|Ioannis Kazakos et.al.|[2106.04403](http://arxiv.org/abs/2106.04403)|**[link](https://github.com/imatge-upc/synthref)**|
|**2021-07-14**|**Referring Transformer: A One-step Approach to Multi-task Visual Grounding**|Muchen Li et.al.|[2106.03089](http://arxiv.org/abs/2106.03089)|**[link](https://github.com/ubc-vision/RefTR)**|
|**2022-03-16**|**VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator**|Ayush Shrivastava et.al.|[2105.11589](http://arxiv.org/abs/2105.11589)|**[link](https://github.com/alexa/visitron)**|
|**2021-05-24**|**Learning Better Visual Dialog Agents with Pretrained Visual-Linguistic Representation**|Tao Tu et.al.|[2105.11541](http://arxiv.org/abs/2105.11541)|**[link](https://github.com/amazon-research/read-up)**|
|**2021-09-22**|**SAT: 2D Semantics Assisted Training for 3D Visual Grounding**|Zhengyuan Yang et.al.|[2105.11450](http://arxiv.org/abs/2105.11450)|**[link](https://github.com/zyang-ur/SAT)**|
|**2021-05-12**|**Connecting What to Say With Where to Look by Modeling Human Attention Traces**|Zihang Meng et.al.|[2105.05964](http://arxiv.org/abs/2105.05964)|**[link](https://github.com/facebookresearch/connect-caption-and-trace)**|
|**2021-05-11**|**Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols**|Rishi Hazra et.al.|[2105.05069](http://arxiv.org/abs/2105.05069)|null|
|**2022-03-14**|**Visual Grounding with Transformers**|Ye Du et.al.|[2105.04281](http://arxiv.org/abs/2105.04281)|**[link](https://github.com/usr922/VGTR)**|
|**2021-05-16**|**gComm: An environment for investigating generalization in Grounded Language Acquisition**|Rishi Hazra et.al.|[2105.03943](http://arxiv.org/abs/2105.03943)|**[link](https://github.com/SonuDixit/gComm)**|
|**2021-05-05**|**Proposal-free One-stage Referring Expression via Grid-Word Cross-Attention**|Wei Suo et.al.|[2105.02061](http://arxiv.org/abs/2105.02061)|null|
|**2021-10-06**|**Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques**|Grzegorz Chrupała et.al.|[2104.13225](http://arxiv.org/abs/2104.13225)|null|
|**2021-10-12**|**MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding**|Aishwarya Kamath et.al.|[2104.12763](http://arxiv.org/abs/2104.12763)|**[link](https://github.com/ashkamath/mdetr)**|
|**2021-12-14**|**Playing Lottery Tickets with Vision and Language**|Zhe Gan et.al.|[2104.11832](http://arxiv.org/abs/2104.11832)|null|
|**2021-04-20**|**Understanding Synonymous Referring Expressions via Contrastive Features**|Yi-Wen Chen et.al.|[2104.10156](http://arxiv.org/abs/2104.10156)|**[link](https://github.com/wenz116/RefContrast)**|
|**2021-04-17**|**Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task Feasibility in Interactive Visual Environments**|Andrea Burns et.al.|[2104.08560](http://arxiv.org/abs/2104.08560)|**[link](https://github.com/aburns4/MoTIF)**|
|**2022-01-14**|**TransVG: End-to-End Visual Grounding with Transformers**|Jiajun Deng et.al.|[2104.08541](http://arxiv.org/abs/2104.08541)|**[link](https://github.com/djiajunustc/TransVG)**|
|**2021-09-13**|**Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training**|Hassan Shahmohammadi et.al.|[2104.07500](http://arxiv.org/abs/2104.07500)|**[link](https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings)**|
|**2021-04-09**|**Look Before You Leap: Learning Landmark Features for One-Stage Visual Grounding**|Binbin Huang et.al.|[2104.04386](http://arxiv.org/abs/2104.04386)|**[link](https://github.com/svip-lab/LBYLNet)**|
|**2021-04-05**|**Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation**|Yapeng Tian et.al.|[2104.02026](http://arxiv.org/abs/2104.02026)|**[link](https://github.com/YapengTian/CCOL-CVPR21)**|

## RVOS

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-02**|**Referring Video Object Segmentation via Language-aligned Track Selection**|Seongchan Kim et.al.|[2412.01136](http://arxiv.org/abs/2412.01136)|null|
|**2024-11-26**|**SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation**|Claudia Cuttano et.al.|[2411.17646](http://arxiv.org/abs/2411.17646)|**[link](https://github.com/claudiacuttano/samwise)**|
|**2024-09-09**|**LSVOS Challenge Report: Large-scale Complex and Long Video Object Segmentation**|Henghui Ding et.al.|[2409.05847](http://arxiv.org/abs/2409.05847)|null|
|**2024-08-22**|**The 2nd Solution for LSVOS Challenge RVOS Track: Spatial-temporal Refinement for Consistent Semantic Segmentation**|Tuyen Tran et.al.|[2408.12447](http://arxiv.org/abs/2408.12447)|null|
|**2024-08-20**|**The Instance-centric Transformer for the RVOS Track of LSVOS Challenge: 3rd Place Solution**|Bin Cao et.al.|[2408.10541](http://arxiv.org/abs/2408.10541)|null|
|**2024-08-24**|**UNINEXT-Cutie: The 1st Solution for LSVOS Challenge RVOS Track**|Hao Fang et.al.|[2408.10129](http://arxiv.org/abs/2408.10129)|null|
|**2024-07-10**|**ActionVOS: Actions as Prompts for Video Object Segmentation**|Liangyang Ouyang et.al.|[2407.07402](http://arxiv.org/abs/2407.07402)|**[link](https://github.com/ut-vision/actionvos)**|
|**2024-06-20**|**2nd Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation**|Bin Cao et.al.|[2406.13939](http://arxiv.org/abs/2406.13939)|null|
|**2024-06-23**|**GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation**|Ci-Siang Lin et.al.|[2406.12834](http://arxiv.org/abs/2406.12834)|null|
|**2024-06-11**|**1st Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation**|Mingqi Gao et.al.|[2406.07043](http://arxiv.org/abs/2406.07043)|**[link](https://github.com/tapall-ai/mevis_track_solution_2024)**|
|**2024-06-07**|**3rd Place Solution for MeViS Track in CVPR 2024 PVUW workshop: Motion Expression guided Video Segmentation**|Feiyu Pan et.al.|[2406.04842](http://arxiv.org/abs/2406.04842)|null|
|**2024-09-22**|**Harnessing Vision-Language Pretrained Models with Temporal-Aware Adaptation for Referring Video Object Segmentation**|Zikun Zhou et.al.|[2405.10610](http://arxiv.org/abs/2405.10610)|null|
|**2024-10-11**|**Temporally Consistent Referring Video Object Segmentation with Hybrid Memory**|Bo Miao et.al.|[2403.19407](http://arxiv.org/abs/2403.19407)|**[link](https://github.com/bo-miao/HTR)**|
|**2024-07-06**|**Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation**|Zixin Zhu et.al.|[2403.12042](http://arxiv.org/abs/2403.12042)|**[link](https://github.com/buxiangzhiren/vd-it)**|
|**2024-01-01**|**1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation**|Zhuoyan Luo et.al.|[2401.00663](http://arxiv.org/abs/2401.00663)|**[link](https://github.com/robertluo1/iccv2023_rvos_challenge)**|
|**2023-12-29**|**Tracking with Human-Intent Reasoning**|Jiawen Zhu et.al.|[2312.17448](http://arxiv.org/abs/2312.17448)|**[link](https://github.com/jiawen-zhu/trackgpt)**|
|**2023-12-25**|**UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces**|Jiannan Wu et.al.|[2312.15715](http://arxiv.org/abs/2312.15715)|**[link](https://github.com/foundationvision/uniref)**|
|**2023-09-21**|**Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation**|Ping Li et.al.|[2309.11933](http://arxiv.org/abs/2309.11933)|null|
|**2023-09-07**|**Temporal Collection and Distribution for Referring Video Object Segmentation**|Jiajin Tang et.al.|[2309.03473](http://arxiv.org/abs/2309.03473)|null|
|**2023-09-05**|**Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples**|Guanghui Li et.al.|[2309.02041](http://arxiv.org/abs/2309.02041)|**[link](https://github.com/hengliusky/few_shot_rvos)**|
|**2023-08-16**|**MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions**|Henghui Ding et.al.|[2308.08544](http://arxiv.org/abs/2308.08544)|**[link](https://github.com/henghuiding/MeViS)**|
|**2023-08-08**|**EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation**|Jiajun Chen et.al.|[2308.04162](http://arxiv.org/abs/2308.04162)|**[link](https://github.com/lab206/epcformer)**|
|**2023-12-15**|**Learning Referring Video Object Segmentation from Weak Annotation**|Wangbo Zhao et.al.|[2308.02162](http://arxiv.org/abs/2308.02162)|null|
|**2023-07-25**|**Spectrum-guided Multi-granularity Referring Video Object Segmentation**|Bo Miao et.al.|[2307.13537](http://arxiv.org/abs/2307.13537)|**[link](https://github.com/bo-miao/sgmg)**|
|**2023-07-18**|**OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation**|Dongming Wu et.al.|[2307.09356](http://arxiv.org/abs/2307.09356)|**[link](https://github.com/wudongming97/onlinerefer)**|
|**2024-09-03**|**RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation**|Yonglin Li et.al.|[2307.00997](http://arxiv.org/abs/2307.00997)|**[link](https://github.com/lancasterli/refsam)**|
|**2023-09-17**|**Bidirectional Correlation-Driven Inter-Frame Interaction Transformer for Referring Video Object Segmentation**|Meng Lan et.al.|[2307.00536](http://arxiv.org/abs/2307.00536)|null|
|**2024-04-02**|**LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation**|Linfeng Yuan et.al.|[2306.08736](http://arxiv.org/abs/2306.08736)|**[link](https://github.com/linfengyuan1997/losh)**|
|**2023-05-26**|**SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation**|Zhuoyan Luo et.al.|[2305.17011](http://arxiv.org/abs/2305.17011)|**[link](https://github.com/RobertLuo1/NeurIPS2023_SOC)**|
|**2023-12-12**|**Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation**|Shilin Yan et.al.|[2305.16318](http://arxiv.org/abs/2305.16318)|**[link](https://github.com/opengvlab/mutr)**|
|**2022-12-27**|**1st Place Solution for YouTubeVOS Challenge 2022: Referring Video Object Segmentation**|Zhiwei Hu et.al.|[2212.14679](http://arxiv.org/abs/2212.14679)|**[link](https://github.com/zhiweihhh/cvpr2022-rvos-challenge)**|
|**2022-07-26**|**Multi-Attention Network for Compressed Video Referring Object Segmentation**|Weidong Chen et.al.|[2207.12622](http://arxiv.org/abs/2207.12622)|**[link](https://github.com/dexianghong/manet)**|
|**2023-08-18**|**Towards Robust Referring Video Object Segmentation with Cyclic Relational Consensus**|Xiang Li et.al.|[2207.01203](http://arxiv.org/abs/2207.01203)|**[link](https://github.com/lxa9867/R2VOS)**|
|**2022-06-24**|**The Second Place Solution for The 4th Large-scale Video Object Segmentation Challenge--Track 3: Referring Video Object Segmentation**|Leilei Cao et.al.|[2206.12035](http://arxiv.org/abs/2206.12035)|null|
|**2022-06-08**|**Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation**|Zihan Ding et.al.|[2206.03789](http://arxiv.org/abs/2206.03789)|**[link](https://github.com/dzh19990407/lbdt)**|
|**2024-01-19**|**Local-Global Context Aware Transformer for Language-Guided Video Segmentation**|Chen Liang et.al.|[2203.09773](http://arxiv.org/abs/2203.09773)|**[link](https://github.com/leonnnop/locater)**|
|**2022-03-13**|**Language as Queries for Referring Video Object Segmentation**|Jiannan Wu et.al.|[2201.00487](http://arxiv.org/abs/2201.00487)|**[link](https://github.com/wjn922/referformer)**|
|**2022-04-03**|**End-to-End Referring Video Object Segmentation with Multimodal Transformers**|Adam Botach et.al.|[2111.14821](http://arxiv.org/abs/2111.14821)|**[link](https://github.com/mttr2021/MTTR)**|
|**2024-01-19**|**Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation**|Chen Liang et.al.|[2106.01061](http://arxiv.org/abs/2106.01061)|null|

## 3D-RES

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-03**|**RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2412.02402](http://arxiv.org/abs/2412.02402)|**[link](https://github.com/sosppxo/rg-san)**|
|**2024-07-31**|**3D-GRES: Generalized 3D Referring Expression Segmentation**|Changli Wu et.al.|[2407.20664](http://arxiv.org/abs/2407.20664)|**[link](https://github.com/sosppxo/MDIN)**|
|**2023-08-31**|**3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation**|Changli Wu et.al.|[2308.16632](http://arxiv.org/abs/2308.16632)|**[link](https://github.com/sosppxo/3d-stmn)**|

## 3D-REC

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-09**|**3D Spatial Understanding in MLLMs: Disambiguation and Evaluation**|Chun-Peng Chang et.al.|[2412.06613](http://arxiv.org/abs/2412.06613)|null|
|**2024-12-05**|**SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding**|Rong Li et.al.|[2412.04383](http://arxiv.org/abs/2412.04383)|null|
|**2024-11-27**|**3D Scene Graph Guided Vision-Language Pre-training**|Hao Liu et.al.|[2411.18666](http://arxiv.org/abs/2411.18666)|null|
|**2024-11-27**|**BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence**|Xuewu Lin et.al.|[2411.14869](http://arxiv.org/abs/2411.14869)|**[link](https://github.com/HorizonRobotics/BIP3D)**|
|**2024-11-21**|**Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction Problems**|Qihao Yuan et.al.|[2411.14594](http://arxiv.org/abs/2411.14594)|**[link](https://github.com/sunsleaf/csvg)**|
|**2024-11-07**|**LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with Transformers**|Yeong-Seung Baek et.al.|[2411.04351](http://arxiv.org/abs/2411.04351)|null|
|**2024-11-05**|**Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding**|Sombit Dey et.al.|[2411.03405](http://arxiv.org/abs/2411.03405)|null|
|**2024-10-21**|**Joint Top-Down and Bottom-Up Frameworks for 3D Visual Grounding**|Yang Liu et.al.|[2410.15615](http://arxiv.org/abs/2410.15615)|null|
|**2024-10-17**|**VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding**|Runsen Xu et.al.|[2410.13860](http://arxiv.org/abs/2410.13860)|**[link](https://github.com/openrobotlab/vlm-grounder)**|
|**2024-09-12**|**Bayesian Self-Training for Semi-Supervised 3D Segmentation**|Ozan Unal et.al.|[2409.08102](http://arxiv.org/abs/2409.08102)|null|
|**2024-08-07**|**Task-oriented Sequential Grounding in 3D Scenes**|Zhuofan Zhang et.al.|[2408.04034](http://arxiv.org/abs/2408.04034)|null|
|**2024-07-25**|**RefMask3D: Language-Guided Transformer for 3D Referring Segmentation**|Shuting He et.al.|[2407.18244](http://arxiv.org/abs/2407.18244)|**[link](https://github.com/heshuting555/refmask3d)**|
|**2024-09-02**|**PD-APE: A Parallel Decoding Framework with Adaptive Position Encoding for 3D Visual Grounding**|Chenshu Hou et.al.|[2407.14491](http://arxiv.org/abs/2407.14491)|null|
|**2024-07-10**|**Multi-branch Collaborative Learning Network for 3D Visual Grounding**|Zhipeng Qian et.al.|[2407.05363](http://arxiv.org/abs/2407.05363)|**[link](https://github.com/qzp2018/MCLN)**|
|**2024-07-17**|**ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities**|Chenming Zhu et.al.|[2407.01525](http://arxiv.org/abs/2407.01525)|null|
|**2024-06-13**|**MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations**|Ruiyuan Lyu et.al.|[2406.09401](http://arxiv.org/abs/2406.09401)|**[link](https://github.com/openrobotlab/embodiedscan)**|
|**2024-06-13**|**Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding**|Yue Xu et.al.|[2406.08907](http://arxiv.org/abs/2406.08907)|null|
|**2024-07-22**|**A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances, and Future Directions**|Daizong Liu et.al.|[2406.05785](http://arxiv.org/abs/2406.05785)|**[link](https://github.com/liudaizong/awesome-3d-visual-grounding)**|
|**2024-07-06**|**Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention**|Weitai Kang et.al.|[2405.18295](http://arxiv.org/abs/2405.18295)|null|
|**2024-05-24**|**Talk to Parallel LiDARs: A Human-LiDAR Interaction Method Based on 3D Visual Grounding**|Yuhang Liu et.al.|[2405.15274](http://arxiv.org/abs/2405.15274)|null|
|**2024-07-19**|**Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension**|Runwei Guan et.al.|[2405.12821](http://arxiv.org/abs/2405.12821)|**[link](https://github.com/guanrunwei/talk2radar)**|
|**2024-04-30**|**Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners**|Chun Feng et.al.|[2404.19696](http://arxiv.org/abs/2404.19696)|null|
|**2024-09-19**|**Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework through Prompt-based Localization**|Yongdong Luo et.al.|[2404.11064](http://arxiv.org/abs/2404.11064)|null|
|**2024-12-04**|**Data-Efficient 3D Visual Grounding via Order-Aware Referring**|Tung-Yu Wu et.al.|[2403.16539](http://arxiv.org/abs/2403.16539)|null|
|**2024-03-13**|**SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention**|Feng Xiao et.al.|[2403.08182](http://arxiv.org/abs/2403.08182)|null|
|**2024-12-01**|**MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding**|Chun-Peng Chang et.al.|[2403.03077](http://arxiv.org/abs/2403.03077)|**[link](https://github.com/dfki-av/mikasa-3dvg)**|
|**2024-09-24**|**SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding**|Baoxiong Jia et.al.|[2401.09340](http://arxiv.org/abs/2401.09340)|null|
|**2024-08-30**|**Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment**|Xiaoxu Xu et.al.|[2312.09625](http://arxiv.org/abs/2312.09625)|null|
|**2023-12-13**|**Mono3DVG: 3D Visual Grounding in Monocular Images**|Yang Zhan et.al.|[2312.08022](http://arxiv.org/abs/2312.08022)|**[link](https://github.com/zhanyang-nwpu/mono3dvg)**|
|**2024-03-23**|**Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding**|Zhihao Yuan et.al.|[2311.15383](http://arxiv.org/abs/2311.15383)|**[link](https://github.com/CurryYuan/ZSVG3D)**|
|**2023-10-28**|**CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data**|Taiki Miyanishi et.al.|[2310.18773](http://arxiv.org/abs/2310.18773)|**[link](https://github.com/atr-dbi/cityrefer)**|
|**2024-10-05**|**CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding**|Eslam Abdelrahman et.al.|[2310.06214](http://arxiv.org/abs/2310.06214)|**[link](https://github.com/eslambakr/CoT3D_VG)**|
|**2023-09-21**|**LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent**|Jianing Yang et.al.|[2309.12311](http://arxiv.org/abs/2309.12311)|**[link](https://github.com/sled-group/chat-with-nerf)**|
|**2023-09-11**|**Multi3DRefer: Grounding Text Description to Multiple 3D Objects**|Yiming Zhang et.al.|[2309.05251](http://arxiv.org/abs/2309.05251)|null|
|**2024-07-16**|**Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding**|Ozan Unal et.al.|[2309.04561](http://arxiv.org/abs/2309.04561)|null|
|**2023-11-20**|**A Unified Framework for 3D Point Cloud Visual Grounding**|Haojia Lin et.al.|[2308.11887](http://arxiv.org/abs/2308.11887)|**[link](https://github.com/leon1207/3dreftr)**|
|**2023-07-25**|**3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding**|Zehan Wang et.al.|[2307.13363](http://arxiv.org/abs/2307.13363)|null|
|**2023-07-18**|**Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding**|Zehan Wang et.al.|[2307.09267](http://arxiv.org/abs/2307.09267)|**[link](https://github.com/ZzZZCHS/WS-3DVG)**|
|**2023-05-25**|**Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving**|Wenhao Cheng et.al.|[2305.15765](http://arxiv.org/abs/2305.15765)|null|
|**2024-02-07**|**Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans**|Taiki Miyanishi et.al.|[2305.13876](http://arxiv.org/abs/2305.13876)|**[link](https://github.com/atr-dbi/cross3dvg)**|
|**2024-07-15**|**WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language**|Zhenxiang Lin et.al.|[2304.05645](http://arxiv.org/abs/2304.05645)|**[link](https://github.com/4dvlab/wildrefer)**|
|**2023-12-05**|**ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance**|Zoey Guo et.al.|[2303.16894](http://arxiv.org/abs/2303.16894)|**[link](https://github.com/ivan-tang-3d/viewrefer3d)**|
|**2023-03-23**|**NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations**|Joy Hsu et.al.|[2303.13483](http://arxiv.org/abs/2303.13483)|null|
|**2023-03-23**|**ScanERU: Interactive 3D Visual Grounding based on Embodied Reference Understanding**|Ziyang Lu et.al.|[2303.13186](http://arxiv.org/abs/2303.13186)|**[link](https://github.com/mrlearnedtoad/scaneru)**|
|**2022-12-01**|**UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2212.00836](http://arxiv.org/abs/2212.00836)|null|
|**2022-11-25**|**Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding**|Eslam Mohamed Bakr et.al.|[2211.14241](http://arxiv.org/abs/2211.14241)|**[link](https://github.com/eslambakr/LAR-Look-Around-and-Refer)**|
|**2023-06-09**|**Learning Point-Language Hierarchical Alignment for 3D Visual Grounding**|Jiaming Chen et.al.|[2210.12513](http://arxiv.org/abs/2210.12513)|**[link](https://github.com/ppjmchen/ham)**|
|**2023-04-24**|**EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding**|Yanmin Wu et.al.|[2209.14941](http://arxiv.org/abs/2209.14941)|**[link](https://github.com/yanmin-wu/eda)**|
|**2023-05-27**|**Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases**|Zhihao Yuan et.al.|[2207.01821](http://arxiv.org/abs/2207.01821)|null|
|**2022-04-13**|**3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection**|Junyu Luo et.al.|[2204.06272](http://arxiv.org/abs/2204.06272)|**[link](https://github.com/fjhzhixi/3d-sps)**|
|**2022-04-05**|**Multi-View Transformer for 3D Visual Grounding**|Shijia Huang et.al.|[2204.02174](http://arxiv.org/abs/2204.02174)|**[link](https://github.com/sega-hsj/mvt-3dvg)**|
|**2022-07-22**|**D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding**|Dave Zhenyu Chen et.al.|[2112.01551](http://arxiv.org/abs/2112.01551)|null|
|**2021-08-11**|**TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding**|Dailan He et.al.|[2108.02388](http://arxiv.org/abs/2108.02388)|null|
|**2021-11-04**|**LanguageRefer: Spatial-Language Model for 3D Visual Grounding**|Junha Roh et.al.|[2107.03438](http://arxiv.org/abs/2107.03438)|null|
|**2021-09-22**|**SAT: 2D Semantics Assisted Training for 3D Visual Grounding**|Zhengyuan Yang et.al.|[2105.11450](http://arxiv.org/abs/2105.11450)|**[link](https://github.com/zyang-ur/SAT)**|
|**2021-03-17**|**Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images**|Haolin Liu et.al.|[2103.07894](http://arxiv.org/abs/2103.07894)|null|
|**2021-07-29**|**InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring**|Zhihao Yuan et.al.|[2103.01128](http://arxiv.org/abs/2103.01128)|**[link](https://github.com/CurryYuan/InstanceRefer)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

